{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOI2Nde5LsngOgKT6E3wI4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BaronVonBussin/NewTransit/blob/main/nested_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "tiYjUvDT7tw6",
        "outputId": "a84063cf-7765-4236-b182-381cc85680d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 0 tests in 0.000s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "False",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import logging\n",
        "from typing import Tuple, Dict, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('temporal_processing.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "@dataclass\n",
        "class ParentPeriodStats:\n",
        "    high: float\n",
        "    low: float\n",
        "    duration: int\n",
        "    completed_bars: int\n",
        "    rpc_count: int\n",
        "    bar_of_high: int\n",
        "    bar_of_low: int\n",
        "\n",
        "class DataValidationError(Exception):\n",
        "    pass\n",
        "\n",
        "class ConnectTemporalPeriods:\n",
        "    REQUIRED_COLUMNS = {'date', 'open', 'high', 'low', 'close'}\n",
        "\n",
        "    def __init__(self, child_period: str = \"D\", parent_period: str = \"M\"):\n",
        "        self.child_period = child_period\n",
        "        self.parent_period = parent_period\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def validate_dataframe(self, df: pd.DataFrame) -> None:\n",
        "        \"\"\"Validate input dataframe structure and content.\"\"\"\n",
        "        missing_cols = self.REQUIRED_COLUMNS - set(df.columns)\n",
        "        if missing_cols:\n",
        "            raise DataValidationError(f\"Missing required columns: {missing_cols}\")\n",
        "\n",
        "        # Check for date format\n",
        "        if not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
        "            try:\n",
        "                pd.to_datetime(df['date'])\n",
        "            except Exception as e:\n",
        "                raise DataValidationError(f\"Invalid date format: {e}\")\n",
        "\n",
        "        # Check for numeric price columns\n",
        "        price_cols = ['open', 'high', 'low', 'close']\n",
        "        for col in price_cols:\n",
        "            if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "                raise DataValidationError(f\"Column {col} must be numeric\")\n",
        "\n",
        "        # Validate high >= low\n",
        "        invalid_rows = df[df['high'] < df['low']].index\n",
        "        if len(invalid_rows) > 0:\n",
        "            raise DataValidationError(f\"Found {len(invalid_rows)} rows where high < low\")\n",
        "\n",
        "    def process_files(self, input_dir: str = \"input/\") -> None:\n",
        "        \"\"\"Process all CSV files in the input directory.\"\"\"\n",
        "        try:\n",
        "            os.makedirs(\"parent_output\", exist_ok=True)\n",
        "            os.makedirs(\"processed_output\", exist_ok=True)\n",
        "\n",
        "            file_count = 0\n",
        "            for filename in os.listdir(input_dir):\n",
        "                if filename.endswith(\".csv\"):\n",
        "                    ticker = filename.replace(\".csv\", \"\")\n",
        "                    self.logger.info(f\"Processing {ticker}\")\n",
        "                    self.process_single_file(os.path.join(input_dir, filename), ticker)\n",
        "                    file_count += 1\n",
        "\n",
        "            self.logger.info(f\"Successfully processed {file_count} files\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing files: {e}\")\n",
        "            raise\n",
        "\n",
        "    def process_single_file(self, filepath: str, ticker: str) -> None:\n",
        "        \"\"\"Process a single ticker's data file.\"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(filepath)\n",
        "            df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "            self.validate_dataframe(df)\n",
        "\n",
        "            df['parent_period'] = df['date'].dt.to_period(self.parent_period)\n",
        "            parent_stats = self._calculate_parent_stats(df)\n",
        "\n",
        "            processed_df = self._process_child_data(df, parent_stats)\n",
        "\n",
        "            # Save outputs with error handling\n",
        "            self._safe_save_csv(parent_stats, f\"parent_output/{ticker}_parent.csv\")\n",
        "            self._safe_save_csv(processed_df, f\"processed_output/{ticker}_processed.csv\")\n",
        "\n",
        "            self.logger.info(f\"Successfully processed {ticker}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing {ticker}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _safe_save_csv(self, df: pd.DataFrame, filepath: str) -> None:\n",
        "        \"\"\"Safely save DataFrame to CSV with error handling.\"\"\"\n",
        "        try:\n",
        "            df.to_csv(filepath, index=False)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error saving to {filepath}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_parent_stats(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Calculate parent period statistics.\"\"\"\n",
        "        try:\n",
        "            parent_stats = df.groupby('parent_period').agg({\n",
        "                'date': ['first', 'count'],\n",
        "                'high': 'max',\n",
        "                'low': 'min'\n",
        "            }).reset_index()\n",
        "\n",
        "            parent_stats.columns = ['parent_period', 'date', 'duration', 'parent_high', 'parent_low']\n",
        "            return parent_stats\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error calculating parent stats: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _process_child_data(self, df: pd.DataFrame, parent_stats: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Process child period data with detailed metrics.\"\"\"\n",
        "        try:\n",
        "            df = df.copy()\n",
        "\n",
        "            # Initialize columns\n",
        "            numeric_cols = ['intra_period_count', 'intra_period_high', 'intra_period_low',\n",
        "                          'intra_period_reu', 'intra_period_red', 'range_expansion_flag',\n",
        "                          'intra_period_bar_of_h', 'intra_period_bar_of_l',\n",
        "                          'bar_rpc', 'intra_period_cumulative_rpc']\n",
        "\n",
        "            for col in numeric_cols:\n",
        "                df[col] = 0\n",
        "\n",
        "            df['rpc_direction'] = 'N'\n",
        "\n",
        "            # Process each parent period\n",
        "            for period in df['parent_period'].unique():\n",
        "                mask = df['parent_period'] == period\n",
        "                period_data = df[mask].copy()\n",
        "\n",
        "                parent_high = parent_stats[parent_stats['parent_period'] == period]['parent_high'].iloc[0]\n",
        "                parent_low = parent_stats[parent_stats['parent_period'] == period]['parent_low'].iloc[0]\n",
        "\n",
        "                processed_period = self._process_parent_period(period_data, parent_high, parent_low)\n",
        "                df.loc[mask] = processed_period\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing child data: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _process_parent_period(\n",
        "        self, period_data: pd.DataFrame,\n",
        "        parent_high: float,\n",
        "        parent_low: float\n",
        "    ) -> pd.DataFrame:\n",
        "        \"\"\"Process data within a single parent period.\"\"\"\n",
        "        try:\n",
        "            active_bars = 0\n",
        "            cumulative_rpc = 0\n",
        "            prev_direction = 'N'\n",
        "            intra_high = float('-inf')\n",
        "            intra_low = float('inf')\n",
        "\n",
        "            for idx in period_data.index:\n",
        "                row = period_data.loc[idx]\n",
        "\n",
        "                if pd.isna(row['high']) or pd.isna(row['low']):\n",
        "                    continue\n",
        "\n",
        "                active_bars += 1\n",
        "                period_data.at[idx, 'intra_period_count'] = active_bars\n",
        "\n",
        "                # Initialize first bar\n",
        "                if intra_high == float('-inf'):\n",
        "                    intra_high = row['high']\n",
        "                    intra_low = row['low']\n",
        "\n",
        "                # Calculate range expansions\n",
        "                reu = max(0, row['high'] - intra_high)\n",
        "                red = max(0, intra_low - row['low'])\n",
        "\n",
        "                # Update metrics\n",
        "                period_data.at[idx, 'intra_period_reu'] = reu\n",
        "                period_data.at[idx, 'intra_period_red'] = red\n",
        "                period_data.at[idx, 'range_expansion_flag'] = 1 if (reu > 0 or red > 0) else 0\n",
        "\n",
        "                new_direction = self._determine_rpc_direction(\n",
        "                    reu, red, row['high'], row['low'], row['close'], prev_direction\n",
        "                )\n",
        "\n",
        "                if new_direction != prev_direction and prev_direction != 'N':\n",
        "                    cumulative_rpc += 1\n",
        "\n",
        "                period_data.at[idx, 'rpc_direction'] = new_direction\n",
        "                period_data.at[idx, 'intra_period_cumulative_rpc'] = cumulative_rpc\n",
        "                period_data.at[idx, 'bar_rpc'] = self._calculate_bar_rpc(\n",
        "                    reu, red, prev_direction, new_direction\n",
        "                )\n",
        "\n",
        "                # Update parent boundary bars\n",
        "                if row['high'] == parent_high and period_data.at[idx, 'intra_period_bar_of_h'] == 0:\n",
        "                    period_data.at[idx, 'intra_period_bar_of_h'] = active_bars\n",
        "                if row['low'] == parent_low and period_data.at[idx, 'intra_period_bar_of_l'] == 0:\n",
        "                    period_data.at[idx, 'intra_period_bar_of_l'] = active_bars\n",
        "\n",
        "                # Update running high/low\n",
        "                intra_high = max(intra_high, row['high'])\n",
        "                intra_low = min(intra_low, row['low'])\n",
        "                period_data.at[idx, 'intra_period_high'] = intra_high\n",
        "                period_data.at[idx, 'intra_period_low'] = intra_low\n",
        "\n",
        "                prev_direction = new_direction\n",
        "\n",
        "            return period_data\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing parent period: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _determine_rpc_direction(\n",
        "        self, reu: float, red: float,\n",
        "        high: float, low: float, close: float,\n",
        "        prev_direction: str\n",
        "    ) -> str:\n",
        "        \"\"\"Determine the RPC direction based on expansions.\"\"\"\n",
        "        try:\n",
        "            if reu == 0 and red == 0:\n",
        "                return prev_direction\n",
        "            elif reu > 0 and red > 0:\n",
        "                # Two-way expansion - use close location\n",
        "                return 'U' if (close - low) / (high - low) >= 0.5 else 'D'\n",
        "            elif reu > 0:\n",
        "                return 'U'\n",
        "            else:\n",
        "                return 'D'\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error determining RPC direction: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_bar_rpc(\n",
        "        self, reu: float, red: float,\n",
        "        prev_direction: str, new_direction: str\n",
        "    ) -> int:\n",
        "        \"\"\"Calculate the bar's RPC value.\"\"\"\n",
        "        try:\n",
        "            if reu == 0 and red == 0:\n",
        "                return 0\n",
        "            elif reu > 0 and red > 0:\n",
        "                if prev_direction == 'N' or new_direction != prev_direction:\n",
        "                    return 2\n",
        "                return 1\n",
        "            elif new_direction != prev_direction and prev_direction != 'N':\n",
        "                return 1\n",
        "            return 0\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error calculating bar RPC: {e}\")\n",
        "            raise\n",
        "\n",
        "def run_tests():\n",
        "    \"\"\"Run unit tests for the framework.\"\"\"\n",
        "    import unittest\n",
        "\n",
        "    class TestTemporalFramework(unittest.TestCase):\n",
        "        def setUp(self):\n",
        "            self.processor = ConnectTemporalPeriods()\n",
        "\n",
        "        def test_data_validation(self):\n",
        "            # Test missing columns\n",
        "            df_missing = pd.DataFrame({'date': [], 'high': [], 'low': []})\n",
        "            with self.assertRaises(DataValidationError):\n",
        "                self.processor.validate_dataframe(df_missing)\n",
        "\n",
        "            # Test invalid date format\n",
        "            df_bad_date = pd.DataFrame({\n",
        "                'date': ['bad_date'],\n",
        "                'open': [1], 'high': [2], 'low': [1], 'close': [1.5]\n",
        "            })\n",
        "            with self.assertRaises(DataValidationError):\n",
        "                self.processor.validate_dataframe(df_bad_date)\n",
        "\n",
        "            # Test high < low\n",
        "            df_invalid_hl = pd.DataFrame({\n",
        "                'date': ['2024-01-01'],\n",
        "                'open': [1], 'high': [1], 'low': [2], 'close': [1.5]\n",
        "            })\n",
        "            with self.assertRaises(DataValidationError):\n",
        "                self.processor.validate_dataframe(df_invalid_hl)\n",
        "\n",
        "        def test_rpc_calculation(self):\n",
        "            # Test no expansion\n",
        "            self.assertEqual(\n",
        "                self.processor._calculate_bar_rpc(0, 0, 'N', 'N'), 0\n",
        "            )\n",
        "\n",
        "            # Test two-way expansion\n",
        "            self.assertEqual(\n",
        "                self.processor._calculate_bar_rpc(1, 1, 'N', 'U'), 2\n",
        "            )\n",
        "\n",
        "            # Test direction change\n",
        "            self.assertEqual(\n",
        "                self.processor._calculate_bar_rpc(1, 0, 'D', 'U'), 1\n",
        "            )\n",
        "\n",
        "        def test_rpc_direction(self):\n",
        "            # Test no expansion\n",
        "            self.assertEqual(\n",
        "                self.processor._determine_rpc_direction(0, 0, 100, 90, 95, 'U'),\n",
        "                'U'\n",
        "            )\n",
        "\n",
        "            # Test two-way expansion with close in upper half\n",
        "            self.assertEqual(\n",
        "                self.processor._determine_rpc_direction(1, 1, 100, 90, 97, 'N'),\n",
        "                'U'\n",
        "            )\n",
        "\n",
        "            # Test upward expansion only\n",
        "            self.assertEqual(\n",
        "                self.processor._determine_rpc_direction(1, 0, 100, 90, 95, 'D'),\n",
        "                'U'\n",
        "            )\n",
        "\n",
        "    # Run tests\n",
        "    unittest.main(argv=['dummy'])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_tests()\n",
        "    processor = ConnectTemporalPeriods()\n",
        "    processor.process_files()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import logging\n",
        "from typing import Tuple, Dict, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('temporal_processing.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "@dataclass\n",
        "class ParentPeriodStats:\n",
        "    high: float\n",
        "    low: float\n",
        "    duration: int\n",
        "    completed_bars: int\n",
        "    rpc_count: int\n",
        "    bar_of_high: int\n",
        "    bar_of_low: int\n",
        "\n",
        "class DataValidationError(Exception):\n",
        "    pass\n",
        "\n",
        "class ConnectTemporalPeriods:\n",
        "    REQUIRED_COLUMNS = {'date', 'open', 'high', 'low', 'close'}\n",
        "\n",
        "    def __init__(self, child_period: str = \"D\", parent_period: str = \"ME\"):\n",
        "        self.child_period = child_period\n",
        "        self.parent_period = parent_period\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def validate_dataframe(self, df: pd.DataFrame) -> None:\n",
        "        \"\"\"Validate input dataframe structure and content.\"\"\"\n",
        "        missing_cols = self.REQUIRED_COLUMNS - set(df.columns)\n",
        "        if missing_cols:\n",
        "            raise DataValidationError(f\"Missing required columns: {missing_cols}\")\n",
        "\n",
        "        # Check for date format\n",
        "        if not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
        "            try:\n",
        "                pd.to_datetime(df['date'])\n",
        "            except Exception as e:\n",
        "                raise DataValidationError(f\"Invalid date format: {e}\")\n",
        "\n",
        "        # Check for numeric price columns\n",
        "        price_cols = ['open', 'high', 'low', 'close']\n",
        "        for col in price_cols:\n",
        "            if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "                raise DataValidationError(f\"Column {col} must be numeric\")\n",
        "\n",
        "        # Validate high >= low\n",
        "        invalid_rows = df[df['high'] < df['low']].index\n",
        "        if len(invalid_rows) > 0:\n",
        "            raise DataValidationError(f\"Found {len(invalid_rows)} rows where high < low\")\n",
        "\n",
        "    def process_files(self, input_dir: str = \"input/\") -> None:\n",
        "        \"\"\"Process all CSV files in the input directory.\"\"\"\n",
        "        try:\n",
        "            os.makedirs(\"parent_output\", exist_ok=True)\n",
        "            os.makedirs(\"processed_output\", exist_ok=True)\n",
        "\n",
        "            file_count = 0\n",
        "            for filename in os.listdir(input_dir):\n",
        "                if filename.endswith(\".csv\"):\n",
        "                    ticker = filename.replace(\".csv\", \"\")\n",
        "                    self.logger.info(f\"Processing {ticker}\")\n",
        "                    self.process_single_file(os.path.join(input_dir, filename), ticker)\n",
        "                    file_count += 1\n",
        "\n",
        "            self.logger.info(f\"Successfully processed {file_count} files\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing files: {e}\")\n",
        "            raise\n",
        "\n",
        "    def process_single_file(self, filepath: str, ticker: str) -> None:\n",
        "        \"\"\"Process a single ticker's data file.\"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(filepath)\n",
        "            df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "            self.validate_dataframe(df)\n",
        "\n",
        "            df['parent_period'] = df['date'].dt.to_period(self.parent_period)\n",
        "            parent_stats = self._calculate_parent_stats(df)\n",
        "\n",
        "            processed_df = self._process_child_data(df, parent_stats)\n",
        "\n",
        "            # Save outputs with error handling\n",
        "            self._safe_save_csv(parent_stats, f\"parent_output/{ticker}_parent.csv\")\n",
        "            self._safe_save_csv(processed_df, f\"processed_output/{ticker}_processed.csv\")\n",
        "\n",
        "            self.logger.info(f\"Successfully processed {ticker}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing {ticker}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _safe_save_csv(self, df: pd.DataFrame, filepath: str) -> None:\n",
        "        \"\"\"Safely save DataFrame to CSV with error handling.\"\"\"\n",
        "        try:\n",
        "            df.to_csv(filepath, index=False)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error saving to {filepath}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_parent_stats(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Calculate parent period statistics.\"\"\"\n",
        "        try:\n",
        "            parent_stats = df.groupby('parent_period').agg({\n",
        "                'date': ['first', 'count'],\n",
        "                'high': 'max',\n",
        "                'low': 'min'\n",
        "            }).reset_index()\n",
        "\n",
        "            parent_stats.columns = ['parent_period', 'date', 'duration', 'parent_high', 'parent_low']\n",
        "            return parent_stats\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error calculating parent stats: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _process_child_data(self, df: pd.DataFrame, parent_stats: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Process child period data with detailed metrics.\"\"\"\n",
        "        try:\n",
        "            df = df.copy()\n",
        "\n",
        "            # Initialize columns\n",
        "            numeric_cols = ['intra_period_count', 'intra_period_high', 'intra_period_low',\n",
        "                          'intra_period_reu', 'intra_period_red', 'range_expansion_flag',\n",
        "                          'intra_period_bar_of_h', 'intra_period_bar_of_l',\n",
        "                          'bar_rpc', 'intra_period_cumulative_rpc']\n",
        "\n",
        "            for col in numeric_cols:\n",
        "                df[col] = 0\n",
        "\n",
        "            df['rpc_direction'] = 'N'\n",
        "\n",
        "            # Process each parent period\n",
        "            for period in df['parent_period'].unique():\n",
        "                mask = df['parent_period'] == period\n",
        "                period_data = df[mask].copy()\n",
        "\n",
        "                parent_high = parent_stats[parent_stats['parent_period'] == period]['parent_high'].iloc[0]\n",
        "                parent_low = parent_stats[parent_stats['parent_period'] == period]['parent_low'].iloc[0]\n",
        "\n",
        "                processed_period = self._process_parent_period(period_data, parent_high, parent_low)\n",
        "                df.loc[mask] = processed_period\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing child data: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _process_parent_period(\n",
        "        self, period_data: pd.DataFrame,\n",
        "        parent_high: float,\n",
        "        parent_low: float\n",
        "    ) -> pd.DataFrame:\n",
        "        \"\"\"Process data within a single parent period.\"\"\"\n",
        "        try:\n",
        "            active_bars = 0\n",
        "            cumulative_rpc = 0\n",
        "            prev_direction = 'N'\n",
        "            intra_high = float('-inf')\n",
        "            intra_low = float('inf')\n",
        "\n",
        "            for idx in period_data.index:\n",
        "                row = period_data.loc[idx]\n",
        "\n",
        "                if pd.isna(row['high']) or pd.isna(row['low']):\n",
        "                    continue\n",
        "\n",
        "                active_bars += 1\n",
        "                period_data.at[idx, 'intra_period_count'] = active_bars\n",
        "\n",
        "                # Initialize first bar\n",
        "                if intra_high == float('-inf'):\n",
        "                    intra_high = row['high']\n",
        "                    intra_low = row['low']\n",
        "\n",
        "                # Calculate range expansions\n",
        "                reu = max(0, row['high'] - intra_high)\n",
        "                red = max(0, intra_low - row['low'])\n",
        "\n",
        "                # Update metrics\n",
        "                period_data.at[idx, 'intra_period_reu'] = reu\n",
        "                period_data.at[idx, 'intra_period_red'] = red\n",
        "                period_data.at[idx, 'range_expansion_flag'] = 1 if (reu > 0 or red > 0) else 0\n",
        "\n",
        "                new_direction = self._determine_rpc_direction(\n",
        "                    reu, red, row['high'], row['low'], row['close'], prev_direction\n",
        "                )\n",
        "\n",
        "                if new_direction != prev_direction and prev_direction != 'N':\n",
        "                    cumulative_rpc += 1\n",
        "\n",
        "                period_data.at[idx, 'rpc_direction'] = new_direction\n",
        "                period_data.at[idx, 'intra_period_cumulative_rpc'] = cumulative_rpc\n",
        "                period_data.at[idx, 'bar_rpc'] = self._calculate_bar_rpc(\n",
        "                    reu, red, prev_direction, new_direction\n",
        "                )\n",
        "\n",
        "                # Update parent boundary bars\n",
        "                if row['high'] == parent_high and period_data.at[idx, 'intra_period_bar_of_h'] == 0:\n",
        "                    period_data.at[idx, 'intra_period_bar_of_h'] = active_bars\n",
        "                if row['low'] == parent_low and period_data.at[idx, 'intra_period_bar_of_l'] == 0:\n",
        "                    period_data.at[idx, 'intra_period_bar_of_l'] = active_bars\n",
        "\n",
        "                # Update running high/low\n",
        "                intra_high = max(intra_high, row['high'])\n",
        "                intra_low = min(intra_low, row['low'])\n",
        "                period_data.at[idx, 'intra_period_high'] = intra_high\n",
        "                period_data.at[idx, 'intra_period_low'] = intra_low\n",
        "\n",
        "                prev_direction = new_direction\n",
        "\n",
        "            return period_data\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing parent period: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _determine_rpc_direction(\n",
        "        self, reu: float, red: float,\n",
        "        high: float, low: float, close: float,\n",
        "        prev_direction: str\n",
        "    ) -> str:\n",
        "        \"\"\"Determine the RPC direction based on expansions.\"\"\"\n",
        "        try:\n",
        "            if reu == 0 and red == 0:\n",
        "                return prev_direction\n",
        "            elif reu > 0 and red > 0:\n",
        "                # Two-way expansion - use close location\n",
        "                return 'U' if (close - low) / (high - low) >= 0.5 else 'D'\n",
        "            elif reu > 0:\n",
        "                return 'U'\n",
        "            else:\n",
        "                return 'D'\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error determining RPC direction: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_bar_rpc(\n",
        "        self, reu: float, red: float,\n",
        "        prev_direction: str, new_direction: str\n",
        "    ) -> int:\n",
        "        \"\"\"Calculate the bar's RPC value.\"\"\"\n",
        "        try:\n",
        "            if reu == 0 and red == 0:\n",
        "                return 0\n",
        "            elif reu > 0 and red > 0:\n",
        "                if prev_direction == 'N' or new_direction != prev_direction:\n",
        "                    return 2\n",
        "                return 1\n",
        "            elif new_direction != prev_direction and prev_direction != 'N':\n",
        "                return 1\n",
        "            return 0\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error calculating bar RPC: {e}\")\n",
        "            raise\n",
        "\n",
        "def run_tests():\n",
        "    \"\"\"Run unit tests for the framework.\"\"\"\n",
        "    import unittest\n",
        "\n",
        "    class TestTemporalFramework(unittest.TestCase):\n",
        "        def setUp(self):\n",
        "            self.processor = ConnectTemporalPeriods()\n",
        "\n",
        "        def test_data_validation(self):\n",
        "            # Test missing columns\n",
        "            df_missing = pd.DataFrame({'date': [], 'high': [], 'low': []})\n",
        "            with self.assertRaises(DataValidationError):\n",
        "                self.processor.validate_dataframe(df_missing)\n",
        "\n",
        "            # Test invalid date format\n",
        "            df_bad_date = pd.DataFrame({\n",
        "                'date': ['bad_date'],\n",
        "                'open': [1], 'high': [2], 'low': [1], 'close': [1.5]\n",
        "            })\n",
        "            with self.assertRaises(DataValidationError):\n",
        "                self.processor.validate_dataframe(df_bad_date)\n",
        "\n",
        "            # Test high < low\n",
        "            df_invalid_hl = pd.DataFrame({\n",
        "                'date': ['2024-01-01'],\n",
        "                'open': [1], 'high': [1], 'low': [2], 'close': [1.5]\n",
        "            })\n",
        "            with self.assertRaises(DataValidationError):\n",
        "                self.processor.validate_dataframe(df_invalid_hl)\n",
        "\n",
        "        def test_rpc_calculation(self):\n",
        "            # Test no expansion\n",
        "            self.assertEqual(\n",
        "                self.processor._calculate_bar_rpc(0, 0, 'N', 'N'), 0\n",
        "            )\n",
        "\n",
        "            # Test two-way expansion\n",
        "            self.assertEqual(\n",
        "                self.processor._calculate_bar_rpc(1, 1, 'N', 'U'), 2\n",
        "            )\n",
        "\n",
        "            # Test direction change\n",
        "            self.assertEqual(\n",
        "                self.processor._calculate_bar_rpc(1, 0, 'D', 'U'), 1\n",
        "            )\n",
        "\n",
        "        def test_rpc_direction(self):\n",
        "            # Test no expansion\n",
        "            self.assertEqual(\n",
        "                self.processor._determine_rpc_direction(0, 0, 100, 90, 95, 'U'),\n",
        "                'U'\n",
        "            )\n",
        "\n",
        "            # Test two-way expansion with close in upper half\n",
        "            self.assertEqual(\n",
        "                self.processor._determine_rpc_direction(1, 1, 100, 90, 97, 'N'),\n",
        "                'U'\n",
        "            )\n",
        "\n",
        "            # Test upward expansion only\n",
        "            self.assertEqual(\n",
        "                self.processor._determine_rpc_direction(1, 0, 100, 90, 95, 'D'),\n",
        "                'U'\n",
        "            )\n",
        "\n",
        "    # Run tests\n",
        "    unittest.main()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_tests()\n",
        "    processor = ConnectTemporalPeriods()\n",
        "    processor.process_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "FhuNbBzP8JTj",
        "outputId": "458c3ce7-114f-43b3-adf7-05f575389669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E\n",
            "======================================================================\n",
            "ERROR: /root/ (unittest.loader._FailedTest)\n",
            "----------------------------------------------------------------------\n",
            "AttributeError: module '__main__' has no attribute '/root/'\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.005s\n",
            "\n",
            "FAILED (errors=1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "True",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "# %%\n",
        "import pandas as pd\n",
        "import os\n",
        "import logging\n",
        "from typing import Tuple, Dict, Optional\n",
        "from dataclasses import dataclass\n",
        "import unittest\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('temporal_processing.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "@dataclass\n",
        "class ParentPeriodStats:\n",
        "    high: float\n",
        "    low: float\n",
        "    duration: int\n",
        "    completed_bars: int\n",
        "    rpc_count: int\n",
        "    bar_of_high: int\n",
        "    bar_of_low: int\n",
        "\n",
        "class DataValidationError(Exception):\n",
        "    pass\n",
        "\n",
        "class ConnectTemporalPeriods:\n",
        "    REQUIRED_COLUMNS = {'date', 'open', 'high', 'low', 'close'}\n",
        "\n",
        "    def __init__(self, child_period: str = \"D\", parent_period: str = \"M\"):\n",
        "        self.child_period = child_period\n",
        "        self.parent_period = parent_period\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def validate_dataframe(self, df: pd.DataFrame) -> None:\n",
        "        \"\"\"Validate input dataframe structure and content.\"\"\"\n",
        "        missing_cols = self.REQUIRED_COLUMNS - set(df.columns)\n",
        "        if missing_cols:\n",
        "            raise DataValidationError(f\"Missing required columns: {missing_cols}\")\n",
        "\n",
        "        # Check for date format\n",
        "        if not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
        "            try:\n",
        "                pd.to_datetime(df['date'])\n",
        "            except Exception as e:\n",
        "                raise DataValidationError(f\"Invalid date format: {e}\")\n",
        "\n",
        "        # Check for numeric price columns\n",
        "        price_cols = ['open', 'high', 'low', 'close']\n",
        "        for col in price_cols:\n",
        "            if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "                raise DataValidationError(f\"Column {col} must be numeric\")\n",
        "\n",
        "        # Validate high >= low\n",
        "        invalid_rows = df[df['high'] < df['low']].index\n",
        "        if len(invalid_rows) > 0:\n",
        "            raise DataValidationError(f\"Found {len(invalid_rows)} rows where high < low\")\n",
        "\n",
        "    def process_files(self, input_dir: str = \"input/\") -> None:\n",
        "        \"\"\"Process all CSV files in the input directory.\"\"\"\n",
        "        try:\n",
        "            os.makedirs(\"parent_output\", exist_ok=True)\n",
        "            os.makedirs(\"processed_output\", exist_ok=True)\n",
        "\n",
        "            file_count = 0\n",
        "            for filename in os.listdir(input_dir):\n",
        "                if filename.endswith(\".csv\"):\n",
        "                    ticker = filename.replace(\".csv\", \"\")\n",
        "                    self.logger.info(f\"Processing {ticker}\")\n",
        "                    self.process_single_file(os.path.join(input_dir, filename), ticker)\n",
        "                    file_count += 1\n",
        "\n",
        "            self.logger.info(f\"Successfully processed {file_count} files\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing files: {e}\")\n",
        "            raise\n",
        "\n",
        "    def process_single_file(self, filepath: str, ticker: str) -> None:\n",
        "        \"\"\"Process a single ticker's data file.\"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(filepath)\n",
        "            df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "            self.validate_dataframe(df)\n",
        "\n",
        "            df['parent_period'] = df['date'].dt.to_period(self.parent_period)\n",
        "            parent_stats = self._calculate_parent_stats(df)\n",
        "\n",
        "            processed_df = self._process_child_data(df, parent_stats)\n",
        "\n",
        "            # Save outputs with error handling\n",
        "            self._safe_save_csv(parent_stats, f\"parent_output/{ticker}_parent.csv\")\n",
        "            self._safe_save_csv(processed_df, f\"processed_output/{ticker}_processed.csv\")\n",
        "\n",
        "            self.logger.info(f\"Successfully processed {ticker}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing {ticker}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _safe_save_csv(self, df: pd.DataFrame, filepath: str) -> None:\n",
        "        \"\"\"Safely save DataFrame to CSV with error handling.\"\"\"\n",
        "        try:\n",
        "            df.to_csv(filepath, index=False)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error saving to {filepath}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_parent_stats(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Calculate parent period statistics.\"\"\"\n",
        "        try:\n",
        "            parent_stats = df.groupby('parent_period').agg({\n",
        "                'date': ['first', 'count'],\n",
        "                'high': 'max',\n",
        "                'low': 'min'\n",
        "            }).reset_index()\n",
        "\n",
        "            parent_stats.columns = ['parent_period', 'date', 'duration', 'parent_high', 'parent_low']\n",
        "            return parent_stats\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error calculating parent stats: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _process_child_data(self, df: pd.DataFrame, parent_stats: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Process child period data with detailed metrics.\"\"\"\n",
        "        try:\n",
        "            df = df.copy()\n",
        "\n",
        "            # Initialize columns\n",
        "            numeric_cols = ['intra_period_count', 'intra_period_high', 'intra_period_low',\n",
        "                          'intra_period_reu', 'intra_period_red', 'range_expansion_flag',\n",
        "                          'intra_period_bar_of_h', 'intra_period_bar_of_l',\n",
        "                          'bar_rpc', 'intra_period_cumulative_rpc']\n",
        "\n",
        "            for col in numeric_cols:\n",
        "                df[col] = 0\n",
        "\n",
        "            df['rpc_direction'] = 'N'\n",
        "\n",
        "            # Process each parent period\n",
        "            for period in df['parent_period'].unique():\n",
        "                mask = df['parent_period'] == period\n",
        "                period_data = df[mask].copy()\n",
        "\n",
        "                parent_high = parent_stats[parent_stats['parent_period'] == period]['parent_high'].iloc[0]\n",
        "                parent_low = parent_stats[parent_stats['parent_period'] == period]['parent_low'].iloc[0]\n",
        "\n",
        "                processed_period = self._process_parent_period(period_data, parent_high, parent_low)\n",
        "                df.loc[mask] = processed_period\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing child data: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _process_parent_period(\n",
        "        self, period_data: pd.DataFrame,\n",
        "        parent_high: float,\n",
        "        parent_low: float\n",
        "    ) -> pd.DataFrame:\n",
        "        \"\"\"Process data within a single parent period.\"\"\"\n",
        "        try:\n",
        "            active_bars = 0\n",
        "            cumulative_rpc = 0\n",
        "            prev_direction = 'N'\n",
        "            intra_high = float('-inf')\n",
        "            intra_low = float('inf')\n",
        "\n",
        "            for idx in period_data.index:\n",
        "                row = period_data.loc[idx]\n",
        "\n",
        "                if pd.isna(row['high']) or pd.isna(row['low']):\n",
        "                    continue\n",
        "\n",
        "                active_bars += 1\n",
        "                period_data.at[idx, 'intra_period_count'] = active_bars\n",
        "\n",
        "                # Initialize first bar\n",
        "                if intra_high == float('-inf'):\n",
        "                    intra_high = row['high']\n",
        "                    intra_low = row['low']\n",
        "\n",
        "                # Calculate range expansions\n",
        "                reu = max(0, row['high'] - intra_high) if intra_high != float('-inf') else 0\n",
        "                red = max(0, intra_low - row['low']) if intra_low != float('inf') else 0\n",
        "\n",
        "                # Then update running high/low for next iteration\n",
        "                intra_high = max(intra_high, row['high']) if intra_high != float('-inf') else row['high']\n",
        "                intra_low = min(intra_low, row['low']) if intra_low != float('inf') else row['low']\n",
        "\n",
        "                # Update metrics\n",
        "                period_data.at[idx, 'intra_period_reu'] = reu\n",
        "                period_data.at[idx, 'intra_period_red'] = red\n",
        "                period_data.at[idx, 'range_expansion_flag'] = 1 if (reu > 0 or red > 0) else 0\n",
        "\n",
        "                new_direction = self._determine_rpc_direction(\n",
        "                    reu, red, row['high'], row['low'], row['close'], prev_direction\n",
        "                )\n",
        "\n",
        "                if new_direction != prev_direction and prev_direction != 'N':\n",
        "                    cumulative_rpc += 1\n",
        "\n",
        "                period_data.at[idx, 'rpc_direction'] = new_direction\n",
        "                period_data.at[idx, 'intra_period_cumulative_rpc'] = cumulative_rpc\n",
        "                period_data.at[idx, 'bar_rpc'] = self._calculate_bar_rpc(\n",
        "                    reu, red, prev_direction, new_direction\n",
        "                )\n",
        "\n",
        "                # Update parent boundary bars\n",
        "                if row['high'] == parent_high and period_data.at[idx, 'intra_period_bar_of_h'] == 0:\n",
        "                    period_data.at[idx, 'intra_period_bar_of_h'] = active_bars\n",
        "                if row['low'] == parent_low and period_data.at[idx, 'intra_period_bar_of_l'] == 0:\n",
        "                    period_data.at[idx, 'intra_period_bar_of_l'] = active_bars\n",
        "\n",
        "                # Update running high/low\n",
        "                intra_high = max(intra_high, row['high'])\n",
        "                intra_low = min(intra_low, row['low'])\n",
        "                period_data.at[idx, 'intra_period_high'] = intra_high\n",
        "                period_data.at[idx, 'intra_period_low'] = intra_low\n",
        "\n",
        "                prev_direction = new_direction\n",
        "\n",
        "            return period_data\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing parent period: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _determine_rpc_direction(\n",
        "        self, reu: float, red: float,\n",
        "        high: float, low: float, close: float,\n",
        "        prev_direction: str\n",
        "    ) -> str:\n",
        "        \"\"\"Determine the RPC direction based on expansions.\"\"\"\n",
        "        try:\n",
        "            if reu == 0 and red == 0:\n",
        "                return prev_direction\n",
        "            elif reu > 0 and red > 0:\n",
        "                # Two-way expansion - use close location\n",
        "                return 'U' if (close - low) / (high - low) >= 0.5 else 'D'\n",
        "            elif reu > 0:\n",
        "                return 'U'\n",
        "            else:\n",
        "                return 'D'\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error determining RPC direction: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_bar_rpc(\n",
        "        self, reu: float, red: float,\n",
        "        prev_direction: str, new_direction: str\n",
        "    ) -> int:\n",
        "        \"\"\"Calculate the bar's RPC value.\"\"\"\n",
        "        try:\n",
        "            if reu == 0 and red == 0:\n",
        "                return 0\n",
        "            elif reu > 0 and red > 0:\n",
        "                if prev_direction == 'N' or new_direction != prev_direction:\n",
        "                    return 2\n",
        "                return 1\n",
        "            elif new_direction != prev_direction and prev_direction != 'N':\n",
        "                return 1\n",
        "            return 0\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error calculating bar RPC: {e}\")\n",
        "            raise\n",
        "\n",
        "def run_tests():\n",
        "    \"\"\"Run unit tests for the framework.\"\"\"\n",
        "\n",
        "    class TestTemporalFramework(unittest.TestCase):\n",
        "        def setUp(self):\n",
        "            self.processor = ConnectTemporalPeriods()\n",
        "\n",
        "        def test_data_validation(self):\n",
        "            # Test missing columns\n",
        "            df_missing = pd.DataFrame({'date': [], 'high': [], 'low': []})\n",
        "            with self.assertRaises(DataValidationError):\n",
        "                self.processor.validate_dataframe(df_missing)\n",
        "\n",
        "            # Test invalid date format\n",
        "            df_bad_date = pd.DataFrame({\n",
        "                'date': ['bad_date'],\n",
        "                'open': [1], 'high': [2], 'low': [1], 'close': [1.5]\n",
        "            })\n",
        "            with self.assertRaises(DataValidationError):\n",
        "                self.processor.validate_dataframe(df_bad_date)\n",
        "\n",
        "            # Test high < low\n",
        "            df_invalid_hl = pd.DataFrame({\n",
        "                'date': ['2024-01-01'],\n",
        "                'open': [1], 'high': [1], 'low': [2], 'close': [1.5]\n",
        "            })\n",
        "            with self.assertRaises(DataValidationError):\n",
        "                self.processor.validate_dataframe(df_invalid_hl)\n",
        "\n",
        "        def test_rpc_calculation(self):\n",
        "            # Test no expansion\n",
        "            self.assertEqual(\n",
        "                self.processor._calculate_bar_rpc(0, 0, 'N', 'N'), 0\n",
        "            )\n",
        "\n",
        "            # Test two-way expansion\n",
        "            self.assertEqual(\n",
        "                self.processor._calculate_bar_rpc(1, 1, 'N', 'U'), 2\n",
        "            )\n",
        "\n",
        "            # Test direction change\n",
        "            self.assertEqual(\n",
        "                self.processor._calculate_bar_rpc(1, 0, 'D', 'U'), 1\n",
        "            )\n",
        "\n",
        "        def test_rpc_direction(self):\n",
        "            # Test no expansion\n",
        "            self.assertEqual(\n",
        "                self.processor._determine_rpc_direction(0, 0, 100, 90, 95, 'U'),\n",
        "                'U'\n",
        "            )\n",
        "\n",
        "            # Test two-way expansion with close in upper half\n",
        "            self.assertEqual(\n",
        "                self.processor._determine_rpc_direction(1, 1, 100, 90, 97, 'N'),\n",
        "                'U'\n",
        "            )\n",
        "\n",
        "            # Test upward expansion only\n",
        "            self.assertEqual(\n",
        "                self.processor._determine_rpc_direction(1, 0, 100, 90, 95, 'D'),\n",
        "                'U'\n",
        "            )\n",
        "\n",
        "    # Run tests\n",
        "    unittest.main(argv=['dummy'], exit=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_tests()\n",
        "    processor = ConnectTemporalPeriods()\n",
        "    processor.process_files()\n",
        "# %%"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nzRmXNa8yyU",
        "outputId": "1876715a-7e43-40d1-ec07-354d233a7a74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 0 tests in 0.000s\n",
            "\n",
            "OK\n",
            "<ipython-input-8-ea54c2fd132a>:228: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '124.0133743' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  period_data.at[idx, 'intra_period_high'] = intra_high\n",
            "<ipython-input-8-ea54c2fd132a>:229: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '121.571907' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  period_data.at[idx, 'intra_period_low'] = intra_low\n",
            "<ipython-input-8-ea54c2fd132a>:203: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1.655517500000002' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  period_data.at[idx, 'intra_period_red'] = red\n",
            "<ipython-input-8-ea54c2fd132a>:202: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '2.240806599999999' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  period_data.at[idx, 'intra_period_reu'] = reu\n",
            "<ipython-input-8-ea54c2fd132a>:158: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[124.0133743 124.0133743 124.0133743 124.0133743 124.0133743 124.0133743\n",
            " 124.0133743 124.0133743 124.0133743 124.0133743 124.0133743 124.0133743\n",
            " 124.0133743 124.0133743 124.0133743 124.0133743 124.0133743 124.0133743\n",
            " 126.2541809]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  df.loc[mask] = processed_period\n",
            "<ipython-input-8-ea54c2fd132a>:158: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[121.571907  121.571907  119.9163895 117.5836105 117.2408066 116.5635452\n",
            " 116.3294296 115.8361206 115.8361206 114.7240829 114.7240829 112.5752487\n",
            " 112.5752487 112.5752487 112.5752487 112.5752487 112.5752487 112.5752487\n",
            " 112.5752487]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  df.loc[mask] = processed_period\n",
            "<ipython-input-8-ea54c2fd132a>:158: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        2.2408066]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  df.loc[mask] = processed_period\n",
            "<ipython-input-8-ea54c2fd132a>:158: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0.        0.        1.6555175 2.332779  0.3428039 0.6772614 0.2341156\n",
            " 0.493309  0.        1.1120377 0.        2.1488342 0.        0.\n",
            " 0.        0.        0.        0.        0.       ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  df.loc[mask] = processed_period\n",
            "<ipython-input-8-ea54c2fd132a>:228: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '29.59000015' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  period_data.at[idx, 'intra_period_high'] = intra_high\n",
            "<ipython-input-8-ea54c2fd132a>:229: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '29.10000038' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  period_data.at[idx, 'intra_period_low'] = intra_low\n",
            "<ipython-input-8-ea54c2fd132a>:202: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.22500037999999734' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  period_data.at[idx, 'intra_period_reu'] = reu\n",
            "<ipython-input-8-ea54c2fd132a>:203: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.125' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  period_data.at[idx, 'intra_period_red'] = red\n",
            "<ipython-input-8-ea54c2fd132a>:158: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[29.59000015 29.81500053 29.81500053 29.81500053 29.81500053 29.81500053\n",
            " 29.81500053 29.81500053 29.81500053 29.81500053 29.81500053 29.81500053\n",
            " 29.81500053 29.81500053 29.81500053 29.81500053 29.81500053 29.81500053\n",
            " 29.81500053]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  df.loc[mask] = processed_period\n",
            "<ipython-input-8-ea54c2fd132a>:158: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[29.10000038 29.10000038 28.97500038 28.46500015 28.30500031 28.13500023\n",
            " 28.13500023 28.13500023 28.13500023 27.94000053 27.94000053 27.28499985\n",
            " 27.28499985 27.28499985 27.28499985 27.28499985 27.28499985 27.28499985\n",
            " 27.28499985]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  df.loc[mask] = processed_period\n",
            "<ipython-input-8-ea54c2fd132a>:158: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0.         0.22500038 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.        ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  df.loc[mask] = processed_period\n",
            "<ipython-input-8-ea54c2fd132a>:158: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0.         0.         0.125      0.51000023 0.15999984 0.17000008\n",
            " 0.         0.         0.         0.1949997  0.         0.65500068\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.        ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  df.loc[mask] = processed_period\n",
            "<ipython-input-8-ea54c2fd132a>:228: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '26.34250069' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  period_data.at[idx, 'intra_period_high'] = intra_high\n",
            "<ipython-input-8-ea54c2fd132a>:229: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '25.5' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  period_data.at[idx, 'intra_period_low'] = intra_low\n",
            "<ipython-input-8-ea54c2fd132a>:202: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.11999892999999773' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  period_data.at[idx, 'intra_period_reu'] = reu\n",
            "<ipython-input-8-ea54c2fd132a>:203: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.5324993099999986' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  period_data.at[idx, 'intra_period_red'] = red\n",
            "<ipython-input-8-ea54c2fd132a>:158: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[26.34250069 26.46249962 26.46249962 26.46249962 26.46249962 26.46249962\n",
            " 26.46249962 26.46249962 26.46249962 26.46249962 26.46249962 26.46249962\n",
            " 26.46249962 26.46249962 26.46249962 26.46249962 26.46249962 26.46249962\n",
            " 26.46249962]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  df.loc[mask] = processed_period\n",
            "<ipython-input-8-ea54c2fd132a>:158: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[25.5        25.5        24.96750069 24.10750008 24.10750008 24.10750008\n",
            " 24.10750008 24.10750008 23.93499947 23.84000015 23.84000015 23.35499954\n",
            " 23.35499954 23.35499954 23.35499954 23.35499954 23.33499908 23.09749985\n",
            " 23.09749985]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  df.loc[mask] = processed_period\n",
            "<ipython-input-8-ea54c2fd132a>:158: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0.         0.11999893 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.        ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  df.loc[mask] = processed_period\n",
            "<ipython-input-8-ea54c2fd132a>:158: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0.         0.         0.53249931 0.86000061 0.         0.\n",
            " 0.         0.         0.17250061 0.09499932 0.         0.48500061\n",
            " 0.         0.         0.         0.         0.02000046 0.23749923\n",
            " 0.        ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  df.loc[mask] = processed_period\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import logging\n",
        "from typing import Tuple, Dict, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('temporal_processing.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "@dataclass\n",
        "class ParentPeriodStats:\n",
        "    high: float\n",
        "    low: float\n",
        "    duration: int\n",
        "    completed_bars: int\n",
        "    rpc_count: int\n",
        "    bar_of_high: int\n",
        "    bar_of_low: int\n",
        "\n",
        "class DataValidationError(Exception):\n",
        "    pass\n",
        "\n",
        "class ConnectTemporalPeriods:\n",
        "    REQUIRED_COLUMNS = {'date', 'open', 'high', 'low', 'close'}\n",
        "\n",
        "    def __init__(self, child_period: str = \"D\", parent_period: str = \"ME\"):\n",
        "        self.child_period = child_period\n",
        "        self.parent_period = parent_period\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def validate_dataframe(self, df: pd.DataFrame) -> None:\n",
        "        \"\"\"Validate input dataframe structure and content.\"\"\"\n",
        "        missing_cols = self.REQUIRED_COLUMNS - set(df.columns)\n",
        "        if missing_cols:\n",
        "            raise DataValidationError(f\"Missing required columns: {missing_cols}\")\n",
        "\n",
        "        if not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
        "            try:\n",
        "                pd.to_datetime(df['date'])\n",
        "            except Exception as e:\n",
        "                raise DataValidationError(f\"Invalid date format: {e}\")\n",
        "\n",
        "        price_cols = ['open', 'high', 'low', 'close']\n",
        "        for col in price_cols:\n",
        "            if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "                raise DataValidationError(f\"Column {col} must be numeric\")\n",
        "\n",
        "        invalid_rows = df[df['high'] < df['low']].index\n",
        "        if len(invalid_rows) > 0:\n",
        "            raise DataValidationError(f\"Found {len(invalid_rows)} rows where high < low\")\n",
        "\n",
        "    def process_files(self, input_dir: str = \"input/\") -> None:\n",
        "        \"\"\"Process all CSV files in the input directory.\"\"\"\n",
        "        try:\n",
        "            os.makedirs(\"parent_output\", exist_ok=True)\n",
        "            os.makedirs(\"processed_output\", exist_ok=True)\n",
        "\n",
        "            file_count = 0\n",
        "            for filename in os.listdir(input_dir):\n",
        "                if filename.endswith(\".csv\"):\n",
        "                    ticker = filename.replace(\".csv\", \"\")\n",
        "                    self.logger.info(f\"Processing {ticker}\")\n",
        "                    self.process_single_file(os.path.join(input_dir, filename), ticker)\n",
        "                    file_count += 1\n",
        "\n",
        "            self.logger.info(f\"Successfully processed {file_count} files\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing files: {e}\")\n",
        "            raise\n",
        "\n",
        "    def process_single_file(self, filepath: str, ticker: str) -> None:\n",
        "        \"\"\"Process a single ticker's data file.\"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(filepath)\n",
        "            df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "            self.validate_dataframe(df)\n",
        "\n",
        "            df['parent_period'] = df['date'].dt.to_period(self.parent_period)\n",
        "            parent_stats = self._calculate_parent_stats(df)\n",
        "\n",
        "            processed_df = self._process_child_data(df, parent_stats)\n",
        "\n",
        "            self._safe_save_csv(parent_stats, f\"parent_output/{ticker}_parent.csv\")\n",
        "            self._safe_save_csv(processed_df, f\"processed_output/{ticker}_processed.csv\")\n",
        "\n",
        "            self.logger.info(f\"Successfully processed {ticker}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing {ticker}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _safe_save_csv(self, df: pd.DataFrame, filepath: str) -> None:\n",
        "        \"\"\"Safely save DataFrame to CSV with error handling.\"\"\"\n",
        "        try:\n",
        "            df.to_csv(filepath, index=False)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error saving to {filepath}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_parent_stats(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Calculate parent period statistics including bar locations and counts.\"\"\"\n",
        "        try:\n",
        "            # Basic stats\n",
        "            parent_stats = df.groupby('parent_period').agg({\n",
        "                'date': ['first', 'count'],\n",
        "                'high': 'max',\n",
        "                'low': 'min',\n",
        "                'range_expansion_flag': 'sum',\n",
        "                'intra_period_cumulative_rpc': 'max'\n",
        "            }).reset_index()\n",
        "\n",
        "            parent_stats.columns = ['parent_period', 'date', 'duration', 'parent_high',\n",
        "                                  'parent_low', 'expansion_count', 'rpc_count']\n",
        "\n",
        "            # Calculate bar positions for each parent period\n",
        "            for period in df['parent_period'].unique():\n",
        "                period_data = df[df['parent_period'] == period]\n",
        "                period_high = parent_stats.loc[parent_stats['parent_period'] == period, 'parent_high'].iloc[0]\n",
        "                period_low = parent_stats.loc[parent_stats['parent_period'] == period, 'parent_low'].iloc[0]\n",
        "\n",
        "                # Find first occurrence of high and low\n",
        "                bar_of_high = period_data[period_data['high'] == period_high].index.min()\n",
        "                bar_of_low = period_data[period_data['low'] == period_low].index.min()\n",
        "\n",
        "                if bar_of_high is not None:\n",
        "                    parent_stats.loc[parent_stats['parent_period'] == period, 'bar_of_high'] = \\\n",
        "                        period_data.loc[:bar_of_high].notna().sum()\n",
        "                if bar_of_low is not None:\n",
        "                    parent_stats.loc[parent_stats['parent_period'] == period, 'bar_of_low'] = \\\n",
        "                        period_data.loc[:bar_of_low].notna().sum()\n",
        "\n",
        "            return parent_stats\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(\"Error calculating parent stats: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _process_child_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Process child period data with detailed metrics.\"\"\"\n",
        "        try:\n",
        "            df = df.copy()\n",
        "\n",
        "            # Initialize columns with correct dtypes\n",
        "            numeric_cols = {\n",
        "                'intra_period_count': 'int64',\n",
        "                'intra_period_high': 'float64',\n",
        "                'intra_period_low': 'float64',\n",
        "                'intra_period_reu': 'float64',\n",
        "                'intra_period_red': 'float64',\n",
        "                'range_expansion_flag': 'int64',\n",
        "                'bar_rpc': 'int64',\n",
        "                'intra_period_cumulative_rpc': 'int64'\n",
        "            }\n",
        "\n",
        "            for col, dtype in numeric_cols.items():\n",
        "                df[col] = pd.Series(dtype=dtype)\n",
        "\n",
        "            df['rpc_direction'] = 'N'\n",
        "\n",
        "            # Process each parent period\n",
        "            for period in df['parent_period'].unique():\n",
        "                mask = df['parent_period'] == period\n",
        "                period_data = df[mask].copy()\n",
        "\n",
        "                processed_period = self._process_parent_period(period_data)\n",
        "                df.loc[mask] = processed_period.astype(df[mask].dtypes)\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing child data: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _process_parent_period(self, period_data: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Process data within a single parent period.\"\"\"\n",
        "        try:\n",
        "            active_bars = 0\n",
        "            cumulative_rpc = 0\n",
        "            prev_direction = 'N'\n",
        "            prior_intra_high = None\n",
        "            prior_intra_low = None\n",
        "\n",
        "            for idx in period_data.index:\n",
        "                row = period_data.loc[idx]\n",
        "\n",
        "                if pd.isna(row['high']) or pd.isna(row['low']):\n",
        "                    continue\n",
        "\n",
        "                active_bars += 1\n",
        "                period_data.at[idx, 'intra_period_count'] = active_bars\n",
        "\n",
        "                # Calculate range expansions using prior values\n",
        "                reu = max(0, row['high'] - prior_intra_high) if prior_intra_high is not None else 0\n",
        "                red = max(0, prior_intra_low - row['low']) if prior_intra_low is not None else 0\n",
        "\n",
        "                # Update metrics\n",
        "                period_data.at[idx, 'intra_period_reu'] = reu\n",
        "                period_data.at[idx, 'intra_period_red'] = red\n",
        "                period_data.at[idx, 'range_expansion_flag'] = 1 if (reu > 0 or red > 0) else 0\n",
        "\n",
        "                new_direction = self._determine_rpc_direction(\n",
        "                    reu, red, row['high'], row['low'], row['close'], prev_direction\n",
        "                )\n",
        "\n",
        "                if new_direction != prev_direction and prev_direction != 'N':\n",
        "                    cumulative_rpc += 1\n",
        "\n",
        "                period_data.at[idx, 'rpc_direction'] = new_direction\n",
        "                period_data.at[idx, 'intra_period_cumulative_rpc'] = cumulative_rpc\n",
        "                period_data.at[idx, 'bar_rpc'] = self._calculate_bar_rpc(\n",
        "                    reu, red, prev_direction, new_direction\n",
        "                )\n",
        "\n",
        "                # Update prior values for next iteration\n",
        "                prior_intra_high = max(row['high'], prior_intra_high if prior_intra_high is not None else row['high'])\n",
        "                prior_intra_low = min(row['low'], prior_intra_low if prior_intra_low is not None else row['low'])\n",
        "\n",
        "                prev_direction = new_direction\n",
        "\n",
        "            return period_data\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing parent period: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _determine_rpc_direction(\n",
        "        self, reu: float, red: float,\n",
        "        high: float, low: float, close: float,\n",
        "        prev_direction: str\n",
        "    ) -> str:\n",
        "        \"\"\"Determine the RPC direction based on expansions.\"\"\"\n",
        "        try:\n",
        "            if reu == 0 and red == 0:\n",
        "                return prev_direction\n",
        "            elif reu > 0 and red > 0:\n",
        "                # Two-way expansion - use close location\n",
        "                return 'U' if (close - low) / (high - low) >= 0.5 else 'D'\n",
        "            elif reu > 0:\n",
        "                return 'U'\n",
        "            else:\n",
        "                return 'D'\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error determining RPC direction: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_bar_rpc(\n",
        "        self, reu: float, red: float,\n",
        "        prev_direction: str, new_direction: str\n",
        "    ) -> int:\n",
        "        \"\"\"Calculate the bar's RPC value.\"\"\"\n",
        "        try:\n",
        "            if reu == 0 and red == 0:\n",
        "                return 0\n",
        "            elif reu > 0 and red > 0:\n",
        "                if prev_direction == 'N' or new_direction != prev_direction:\n",
        "                    return 2\n",
        "                return 1\n",
        "            elif new_direction != prev_direction and prev_direction != 'N':\n",
        "                return 1\n",
        "            return 0\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error calculating bar RPC: {e}\")\n",
        "            raise\n",
        "\n",
        "def run_tests():\n",
        "    \"\"\"Run unit tests for the framework.\"\"\"\n",
        "    import unittest\n",
        "\n",
        "    class TestTemporalFramework(unittest.TestCase):\n",
        "        def setUp(self):\n",
        "            self.processor = ConnectTemporalPeriods()\n",
        "\n",
        "        def test_data_validation(self):\n",
        "            # Test missing columns\n",
        "            df_missing = pd.DataFrame({'date': [], 'high': [], 'low': []})\n",
        "            with self.assertRaises(DataValidationError):\n",
        "                self.processor.validate_dataframe(df_missing)\n",
        "\n",
        "            # Test invalid date format\n",
        "            df_bad_date = pd.DataFrame({\n",
        "                'date': ['bad_date'],\n",
        "                'open': [1], 'high': [2], 'low': [1], 'close': [1.5]\n",
        "            })\n",
        "            with self.assertRaises(DataValidationError):\n",
        "                self.processor.validate_dataframe(df_bad_date)\n",
        "\n",
        "            # Test high < low\n",
        "            df_invalid_hl = pd.DataFrame({\n",
        "                'date': ['2024-01-01'],\n",
        "                'open': [1], 'high': [1], 'low': [2], 'close': [1.5]\n",
        "            })\n",
        "            with self.assertRaises(DataValidationError):\n",
        "                self.processor.validate_dataframe(df_invalid_hl)\n",
        "\n",
        "        def test_rpc_calculation(self):\n",
        "            # Test no expansion\n",
        "            self.assertEqual(\n",
        "                self.processor._calculate_bar_rpc(0, 0, 'N', 'N'), 0\n",
        "            )\n",
        "\n",
        "            # Test two-way expansion\n",
        "            self.assertEqual(\n",
        "                self.processor._calculate_bar_rpc(1, 1, 'N', 'U'), 2\n",
        "            )\n",
        "\n",
        "            # Test direction change\n",
        "            self.assertEqual(\n",
        "                self.processor._calculate_bar_rpc(1, 0, 'D', 'U'), 1\n",
        "            )\n",
        "\n",
        "        def test_rpc_direction(self):\n",
        "            # Test no expansion\n",
        "            self.assertEqual(\n",
        "                self.processor._determine_rpc_direction(0, 0, 100, 90, 95, 'U'),\n",
        "                'U'\n",
        "            )\n",
        "\n",
        "            # Test two-way expansion with close in upper half\n",
        "            self.assertEqual(\n",
        "                self.processor._determine_rpc_direction(1, 1, 100, 90, 97, 'N'),\n",
        "                'U'\n",
        "            )\n",
        "\n",
        "            # Test upward expansion only\n",
        "            self.assertEqual(\n",
        "                self.processor._determine_rpc_direction(1, 0, 100, 90, 95, 'D'),\n",
        "                'U'\n",
        "            )\n",
        "\n",
        "    # Run tests\n",
        "    unittest.main(argv=['dummy'])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_tests()\n",
        "    processor = ConnectTemporalPeriods()\n",
        "    processor.process_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "vxVM3sTn8_Q7",
        "outputId": "fb2f525a-8eb8-4bba-ac62-28c2edd13492"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 0 tests in 0.000s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "False",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import logging\n",
        "from typing import Tuple, Dict, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('temporal_processing.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "@dataclass\n",
        "class ParentPeriodStats:\n",
        "    high: float\n",
        "    low: float\n",
        "    duration: int\n",
        "    completed_bars: int\n",
        "    rpc_count: int\n",
        "    bar_of_high: int\n",
        "    bar_of_low: int\n",
        "\n",
        "class DataValidationError(Exception):\n",
        "    pass\n",
        "\n",
        "class ConnectTemporalPeriods:\n",
        "    REQUIRED_COLUMNS = {'date', 'open', 'high', 'low', 'close'}\n",
        "\n",
        "    def __init__(self, child_period: str = \"D\", parent_period: str = \"ME\"):\n",
        "        self.child_period = child_period\n",
        "        self.parent_period = parent_period\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def validate_dataframe(self, df: pd.DataFrame) -> None:\n",
        "        \"\"\"Validate input dataframe structure and content.\"\"\"\n",
        "        missing_cols = self.REQUIRED_COLUMNS - set(df.columns)\n",
        "        if missing_cols:\n",
        "            raise DataValidationError(f\"Missing required columns: {missing_cols}\")\n",
        "\n",
        "        if not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
        "            try:\n",
        "                pd.to_datetime(df['date'])\n",
        "            except Exception as e:\n",
        "                raise DataValidationError(f\"Invalid date format: {e}\")\n",
        "\n",
        "        price_cols = ['open', 'high', 'low', 'close']\n",
        "        for col in price_cols:\n",
        "            if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "                raise DataValidationError(f\"Column {col} must be numeric\")\n",
        "\n",
        "        invalid_rows = df[df['high'] < df['low']].index\n",
        "        if len(invalid_rows) > 0:\n",
        "            raise DataValidationError(f\"Found {len(invalid_rows)} rows where high < low\")\n",
        "\n",
        "    def process_files(self, input_dir: str = \"input/\") -> None:\n",
        "        \"\"\"Process all CSV files in the input directory.\"\"\"\n",
        "        try:\n",
        "            os.makedirs(\"parent_output\", exist_ok=True)\n",
        "            os.makedirs(\"processed_output\", exist_ok=True)\n",
        "\n",
        "            file_count = 0\n",
        "            for filename in os.listdir(input_dir):\n",
        "                if filename.endswith(\".csv\"):\n",
        "                    ticker = filename.replace(\".csv\", \"\")\n",
        "                    self.logger.info(f\"Processing {ticker}\")\n",
        "                    self.process_single_file(os.path.join(input_dir, filename), ticker)\n",
        "                    file_count += 1\n",
        "\n",
        "            self.logger.info(f\"Successfully processed {file_count} files\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing files: {e}\")\n",
        "            raise\n",
        "\n",
        "    def process_single_file(self, filepath: str, ticker: str) -> None:\n",
        "        \"\"\"Process a single ticker's data file.\"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(filepath)\n",
        "            df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "            self.validate_dataframe(df)\n",
        "\n",
        "            df['parent_period'] = df['date'].dt.to_period(self.parent_period)\n",
        "            parent_stats = self._calculate_parent_stats(df)\n",
        "\n",
        "            processed_df = self._process_child_data(df, parent_stats)\n",
        "\n",
        "            self._safe_save_csv(parent_stats, f\"parent_output/{ticker}_parent.csv\")\n",
        "            self._safe_save_csv(processed_df, f\"processed_output/{ticker}_processed.csv\")\n",
        "\n",
        "            self.logger.info(f\"Successfully processed {ticker}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing {ticker}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _safe_save_csv(self, df: pd.DataFrame, filepath: str) -> None:\n",
        "        \"\"\"Safely save DataFrame to CSV with error handling.\"\"\"\n",
        "        try:\n",
        "            df.to_csv(filepath, index=False)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error saving to {filepath}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_parent_stats(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Calculate parent period statistics including bar locations and counts.\"\"\"\n",
        "        try:\n",
        "            # Basic stats\n",
        "            parent_stats = df.groupby('parent_period').agg({\n",
        "                'date': ['first', 'count'],\n",
        "                'high': 'max',\n",
        "                'low': 'min',\n",
        "                'range_expansion_flag': 'sum',\n",
        "                'intra_period_cumulative_rpc': 'max'\n",
        "            }).reset_index()\n",
        "\n",
        "            parent_stats.columns = ['parent_period', 'date', 'duration', 'parent_high',\n",
        "                                  'parent_low', 'expansion_count', 'rpc_count']\n",
        "\n",
        "            # Calculate bar positions for each parent period\n",
        "            for period in df['parent_period'].unique():\n",
        "                period_data = df[df['parent_period'] == period]\n",
        "                period_high = parent_stats.loc[parent_stats['parent_period'] == period, 'parent_high'].iloc[0]\n",
        "                period_low = parent_stats.loc[parent_stats['parent_period'] == period, 'parent_low'].iloc[0]\n",
        "\n",
        "                # Find first occurrence of high and low\n",
        "                bar_of_high = period_data[period_data['high'] == period_high].index.min()\n",
        "                bar_of_low = period_data[period_data['low'] == period_low].index.min()\n",
        "\n",
        "                if bar_of_high is not None:\n",
        "                    parent_stats.loc[parent_stats['parent_period'] == period, 'bar_of_high'] = \\\n",
        "                        period_data.loc[:bar_of_high].notna().sum()\n",
        "                if bar_of_low is not None:\n",
        "                    parent_stats.loc[parent_stats['parent_period'] == period, 'bar_of_low'] = \\\n",
        "                        period_data.loc[:bar_of_low].notna().sum()\n",
        "\n",
        "            return parent_stats\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(\"Error calculating parent stats: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _process_child_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Process child period data with detailed metrics.\"\"\"\n",
        "        try:\n",
        "            df = df.copy()\n",
        "\n",
        "            # Initialize columns with correct dtypes\n",
        "            numeric_cols = {\n",
        "                'intra_period_count': 'int64',\n",
        "                'intra_period_high': 'float64',\n",
        "                'intra_period_low': 'float64',\n",
        "                'intra_period_reu': 'float64',\n",
        "                'intra_period_red': 'float64',\n",
        "                'range_expansion_flag': 'int64',\n",
        "                'bar_rpc': 'int64',\n",
        "                'intra_period_cumulative_rpc': 'int64'\n",
        "            }\n",
        "\n",
        "            for col, dtype in numeric_cols.items():\n",
        "                df[col] = pd.Series(dtype=dtype)\n",
        "\n",
        "            df['rpc_direction'] = 'N'\n",
        "\n",
        "            # Process each parent period\n",
        "            for period in df['parent_period'].unique():\n",
        "                mask = df['parent_period'] == period\n",
        "                period_data = df[mask].copy()\n",
        "\n",
        "                processed_period = self._process_parent_period(period_data)\n",
        "                df.loc[mask] = processed_period.astype(df[mask].dtypes)\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing child data: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _process_parent_period(self, period_data: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Process data within a single parent period.\"\"\"\n",
        "        try:\n",
        "            active_bars = 0\n",
        "            cumulative_rpc = 0\n",
        "            prev_direction = 'N'\n",
        "            prior_intra_high = None\n",
        "            prior_intra_low = None\n",
        "\n",
        "            for idx in period_data.index:\n",
        "                row = period_data.loc[idx]\n",
        "\n",
        "                if pd.isna(row['high']) or pd.isna(row['low']):\n",
        "                    continue\n",
        "\n",
        "                active_bars += 1\n",
        "                period_data.at[idx, 'intra_period_count'] = active_bars\n",
        "\n",
        "                # Calculate range expansions using prior values\n",
        "                reu = max(0, row['high'] - prior_intra_high) if prior_intra_high is not None else 0\n",
        "                red = max(0, prior_intra_low - row['low']) if prior_intra_low is not None else 0\n",
        "\n",
        "                # Update metrics\n",
        "                period_data.at[idx, 'intra_period_reu'] = reu\n",
        "                period_data.at[idx, 'intra_period_red'] = red\n",
        "                period_data.at[idx, 'range_expansion_flag'] = 1 if (reu > 0 or red > 0) else 0\n",
        "\n",
        "                new_direction = self._determine_rpc_direction(\n",
        "                    reu, red, row['high'], row['low'], row['close'], prev_direction\n",
        "                )\n",
        "\n",
        "                if new_direction != prev_direction and prev_direction != 'N':\n",
        "                    cumulative_rpc += 1\n",
        "\n",
        "                period_data.at[idx, 'rpc_direction'] = new_direction\n",
        "                period_data.at[idx, 'intra_period_cumulative_rpc'] = cumulative_rpc\n",
        "                period_data.at[idx, 'bar_rpc'] = self._calculate_bar_rpc(\n",
        "                    reu, red, prev_direction, new_direction\n",
        "                )\n",
        "\n",
        "                # Update prior values for next iteration\n",
        "                prior_intra_high = max(row['high'], prior_intra_high if prior_intra_high is not None else row['high'])\n",
        "                prior_intra_low = min(row['low'], prior_intra_low if prior_intra_low is not None else row['low'])\n",
        "\n",
        "                prev_direction = new_direction\n",
        "\n",
        "            return period_data\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing parent period: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _determine_rpc_direction(\n",
        "        self, reu: float, red: float,\n",
        "        high: float, low: float, close: float,\n",
        "        prev_direction: str\n",
        "    ) -> str:\n",
        "        \"\"\"Determine the RPC direction based on expansions.\"\"\"\n",
        "        try:\n",
        "            if reu == 0 and red == 0:\n",
        "                return prev_direction\n",
        "            elif reu > 0 and red > 0:\n",
        "                # Two-way expansion - use close location\n",
        "                return 'U' if (close - low) / (high - low) >= 0.5 else 'D'\n",
        "            elif reu > 0:\n",
        "                return 'U'\n",
        "            else:\n",
        "                return 'D'\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error determining RPC direction: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_bar_rpc(\n",
        "        self, reu: float, red: float,\n",
        "        prev_direction: str, new_direction: str\n",
        "    ) -> int:\n",
        "        \"\"\"Calculate the bar's RPC value.\"\"\"\n",
        "        try:\n",
        "            if reu == 0 and red == 0:\n",
        "                return 0\n",
        "            elif reu > 0 and red > 0:\n",
        "                if prev_direction == 'N' or new_direction != prev_direction:\n",
        "                    return 2\n",
        "                return 1\n",
        "            elif new_direction != prev_direction and prev_direction != 'N':\n",
        "                return 1\n",
        "            return 0\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error calculating bar RPC: {e}\")\n",
        "            raise\n",
        "\n",
        "class TestTemporalFramework:\n",
        "    def run_all_tests(self):\n",
        "        processor = ConnectTemporalPeriods()\n",
        "\n",
        "        # Test RPC calculation\n",
        "        assert processor._calculate_bar_rpc(0, 0, 'N', 'N') == 0, \"No expansion should return 0\"\n",
        "        assert processor._calculate_bar_rpc(1, 1, 'N', 'U') == 2, \"Two-way expansion should return 2\"\n",
        "        assert processor._calculate_bar_rpc(1, 0, 'D', 'U') == 1, \"Direction change should return 1\"\n",
        "\n",
        "        # Test RPC direction\n",
        "        assert processor._determine_rpc_direction(0, 0, 100, 90, 95, 'U') == 'U', \"No expansion should maintain direction\"\n",
        "        assert processor._determine_rpc_direction(1, 1, 100, 90, 97, 'N') == 'U', \"Close in upper half should return U\"\n",
        "        assert processor._determine_rpc_direction(1, 0, 100, 90, 95, 'D') == 'U', \"Upward expansion should return U\"\n",
        "\n",
        "        print(\"All tests passed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run tests\n",
        "    tests = TestTemporalFramework()\n",
        "    tests.run_all_tests()\n",
        "\n",
        "    # Process files\n",
        "    processor = ConnectTemporalPeriods()\n",
        "    processor.process_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "9EJidKFZX6OV",
        "outputId": "2140b119-aaea-4ccd-bf3f-66508f694afc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error processing MMM: Invalid frequency: ME, failed to parse with error message: ValueError(\"for Period, please use 'M' instead of 'ME'\")\n",
            "ERROR:__main__:Error processing files: Invalid frequency: ME, failed to parse with error message: ValueError(\"for Period, please use 'M' instead of 'ME'\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Invalid frequency: ME, failed to parse with error message: ValueError(\"for Period, please use 'M' instead of 'ME'\")",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32moffsets.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.offsets.to_offset\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: for Period, please use 'M' instead of 'ME'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-2d60e43a1931>\u001b[0m in \u001b[0;36m<cell line: 292>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;31m# Process files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConnectTemporalPeriods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m     \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-2d60e43a1931>\u001b[0m in \u001b[0;36mprocess_files\u001b[0;34m(self, input_dir)\u001b[0m\n\u001b[1;32m     68\u001b[0m                     \u001b[0mticker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Processing {ticker}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_single_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                     \u001b[0mfile_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-2d60e43a1931>\u001b[0m in \u001b[0;36mprocess_single_file\u001b[0;34m(self, filepath, ticker)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parent_period'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_period\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_period\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mparent_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_parent_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/accessor.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_create_delegator_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delegate_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/accessors.py\u001b[0m in \u001b[0;36m_delegate_method\u001b[0;34m(self, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/extension.py\u001b[0m in \u001b[0;36mmethod\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"inplace\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"cannot use inplace with {type(self).__name__}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/arrays/datetimes.py\u001b[0m in \u001b[0;36mto_period\u001b[0;34m(self, freq)\u001b[0m\n\u001b[1;32m   1247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0mfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mPeriodArray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_datetime64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m     \u001b[0;31m# -----------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/arrays/period.py\u001b[0m in \u001b[0;36m_from_datetime64\u001b[0;34m(cls, data, freq, tz)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseOffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfreq_to_period_freqstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt64arr_to_periodarr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPeriodDtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/arrays/period.py\u001b[0m in \u001b[0;36mdt64arr_to_periodarr\u001b[0;34m(data, freq, tz)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m     \u001b[0mreso\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_unit_from_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m     \u001b[0mfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPeriod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_convert_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m     \u001b[0mbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_period_dtype_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mc_dt64arr_to_periodarr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreso\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreso\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mperiod.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.period._Period._maybe_convert_freq\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32moffsets.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.offsets.to_offset\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid frequency: ME, failed to parse with error message: ValueError(\"for Period, please use 'M' instead of 'ME'\")"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import logging\n",
        "from typing import Tuple, Dict, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('temporal_processing.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "@dataclass\n",
        "class ParentPeriodStats:\n",
        "    high: float\n",
        "    low: float\n",
        "    duration: int\n",
        "    completed_bars: int\n",
        "    rpc_count: int\n",
        "    bar_of_high: int\n",
        "    bar_of_low: int\n",
        "\n",
        "class DataValidationError(Exception):\n",
        "    pass\n",
        "\n",
        "class ConnectTemporalPeriods:\n",
        "    REQUIRED_COLUMNS = {'date', 'open', 'high', 'low', 'close'}\n",
        "\n",
        "    def __init__(self, child_period: str = \"D\", parent_period: str = \"M\"):\n",
        "        self.child_period = child_period\n",
        "        self.parent_period = parent_period\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def validate_dataframe(self, df: pd.DataFrame) -> None:\n",
        "        \"\"\"Validate input dataframe structure and content.\"\"\"\n",
        "        missing_cols = self.REQUIRED_COLUMNS - set(df.columns)\n",
        "        if missing_cols:\n",
        "            raise DataValidationError(f\"Missing required columns: {missing_cols}\")\n",
        "\n",
        "        if not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
        "            try:\n",
        "                pd.to_datetime(df['date'])\n",
        "            except Exception as e:\n",
        "                raise DataValidationError(f\"Invalid date format: {e}\")\n",
        "\n",
        "        price_cols = ['open', 'high', 'low', 'close']\n",
        "        for col in price_cols:\n",
        "            if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "                raise DataValidationError(f\"Column {col} must be numeric\")\n",
        "\n",
        "        invalid_rows = df[df['high'] < df['low']].index\n",
        "        if len(invalid_rows) > 0:\n",
        "            raise DataValidationError(f\"Found {len(invalid_rows)} rows where high < low\")\n",
        "\n",
        "    def process_files(self, input_dir: str = \"input/\") -> None:\n",
        "        \"\"\"Process all CSV files in the input directory.\"\"\"\n",
        "        try:\n",
        "            os.makedirs(\"parent_output\", exist_ok=True)\n",
        "            os.makedirs(\"processed_output\", exist_ok=True)\n",
        "\n",
        "            file_count = 0\n",
        "            for filename in os.listdir(input_dir):\n",
        "                if filename.endswith(\".csv\"):\n",
        "                    ticker = filename.replace(\".csv\", \"\")\n",
        "                    self.logger.info(f\"Processing {ticker}\")\n",
        "                    self.process_single_file(os.path.join(input_dir, filename), ticker)\n",
        "                    file_count += 1\n",
        "\n",
        "            self.logger.info(f\"Successfully processed {file_count} files\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing files: {e}\")\n",
        "            raise\n",
        "\n",
        "    def process_single_file(self, filepath: str, ticker: str) -> None:\n",
        "        \"\"\"Process a single ticker's data file.\"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(filepath)\n",
        "            df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "            self.validate_dataframe(df)\n",
        "\n",
        "            df['parent_period'] = df['date'].dt.to_period(self.parent_period)\n",
        "            parent_stats = self._calculate_parent_stats(df)\n",
        "\n",
        "            processed_df = self._process_child_data(df, parent_stats)\n",
        "\n",
        "            self._safe_save_csv(parent_stats, f\"parent_output/{ticker}_parent.csv\")\n",
        "            self._safe_save_csv(processed_df, f\"processed_output/{ticker}_processed.csv\")\n",
        "\n",
        "            self.logger.info(f\"Successfully processed {ticker}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing {ticker}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _safe_save_csv(self, df: pd.DataFrame, filepath: str) -> None:\n",
        "        \"\"\"Safely save DataFrame to CSV with error handling.\"\"\"\n",
        "        try:\n",
        "            df.to_csv(filepath, index=False)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error saving to {filepath}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_parent_stats(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Calculate parent period statistics including bar locations and counts.\"\"\"\n",
        "        try:\n",
        "            # Basic stats\n",
        "            parent_stats = df.groupby('parent_period').agg({\n",
        "                'date': ['first', 'count'],\n",
        "                'high': 'max',\n",
        "                'low': 'min',\n",
        "                'range_expansion_flag': 'sum',\n",
        "                'intra_period_cumulative_rpc': 'max'\n",
        "            }).reset_index()\n",
        "\n",
        "            parent_stats.columns = ['parent_period', 'date', 'duration', 'parent_high',\n",
        "                                  'parent_low', 'expansion_count', 'rpc_count']\n",
        "\n",
        "            # Calculate bar positions for each parent period\n",
        "            for period in df['parent_period'].unique():\n",
        "                period_data = df[df['parent_period'] == period]\n",
        "                period_high = parent_stats.loc[parent_stats['parent_period'] == period, 'parent_high'].iloc[0]\n",
        "                period_low = parent_stats.loc[parent_stats['parent_period'] == period, 'parent_low'].iloc[0]\n",
        "\n",
        "                # Find first occurrence of high and low\n",
        "                bar_of_high = period_data[period_data['high'] == period_high].index.min()\n",
        "                bar_of_low = period_data[period_data['low'] == period_low].index.min()\n",
        "\n",
        "                if bar_of_high is not None:\n",
        "                    parent_stats.loc[parent_stats['parent_period'] == period, 'bar_of_high'] = \\\n",
        "                        period_data.loc[:bar_of_high].notna().sum()\n",
        "                if bar_of_low is not None:\n",
        "                    parent_stats.loc[parent_stats['parent_period'] == period, 'bar_of_low'] = \\\n",
        "                        period_data.loc[:bar_of_low].notna().sum()\n",
        "\n",
        "            return parent_stats\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(\"Error calculating parent stats: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _process_child_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Process child period data with detailed metrics.\"\"\"\n",
        "        try:\n",
        "            df = df.copy()\n",
        "\n",
        "            # Initialize columns with correct dtypes\n",
        "            numeric_cols = {\n",
        "                'intra_period_count': 'int64',\n",
        "                'intra_period_high': 'float64',\n",
        "                'intra_period_low': 'float64',\n",
        "                'intra_period_reu': 'float64',\n",
        "                'intra_period_red': 'float64',\n",
        "                'range_expansion_flag': 'int64',\n",
        "                'bar_rpc': 'int64',\n",
        "                'intra_period_cumulative_rpc': 'int64'\n",
        "            }\n",
        "\n",
        "            for col, dtype in numeric_cols.items():\n",
        "                df[col] = pd.Series(dtype=dtype)\n",
        "\n",
        "            df['rpc_direction'] = 'N'\n",
        "\n",
        "            # Process each parent period\n",
        "            for period in df['parent_period'].unique():\n",
        "                mask = df['parent_period'] == period\n",
        "                period_data = df[mask].copy()\n",
        "\n",
        "                processed_period = self._process_parent_period(period_data)\n",
        "                df.loc[mask] = processed_period.astype(df[mask].dtypes)\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing child data: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _process_parent_period(self, period_data: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Process data within a single parent period.\"\"\"\n",
        "        try:\n",
        "            active_bars = 0\n",
        "            cumulative_rpc = 0\n",
        "            prev_direction = 'N'\n",
        "            prior_intra_high = None\n",
        "            prior_intra_low = None\n",
        "\n",
        "            for idx in period_data.index:\n",
        "                row = period_data.loc[idx]\n",
        "\n",
        "                if pd.isna(row['high']) or pd.isna(row['low']):\n",
        "                    continue\n",
        "\n",
        "                active_bars += 1\n",
        "                period_data.at[idx, 'intra_period_count'] = active_bars\n",
        "\n",
        "                # Calculate range expansions using prior values\n",
        "                reu = max(0, row['high'] - prior_intra_high) if prior_intra_high is not None else 0\n",
        "                red = max(0, prior_intra_low - row['low']) if prior_intra_low is not None else 0\n",
        "\n",
        "                # Update metrics\n",
        "                period_data.at[idx, 'intra_period_reu'] = reu\n",
        "                period_data.at[idx, 'intra_period_red'] = red\n",
        "                period_data.at[idx, 'range_expansion_flag'] = 1 if (reu > 0 or red > 0) else 0\n",
        "\n",
        "                new_direction = self._determine_rpc_direction(\n",
        "                    reu, red, row['high'], row['low'], row['close'], prev_direction\n",
        "                )\n",
        "\n",
        "                if new_direction != prev_direction and prev_direction != 'N':\n",
        "                    cumulative_rpc += 1\n",
        "\n",
        "                period_data.at[idx, 'rpc_direction'] = new_direction\n",
        "                period_data.at[idx, 'intra_period_cumulative_rpc'] = cumulative_rpc\n",
        "                period_data.at[idx, 'bar_rpc'] = self._calculate_bar_rpc(\n",
        "                    reu, red, prev_direction, new_direction\n",
        "                )\n",
        "\n",
        "                # Update prior values for next iteration\n",
        "                prior_intra_high = max(row['high'], prior_intra_high if prior_intra_high is not None else row['high'])\n",
        "                prior_intra_low = min(row['low'], prior_intra_low if prior_intra_low is not None else row['low'])\n",
        "\n",
        "                prev_direction = new_direction\n",
        "\n",
        "            return period_data\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing parent period: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _determine_rpc_direction(\n",
        "        self, reu: float, red: float,\n",
        "        high: float, low: float, close: float,\n",
        "        prev_direction: str\n",
        "    ) -> str:\n",
        "        \"\"\"Determine the RPC direction based on expansions.\"\"\"\n",
        "        try:\n",
        "            if reu == 0 and red == 0:\n",
        "                return prev_direction\n",
        "            elif reu > 0 and red > 0:\n",
        "                # Two-way expansion - use close location\n",
        "                return 'U' if (close - low) / (high - low) >= 0.5 else 'D'\n",
        "            elif reu > 0:\n",
        "                return 'U'\n",
        "            else:\n",
        "                return 'D'\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error determining RPC direction: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_bar_rpc(\n",
        "        self, reu: float, red: float,\n",
        "        prev_direction: str, new_direction: str\n",
        "    ) -> int:\n",
        "        \"\"\"Calculate the bar's RPC value.\"\"\"\n",
        "        try:\n",
        "            if reu == 0 and red == 0:\n",
        "                return 0\n",
        "            elif reu > 0 and red > 0:\n",
        "                if prev_direction == 'N' or new_direction != prev_direction:\n",
        "                    return 2\n",
        "                return 1\n",
        "            elif new_direction != prev_direction and prev_direction != 'N':\n",
        "                return 1\n",
        "            return 0\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error calculating bar RPC: {e}\")\n",
        "            raise\n",
        "\n",
        "class TestTemporalFramework:\n",
        "    def run_all_tests(self):\n",
        "        processor = ConnectTemporalPeriods()\n",
        "\n",
        "        # Test RPC calculation\n",
        "        assert processor._calculate_bar_rpc(0, 0, 'N', 'N') == 0, \"No expansion should return 0\"\n",
        "        assert processor._calculate_bar_rpc(1, 1, 'N', 'U') == 2, \"Two-way expansion should return 2\"\n",
        "        assert processor._calculate_bar_rpc(1, 0, 'D', 'U') == 1, \"Direction change should return 1\"\n",
        "\n",
        "        # Test RPC direction\n",
        "        assert processor._determine_rpc_direction(0, 0, 100, 90, 95, 'U') == 'U', \"No expansion should maintain direction\"\n",
        "        assert processor._determine_rpc_direction(1, 1, 100, 90, 97, 'N') == 'U', \"Close in upper half should return U\"\n",
        "        assert processor._determine_rpc_direction(1, 0, 100, 90, 95, 'D') == 'U', \"Upward expansion should return U\"\n",
        "\n",
        "        print(\"All tests passed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run tests\n",
        "    tests = TestTemporalFramework()\n",
        "    tests.run_all_tests()\n",
        "\n",
        "    # Process files\n",
        "    processor = ConnectTemporalPeriods()\n",
        "    processor.process_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "YFvPooHMYF-z",
        "outputId": "00274d53-4a96-43cd-f2ef-8f78fc3be25c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error calculating parent stats: {e}\n",
            "ERROR:__main__:Error processing MMM: \"Column(s) ['intra_period_cumulative_rpc', 'range_expansion_flag'] do not exist\"\n",
            "ERROR:__main__:Error processing files: \"Column(s) ['intra_period_cumulative_rpc', 'range_expansion_flag'] do not exist\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"Column(s) ['intra_period_cumulative_rpc', 'range_expansion_flag'] do not exist\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-792542c0e126>\u001b[0m in \u001b[0;36m<cell line: 292>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;31m# Process files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConnectTemporalPeriods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m     \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-792542c0e126>\u001b[0m in \u001b[0;36mprocess_files\u001b[0;34m(self, input_dir)\u001b[0m\n\u001b[1;32m     68\u001b[0m                     \u001b[0mticker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Processing {ticker}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_single_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                     \u001b[0mfile_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-792542c0e126>\u001b[0m in \u001b[0;36mprocess_single_file\u001b[0;34m(self, filepath, ticker)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parent_period'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_period\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_period\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mparent_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_parent_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mprocessed_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_child_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-792542c0e126>\u001b[0m in \u001b[0;36m_calculate_parent_stats\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;31m# Basic stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             parent_stats = df.groupby('parent_period').agg({\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0;34m'date'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'first'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;34m'high'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'max'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGroupByApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1432\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m             \u001b[0;31m# GH #52849\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36magg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;31m# we require a list, but not a 'str'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36magg_dict_like\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mResult\u001b[0m \u001b[0mof\u001b[0m \u001b[0maggregation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \"\"\"\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg_or_apply_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"agg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     def compute_dict_like(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36magg_or_apply_dict_like\u001b[0;34m(self, op_name)\u001b[0m\n\u001b[1;32m   1606\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"as_index\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"as_index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         ):\n\u001b[0;32m-> 1608\u001b[0;31m             result_index, result_data = self.compute_dict_like(\n\u001b[0m\u001b[1;32m   1609\u001b[0m                 \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mcompute_dict_like\u001b[0;34m(self, op_name, selected_obj, selection, kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0mis_groupby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDataFrameGroupBy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeriesGroupBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAggFuncTypeDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_dictlike_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         is_non_unique_col = (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mnormalize_dictlike_arg\u001b[0;34m(self, how, obj, func)\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Column(s) {list(cols)} do not exist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0maggregator_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Column(s) ['intra_period_cumulative_rpc', 'range_expansion_flag'] do not exist\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import logging\n",
        "from typing import Tuple, Dict, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('temporal_processing.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "@dataclass\n",
        "class ParentPeriodStats:\n",
        "    high: float\n",
        "    low: float\n",
        "    duration: int\n",
        "    completed_bars: int\n",
        "    rpc_count: int\n",
        "    bar_of_high: int\n",
        "    bar_of_low: int\n",
        "\n",
        "class DataValidationError(Exception):\n",
        "    pass\n",
        "\n",
        "class ConnectTemporalPeriods:\n",
        "    REQUIRED_COLUMNS = {'date', 'open', 'high', 'low', 'close'}\n",
        "\n",
        "    def __init__(self, child_period: str = \"D\", parent_period: str = \"M\"):\n",
        "        self.child_period = child_period\n",
        "        self.parent_period = parent_period\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def validate_dataframe(self, df: pd.DataFrame) -> None:\n",
        "        \"\"\"Validate input dataframe structure and content.\"\"\"\n",
        "        missing_cols = self.REQUIRED_COLUMNS - set(df.columns)\n",
        "        if missing_cols:\n",
        "            raise DataValidationError(f\"Missing required columns: {missing_cols}\")\n",
        "\n",
        "        if not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
        "            try:\n",
        "                pd.to_datetime(df['date'])\n",
        "            except Exception as e:\n",
        "                raise DataValidationError(f\"Invalid date format: {e}\")\n",
        "\n",
        "        price_cols = ['open', 'high', 'low', 'close']\n",
        "        for col in price_cols:\n",
        "            if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "                raise DataValidationError(f\"Column {col} must be numeric\")\n",
        "\n",
        "        invalid_rows = df[df['high'] < df['low']].index\n",
        "        if len(invalid_rows) > 0:\n",
        "            raise DataValidationError(f\"Found {len(invalid_rows)} rows where high < low\")\n",
        "\n",
        "    def process_files(self, input_dir: str = \"input/\") -> None:\n",
        "        \"\"\"Process all CSV files in the input directory.\"\"\"\n",
        "        try:\n",
        "            os.makedirs(\"parent_output\", exist_ok=True)\n",
        "            os.makedirs(\"processed_output\", exist_ok=True)\n",
        "\n",
        "            file_count = 0\n",
        "            for filename in os.listdir(input_dir):\n",
        "                if filename.endswith(\".csv\"):\n",
        "                    ticker = filename.replace(\".csv\", \"\")\n",
        "                    self.logger.info(f\"Processing {ticker}\")\n",
        "                    self.process_single_file(os.path.join(input_dir, filename), ticker)\n",
        "                    file_count += 1\n",
        "\n",
        "            self.logger.info(f\"Successfully processed {file_count} files\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing files: {e}\")\n",
        "            raise\n",
        "\n",
        "    def process_single_file(self, filepath: str, ticker: str) -> None:\n",
        "        \"\"\"Process a single ticker's data file.\"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(filepath)\n",
        "            df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "            self.validate_dataframe(df)\n",
        "\n",
        "            df['parent_period'] = df['date'].dt.to_period(self.parent_period)\n",
        "            parent_stats = self._calculate_parent_stats(df)\n",
        "\n",
        "            processed_df = self._process_child_data(df, parent_stats)\n",
        "\n",
        "            self._safe_save_csv(parent_stats, f\"parent_output/{ticker}_parent.csv\")\n",
        "            self._safe_save_csv(processed_df, f\"processed_output/{ticker}_processed.csv\")\n",
        "\n",
        "            self.logger.info(f\"Successfully processed {ticker}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing {ticker}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _safe_save_csv(self, df: pd.DataFrame, filepath: str) -> None:\n",
        "        \"\"\"Safely save DataFrame to CSV with error handling.\"\"\"\n",
        "        try:\n",
        "            df.to_csv(filepath, index=False)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error saving to {filepath}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_parent_stats(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Calculate parent period statistics including bar locations and counts.\"\"\"\n",
        "        try:\n",
        "            # Basic stats\n",
        "            parent_stats = df.groupby('parent_period').agg({\n",
        "                'date': ['first', 'count'],\n",
        "                'high': 'max',\n",
        "                'low': 'min',\n",
        "                'range_expansion_flag': 'sum',\n",
        "                'intra_period_cumulative_rpc': 'max'\n",
        "            }).reset_index()\n",
        "\n",
        "            parent_stats.columns = ['parent_period', 'date', 'duration', 'parent_high',\n",
        "                                  'parent_low', 'expansion_count', 'rpc_count']\n",
        "\n",
        "            # Calculate bar positions for each parent period\n",
        "            for period in df['parent_period'].unique():\n",
        "                period_data = df[df['parent_period'] == period]\n",
        "                period_high = parent_stats.loc[parent_stats['parent_period'] == period, 'parent_high'].iloc[0]\n",
        "                period_low = parent_stats.loc[parent_stats['parent_period'] == period, 'parent_low'].iloc[0]\n",
        "\n",
        "                # Find first occurrence of high and low\n",
        "                bar_of_high = period_data[period_data['high'] == period_high].index.min()\n",
        "                bar_of_low = period_data[period_data['low'] == period_low].index.min()\n",
        "\n",
        "                if bar_of_high is not None:\n",
        "                    parent_stats.loc[parent_stats['parent_period'] == period, 'bar_of_high'] = \\\n",
        "                        period_data.loc[:bar_of_high].notna().sum()\n",
        "                if bar_of_low is not None:\n",
        "                    parent_stats.loc[parent_stats['parent_period'] == period, 'bar_of_low'] = \\\n",
        "                        period_data.loc[:bar_of_low].notna().sum()\n",
        "\n",
        "            return parent_stats\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(\"Error calculating parent stats: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _process_child_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Process child period data with detailed metrics.\"\"\"\n",
        "        try:\n",
        "            df = df.copy()\n",
        "\n",
        "            # Initialize columns with correct dtypes\n",
        "            numeric_cols = {\n",
        "                'intra_period_count': 'int64',\n",
        "                'intra_period_high': 'float64',\n",
        "                'intra_period_low': 'float64',\n",
        "                'intra_period_reu': 'float64',\n",
        "                'intra_period_red': 'float64',\n",
        "                'range_expansion_flag': 'int64',\n",
        "                'bar_rpc': 'int64',\n",
        "                'intra_period_cumulative_rpc': 'int64'\n",
        "            }\n",
        "\n",
        "            for col, dtype in numeric_cols.items():\n",
        "                df[col] = pd.Series(dtype=dtype)\n",
        "\n",
        "            df['rpc_direction'] = 'N'\n",
        "\n",
        "            # Process each parent period\n",
        "            for period in df['parent_period'].unique():\n",
        "                mask = df['parent_period'] == period\n",
        "                period_data = df[mask].copy()\n",
        "\n",
        "                processed_period = self._process_parent_period(period_data)\n",
        "                df.loc[mask] = processed_period.astype(df[mask].dtypes)\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing child data: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _process_parent_period(self, period_data: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Process data within a single parent period.\"\"\"\n",
        "        try:\n",
        "            active_bars = 0\n",
        "            cumulative_rpc = 0\n",
        "            prev_direction = 'N'\n",
        "            prior_intra_high = None\n",
        "            prior_intra_low = None\n",
        "\n",
        "            for idx in period_data.index:\n",
        "                row = period_data.loc[idx]\n",
        "\n",
        "                if pd.isna(row['high']) or pd.isna(row['low']):\n",
        "                    continue\n",
        "\n",
        "                active_bars += 1\n",
        "                period_data.at[idx, 'intra_period_count'] = active_bars\n",
        "\n",
        "                # Calculate range expansions using prior values\n",
        "                reu = max(0, row['high'] - prior_intra_high) if prior_intra_high is not None else 0\n",
        "                red = max(0, prior_intra_low - row['low']) if prior_intra_low is not None else 0\n",
        "\n",
        "                # Update metrics\n",
        "                period_data.at[idx, 'intra_period_reu'] = reu\n",
        "                period_data.at[idx, 'intra_period_red'] = red\n",
        "                period_data.at[idx, 'range_expansion_flag'] = 1 if (reu > 0 or red > 0) else 0\n",
        "\n",
        "                new_direction = self._determine_rpc_direction(\n",
        "                    reu, red, row['high'], row['low'], row['close'], prev_direction\n",
        "                )\n",
        "\n",
        "                if new_direction != prev_direction and prev_direction != 'N':\n",
        "                    cumulative_rpc += 1\n",
        "\n",
        "                period_data.at[idx, 'rpc_direction'] = new_direction\n",
        "                period_data.at[idx, 'intra_period_cumulative_rpc'] = cumulative_rpc\n",
        "                period_data.at[idx, 'bar_rpc'] = self._calculate_bar_rpc(\n",
        "                    reu, red, prev_direction, new_direction\n",
        "                )\n",
        "\n",
        "                # Update prior values for next iteration\n",
        "                prior_intra_high = max(row['high'], prior_intra_high if prior_intra_high is not None else row['high'])\n",
        "                prior_intra_low = min(row['low'], prior_intra_low if prior_intra_low is not None else row['low'])\n",
        "\n",
        "                prev_direction = new_direction\n",
        "\n",
        "            return period_data\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing parent period: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _determine_rpc_direction(\n",
        "        self, reu: float, red: float,\n",
        "        high: float, low: float, close: float,\n",
        "        prev_direction: str\n",
        "    ) -> str:\n",
        "        \"\"\"Determine the RPC direction based on expansions.\"\"\"\n",
        "        try:\n",
        "            if reu == 0 and red == 0:\n",
        "                return prev_direction\n",
        "            elif reu > 0 and red > 0:\n",
        "                # Two-way expansion - use close location\n",
        "                return 'U' if (close - low) / (high - low) >= 0.5 else 'D'\n",
        "            elif reu > 0:\n",
        "                return 'U'\n",
        "            else:\n",
        "                return 'D'\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error determining RPC direction: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_bar_rpc(\n",
        "        self, reu: float, red: float,\n",
        "        prev_direction: str, new_direction: str\n",
        "    ) -> int:\n",
        "        \"\"\"Calculate the bar's RPC value.\"\"\"\n",
        "        try:\n",
        "            if reu == 0 and red == 0:\n",
        "                return 0\n",
        "            elif reu > 0 and red > 0:\n",
        "                if prev_direction == 'N' or new_direction != prev_direction:\n",
        "                    return 2\n",
        "                return 1\n",
        "            elif new_direction != prev_direction and prev_direction != 'N':\n",
        "                return 1\n",
        "            return 0\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error calculating bar RPC: {e}\")\n",
        "            raise\n",
        "\n",
        "class TestTemporalFramework:\n",
        "    def run_all_tests(self):\n",
        "        processor = ConnectTemporalPeriods()\n",
        "\n",
        "        # Test RPC calculation\n",
        "        assert processor._calculate_bar_rpc(0, 0, 'N', 'N') == 0, \"No expansion should return 0\"\n",
        "        assert processor._calculate_bar_rpc(1, 1, 'N', 'U') == 2, \"Two-way expansion should return 2\"\n",
        "        assert processor._calculate_bar_rpc(1, 0, 'D', 'U') == 1, \"Direction change should return 1\"\n",
        "\n",
        "        # Test RPC direction\n",
        "        assert processor._determine_rpc_direction(0, 0, 100, 90, 95, 'U') == 'U', \"No expansion should maintain direction\"\n",
        "        assert processor._determine_rpc_direction(1, 1, 100, 90, 97, 'N') == 'U', \"Close in upper half should return U\"\n",
        "        assert processor._determine_rpc_direction(1, 0, 100, 90, 95, 'D') == 'U', \"Upward expansion should return U\"\n",
        "\n",
        "        print(\"All tests passed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run tests\n",
        "    tests = TestTemporalFramework()\n",
        "    tests.run_all_tests()\n",
        "\n",
        "    # Process files\n",
        "    processor = ConnectTemporalPeriods()\n",
        "    processor.process_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "IVN7RTUpYZXM",
        "outputId": "1160706f-5839-44a5-f66e-7355b6ee8803"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error calculating parent stats: {e}\n",
            "ERROR:__main__:Error processing MMM: \"Column(s) ['intra_period_cumulative_rpc', 'range_expansion_flag'] do not exist\"\n",
            "ERROR:__main__:Error processing files: \"Column(s) ['intra_period_cumulative_rpc', 'range_expansion_flag'] do not exist\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"Column(s) ['intra_period_cumulative_rpc', 'range_expansion_flag'] do not exist\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-792542c0e126>\u001b[0m in \u001b[0;36m<cell line: 292>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;31m# Process files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConnectTemporalPeriods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m     \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-792542c0e126>\u001b[0m in \u001b[0;36mprocess_files\u001b[0;34m(self, input_dir)\u001b[0m\n\u001b[1;32m     68\u001b[0m                     \u001b[0mticker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Processing {ticker}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_single_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                     \u001b[0mfile_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-792542c0e126>\u001b[0m in \u001b[0;36mprocess_single_file\u001b[0;34m(self, filepath, ticker)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parent_period'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_period\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_period\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mparent_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_parent_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mprocessed_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_child_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-792542c0e126>\u001b[0m in \u001b[0;36m_calculate_parent_stats\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;31m# Basic stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             parent_stats = df.groupby('parent_period').agg({\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0;34m'date'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'first'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;34m'high'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'max'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGroupByApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1432\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m             \u001b[0;31m# GH #52849\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36magg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;31m# we require a list, but not a 'str'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36magg_dict_like\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mResult\u001b[0m \u001b[0mof\u001b[0m \u001b[0maggregation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \"\"\"\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg_or_apply_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"agg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     def compute_dict_like(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36magg_or_apply_dict_like\u001b[0;34m(self, op_name)\u001b[0m\n\u001b[1;32m   1606\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"as_index\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"as_index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         ):\n\u001b[0;32m-> 1608\u001b[0;31m             result_index, result_data = self.compute_dict_like(\n\u001b[0m\u001b[1;32m   1609\u001b[0m                 \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mcompute_dict_like\u001b[0;34m(self, op_name, selected_obj, selection, kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0mis_groupby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDataFrameGroupBy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeriesGroupBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAggFuncTypeDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_dictlike_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         is_non_unique_col = (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mnormalize_dictlike_arg\u001b[0;34m(self, how, obj, func)\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Column(s) {list(cols)} do not exist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0maggregator_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Column(s) ['intra_period_cumulative_rpc', 'range_expansion_flag'] do not exist\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import logging\n",
        "from typing import Tuple, Dict, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('temporal_processing.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "@dataclass\n",
        "class ParentPeriodStats:\n",
        "    high: float\n",
        "    low: float\n",
        "    duration: int\n",
        "    completed_bars: int\n",
        "    rpc_count: int\n",
        "    bar_of_high: int\n",
        "    bar_of_low: int\n",
        "\n",
        "class DataValidationError(Exception):\n",
        "    pass\n",
        "\n",
        "class ConnectTemporalPeriods:\n",
        "    REQUIRED_COLUMNS = {'date', 'open', 'high', 'low', 'close'}\n",
        "\n",
        "    def __init__(self, child_period: str = \"D\", parent_period: str = \"M\"):\n",
        "        self.child_period = child_period\n",
        "        self.parent_period = parent_period\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def validate_dataframe(self, df: pd.DataFrame) -> None:\n",
        "        \"\"\"Validate input dataframe structure and content.\"\"\"\n",
        "        missing_cols = self.REQUIRED_COLUMNS - set(df.columns)\n",
        "        if missing_cols:\n",
        "            raise DataValidationError(f\"Missing required columns: {missing_cols}\")\n",
        "\n",
        "        if not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
        "            try:\n",
        "                pd.to_datetime(df['date'])\n",
        "            except Exception as e:\n",
        "                raise DataValidationError(f\"Invalid date format: {e}\")\n",
        "\n",
        "        price_cols = ['open', 'high', 'low', 'close']\n",
        "        for col in price_cols:\n",
        "            if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "                raise DataValidationError(f\"Column {col} must be numeric\")\n",
        "\n",
        "        invalid_rows = df[df['high'] < df['low']].index\n",
        "        if len(invalid_rows) > 0:\n",
        "            raise DataValidationError(f\"Found {len(invalid_rows)} rows where high < low\")\n",
        "\n",
        "    def process_files(self, input_dir: str = \"input/\") -> None:\n",
        "        \"\"\"Process all CSV files in the input directory.\"\"\"\n",
        "        try:\n",
        "            os.makedirs(\"parent_output\", exist_ok=True)\n",
        "            os.makedirs(\"processed_output\", exist_ok=True)\n",
        "\n",
        "            file_count = 0\n",
        "            for filename in os.listdir(input_dir):\n",
        "                if filename.endswith(\".csv\"):\n",
        "                    ticker = filename.replace(\".csv\", \"\")\n",
        "                    self.logger.info(f\"Processing {ticker}\")\n",
        "                    self.process_single_file(os.path.join(input_dir, filename), ticker)\n",
        "                    file_count += 1\n",
        "\n",
        "            self.logger.info(f\"Successfully processed {file_count} files\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing files: {e}\")\n",
        "            raise\n",
        "\n",
        "    def process_single_file(self, filepath: str, ticker: str) -> None:\n",
        "        \"\"\"Process a single ticker's data file.\"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(filepath)\n",
        "            df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "            self.validate_dataframe(df)\n",
        "\n",
        "            df['parent_period'] = df['date'].dt.to_period(self.parent_period)\n",
        "            parent_stats = self._calculate_parent_stats(df)\n",
        "\n",
        "            processed_df = self._process_child_data(df, parent_stats)\n",
        "\n",
        "            self._safe_save_csv(parent_stats, f\"parent_output/{ticker}_parent.csv\")\n",
        "            self._safe_save_csv(processed_df, f\"processed_output/{ticker}_processed.csv\")\n",
        "\n",
        "            self.logger.info(f\"Successfully processed {ticker}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing {ticker}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _safe_save_csv(self, df: pd.DataFrame, filepath: str) -> None:\n",
        "        \"\"\"Safely save DataFrame to CSV with error handling.\"\"\"\n",
        "        try:\n",
        "            df.to_csv(filepath, index=False)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error saving to {filepath}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_parent_stats(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Calculate parent period statistics including bar locations and counts.\"\"\"\n",
        "        try:\n",
        "            # Basic stats\n",
        "            parent_stats = df.groupby('parent_period').agg({\n",
        "                'date': ['first', 'count'],\n",
        "                'high': 'max',\n",
        "                'low': 'min',\n",
        "                'range_expansion_flag': 'sum',\n",
        "                'intra_period_cumulative_rpc': 'max'\n",
        "            }).reset_index()\n",
        "\n",
        "            parent_stats.columns = ['parent_period', 'date', 'duration', 'parent_high',\n",
        "                                  'parent_low', 'expansion_count', 'rpc_count']\n",
        "\n",
        "            # Calculate bar positions for each parent period\n",
        "            for period in df['parent_period'].unique():\n",
        "                period_data = df[df['parent_period'] == period]\n",
        "                period_high = parent_stats.loc[parent_stats['parent_period'] == period, 'parent_high'].iloc[0]\n",
        "                period_low = parent_stats.loc[parent_stats['parent_period'] == period, 'parent_low'].iloc[0]\n",
        "\n",
        "                # Find first occurrence of high and low\n",
        "                bar_of_high = period_data[period_data['high'] == period_high].index.min()\n",
        "                bar_of_low = period_data[period_data['low'] == period_low].index.min()\n",
        "\n",
        "                if bar_of_high is not None:\n",
        "                    parent_stats.loc[parent_stats['parent_period'] == period, 'bar_of_high'] = \\\n",
        "                        period_data.loc[:bar_of_high].notna().sum()\n",
        "                if bar_of_low is not None:\n",
        "                    parent_stats.loc[parent_stats['parent_period'] == period, 'bar_of_low'] = \\\n",
        "                        period_data.loc[:bar_of_low].notna().sum()\n",
        "\n",
        "            return parent_stats\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(\"Error calculating parent stats: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _process_child_data(self, df: pd.DataFrame, parent_stats: pd.DataFrame = None) -> pd.DataFrame:\n",
        "        \"\"\"Process child period data with detailed metrics.\"\"\"\n",
        "        try:\n",
        "            df = df.copy()\n",
        "\n",
        "            # Initialize columns with correct dtypes\n",
        "            numeric_cols = {\n",
        "                'intra_period_count': 'int64',\n",
        "                'intra_period_high': 'float64',\n",
        "                'intra_period_low': 'float64',\n",
        "                'intra_period_reu': 'float64',\n",
        "                'intra_period_red': 'float64',\n",
        "                'range_expansion_flag': 'int64',\n",
        "                'bar_rpc': 'int64',\n",
        "                'intra_period_cumulative_rpc': 'int64'\n",
        "            }\n",
        "\n",
        "            for col, dtype in numeric_cols.items():\n",
        "                df[col] = pd.Series(dtype=dtype)\n",
        "\n",
        "            df['rpc_direction'] = 'N'\n",
        "\n",
        "            # Process each parent period\n",
        "            for period in df['parent_period'].unique():\n",
        "                mask = df['parent_period'] == period\n",
        "                period_data = df[mask].copy()\n",
        "\n",
        "                processed_period = self._process_parent_period(period_data)\n",
        "                df.loc[mask] = processed_period.astype(df[mask].dtypes)\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing child data: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _process_parent_period(self, period_data: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Process data within a single parent period.\"\"\"\n",
        "        try:\n",
        "            active_bars = 0\n",
        "            cumulative_rpc = 0\n",
        "            prev_direction = 'N'\n",
        "            prior_intra_high = None\n",
        "            prior_intra_low = None\n",
        "\n",
        "            for idx in period_data.index:\n",
        "                row = period_data.loc[idx]\n",
        "\n",
        "                if pd.isna(row['high']) or pd.isna(row['low']):\n",
        "                    continue\n",
        "\n",
        "                active_bars += 1\n",
        "                period_data.at[idx, 'intra_period_count'] = active_bars\n",
        "\n",
        "                # Calculate range expansions using prior values\n",
        "                reu = max(0, row['high'] - prior_intra_high) if prior_intra_high is not None else 0\n",
        "                red = max(0, prior_intra_low - row['low']) if prior_intra_low is not None else 0\n",
        "\n",
        "                # Update metrics\n",
        "                period_data.at[idx, 'intra_period_reu'] = reu\n",
        "                period_data.at[idx, 'intra_period_red'] = red\n",
        "                period_data.at[idx, 'range_expansion_flag'] = 1 if (reu > 0 or red > 0) else 0\n",
        "\n",
        "                new_direction = self._determine_rpc_direction(\n",
        "                    reu, red, row['high'], row['low'], row['close'], prev_direction\n",
        "                )\n",
        "\n",
        "                if new_direction != prev_direction and prev_direction != 'N':\n",
        "                    cumulative_rpc += 1\n",
        "\n",
        "                period_data.at[idx, 'rpc_direction'] = new_direction\n",
        "                period_data.at[idx, 'intra_period_cumulative_rpc'] = cumulative_rpc\n",
        "                period_data.at[idx, 'bar_rpc'] = self._calculate_bar_rpc(\n",
        "                    reu, red, prev_direction, new_direction\n",
        "                )\n",
        "\n",
        "                # Update prior values for next iteration\n",
        "                prior_intra_high = max(row['high'], prior_intra_high if prior_intra_high is not None else row['high'])\n",
        "                prior_intra_low = min(row['low'], prior_intra_low if prior_intra_low is not None else row['low'])\n",
        "\n",
        "                prev_direction = new_direction\n",
        "\n",
        "            return period_data\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing parent period: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _determine_rpc_direction(\n",
        "        self, reu: float, red: float,\n",
        "        high: float, low: float, close: float,\n",
        "        prev_direction: str\n",
        "    ) -> str:\n",
        "        \"\"\"Determine the RPC direction based on expansions.\"\"\"\n",
        "        try:\n",
        "            if reu == 0 and red == 0:\n",
        "                return prev_direction\n",
        "            elif reu > 0 and red > 0:\n",
        "                # Two-way expansion - use close location\n",
        "                return 'U' if (close - low) / (high - low) >= 0.5 else 'D'\n",
        "            elif reu > 0:\n",
        "                return 'U'\n",
        "            else:\n",
        "                return 'D'\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error determining RPC direction: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_bar_rpc(\n",
        "        self, reu: float, red: float,\n",
        "        prev_direction: str, new_direction: str\n",
        "    ) -> int:\n",
        "        \"\"\"Calculate the bar's RPC value.\"\"\"\n",
        "        try:\n",
        "            if reu == 0 and red == 0:\n",
        "                return 0\n",
        "            elif reu > 0 and red > 0:\n",
        "                if prev_direction == 'N' or new_direction != prev_direction:\n",
        "                    return 2\n",
        "                return 1\n",
        "            elif new_direction != prev_direction and prev_direction != 'N':\n",
        "                return 1\n",
        "            return 0\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error calculating bar RPC: {e}\")\n",
        "            raise\n",
        "\n",
        "class TestTemporalFramework:\n",
        "    def run_all_tests(self):\n",
        "        processor = ConnectTemporalPeriods()\n",
        "\n",
        "        # Test RPC calculation\n",
        "        assert processor._calculate_bar_rpc(0, 0, 'N', 'N') == 0, \"No expansion should return 0\"\n",
        "        assert processor._calculate_bar_rpc(1, 1, 'N', 'U') == 2, \"Two-way expansion should return 2\"\n",
        "        assert processor._calculate_bar_rpc(1, 0, 'D', 'U') == 1, \"Direction change should return 1\"\n",
        "\n",
        "        # Test RPC direction\n",
        "        assert processor._determine_rpc_direction(0, 0, 100, 90, 95, 'U') == 'U', \"No expansion should maintain direction\"\n",
        "        assert processor._determine_rpc_direction(1, 1, 100, 90, 97, 'N') == 'U', \"Close in upper half should return U\"\n",
        "        assert processor._determine_rpc_direction(1, 0, 100, 90, 95, 'D') == 'U', \"Upward expansion should return U\"\n",
        "\n",
        "        print(\"All tests passed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run tests\n",
        "    tests = TestTemporalFramework()\n",
        "    tests.run_all_tests()\n",
        "\n",
        "    # Process files\n",
        "    processor = ConnectTemporalPeriods()\n",
        "    processor.process_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "ZD7SoAoLYeEE",
        "outputId": "50831f96-d6cf-4ff2-ec92-6234717fd657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error calculating parent stats: {e}\n",
            "ERROR:__main__:Error processing MMM: \"Column(s) ['intra_period_cumulative_rpc', 'range_expansion_flag'] do not exist\"\n",
            "ERROR:__main__:Error processing files: \"Column(s) ['intra_period_cumulative_rpc', 'range_expansion_flag'] do not exist\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"Column(s) ['intra_period_cumulative_rpc', 'range_expansion_flag'] do not exist\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-894978aad22e>\u001b[0m in \u001b[0;36m<cell line: 292>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;31m# Process files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConnectTemporalPeriods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m     \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-894978aad22e>\u001b[0m in \u001b[0;36mprocess_files\u001b[0;34m(self, input_dir)\u001b[0m\n\u001b[1;32m     68\u001b[0m                     \u001b[0mticker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Processing {ticker}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_single_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                     \u001b[0mfile_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-894978aad22e>\u001b[0m in \u001b[0;36mprocess_single_file\u001b[0;34m(self, filepath, ticker)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parent_period'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_period\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_period\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mparent_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_parent_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mprocessed_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_child_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-894978aad22e>\u001b[0m in \u001b[0;36m_calculate_parent_stats\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;31m# Basic stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             parent_stats = df.groupby('parent_period').agg({\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0;34m'date'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'first'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;34m'high'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'max'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGroupByApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1432\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m             \u001b[0;31m# GH #52849\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36magg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;31m# we require a list, but not a 'str'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36magg_dict_like\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mResult\u001b[0m \u001b[0mof\u001b[0m \u001b[0maggregation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \"\"\"\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg_or_apply_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"agg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     def compute_dict_like(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36magg_or_apply_dict_like\u001b[0;34m(self, op_name)\u001b[0m\n\u001b[1;32m   1606\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"as_index\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"as_index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         ):\n\u001b[0;32m-> 1608\u001b[0;31m             result_index, result_data = self.compute_dict_like(\n\u001b[0m\u001b[1;32m   1609\u001b[0m                 \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mcompute_dict_like\u001b[0;34m(self, op_name, selected_obj, selection, kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0mis_groupby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDataFrameGroupBy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeriesGroupBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAggFuncTypeDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_dictlike_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         is_non_unique_col = (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mnormalize_dictlike_arg\u001b[0;34m(self, how, obj, func)\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Column(s) {list(cols)} do not exist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0maggregator_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Column(s) ['intra_period_cumulative_rpc', 'range_expansion_flag'] do not exist\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import logging\n",
        "from typing import Tuple, Dict, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('temporal_processing.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "@dataclass\n",
        "class ParentPeriodStats:\n",
        "    high: float\n",
        "    low: float\n",
        "    duration: int\n",
        "    completed_bars: int\n",
        "    rpc_count: int\n",
        "    bar_of_high: int\n",
        "    bar_of_low: int\n",
        "\n",
        "class DataValidationError(Exception):\n",
        "    pass\n",
        "\n",
        "class ConnectTemporalPeriods:\n",
        "    REQUIRED_COLUMNS = {'date', 'open', 'high', 'low', 'close'}\n",
        "\n",
        "    def __init__(self, child_period: str = \"D\", parent_period: str = \"M\"):\n",
        "        self.child_period = child_period\n",
        "        self.parent_period = parent_period\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def validate_dataframe(self, df: pd.DataFrame) -> None:\n",
        "        \"\"\"Validate input dataframe structure and content.\"\"\"\n",
        "        missing_cols = self.REQUIRED_COLUMNS - set(df.columns)\n",
        "        if missing_cols:\n",
        "            raise DataValidationError(f\"Missing required columns: {missing_cols}\")\n",
        "\n",
        "        if not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
        "            try:\n",
        "                pd.to_datetime(df['date'])\n",
        "            except Exception as e:\n",
        "                raise DataValidationError(f\"Invalid date format: {e}\")\n",
        "\n",
        "        price_cols = ['open', 'high', 'low', 'close']\n",
        "        for col in price_cols:\n",
        "            if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "                raise DataValidationError(f\"Column {col} must be numeric\")\n",
        "\n",
        "        invalid_rows = df[df['high'] < df['low']].index\n",
        "        if len(invalid_rows) > 0:\n",
        "            raise DataValidationError(f\"Found {len(invalid_rows)} rows where high < low\")\n",
        "\n",
        "    def process_files(self, input_dir: str = \"input/\") -> None:\n",
        "        \"\"\"Process all CSV files in the input directory.\"\"\"\n",
        "        try:\n",
        "            os.makedirs(\"parent_output\", exist_ok=True)\n",
        "            os.makedirs(\"processed_output\", exist_ok=True)\n",
        "\n",
        "            file_count = 0\n",
        "            for filename in os.listdir(input_dir):\n",
        "                if filename.endswith(\".csv\"):\n",
        "                    ticker = filename.replace(\".csv\", \"\")\n",
        "                    self.logger.info(f\"Processing {ticker}\")\n",
        "                    self.process_single_file(os.path.join(input_dir, filename), ticker)\n",
        "                    file_count += 1\n",
        "\n",
        "            self.logger.info(f\"Successfully processed {file_count} files\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing files: {e}\")\n",
        "            raise\n",
        "\n",
        "    def process_single_file(self, filepath: str, ticker: str) -> None:\n",
        "        \"\"\"Process a single ticker's data file.\"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(filepath)\n",
        "            df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "            self.validate_dataframe(df)\n",
        "\n",
        "            df['parent_period'] = df['date'].dt.to_period(self.parent_period)\n",
        "            parent_stats = self._calculate_parent_stats(df)\n",
        "\n",
        "            processed_df = self._process_child_data(df, parent_stats)\n",
        "\n",
        "            self._safe_save_csv(parent_stats, f\"parent_output/{ticker}_parent.csv\")\n",
        "            self._safe_save_csv(processed_df, f\"processed_output/{ticker}_processed.csv\")\n",
        "\n",
        "            self.logger.info(f\"Successfully processed {ticker}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing {ticker}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _safe_save_csv(self, df: pd.DataFrame, filepath: str) -> None:\n",
        "        \"\"\"Safely save DataFrame to CSV with error handling.\"\"\"\n",
        "        try:\n",
        "            df.to_csv(filepath, index=False)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error saving to {filepath}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_parent_stats(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Calculate parent period statistics including bar locations and counts.\"\"\"\n",
        "        try:\n",
        "            # Basic stats\n",
        "            parent_stats = df.groupby('parent_period').agg({\n",
        "                'date': ['first', 'count'],\n",
        "                'high': 'max',\n",
        "                'low': 'min',\n",
        "                'range_expansion_flag': 'sum',\n",
        "                'intra_period_cumulative_rpc': 'max'\n",
        "            }).reset_index()\n",
        "\n",
        "            parent_stats.columns = ['parent_period', 'date', 'duration', 'parent_high',\n",
        "                                  'parent_low', 'expansion_count', 'rpc_count']\n",
        "\n",
        "            # Calculate bar positions for each parent period\n",
        "            for period in df['parent_period'].unique():\n",
        "                period_data = df[df['parent_period'] == period]\n",
        "                period_high = parent_stats.loc[parent_stats['parent_period'] == period, 'parent_high'].iloc[0]\n",
        "                period_low = parent_stats.loc[parent_stats['parent_period'] == period, 'parent_low'].iloc[0]\n",
        "\n",
        "                # Find first occurrence of high and low\n",
        "                bar_of_high = period_data[period_data['high'] == period_high].index.min()\n",
        "                bar_of_low = period_data[period_data['low'] == period_low].index.min()\n",
        "\n",
        "                if bar_of_high is not None:\n",
        "                    parent_stats.loc[parent_stats['parent_period'] == period, 'bar_of_high'] = \\\n",
        "                        period_data.loc[:bar_of_high].notna().sum()\n",
        "                if bar_of_low is not None:\n",
        "                    parent_stats.loc[parent_stats['parent_period'] == period, 'bar_of_low'] = \\\n",
        "                        period_data.loc[:bar_of_low].notna().sum()\n",
        "\n",
        "            return parent_stats\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(\"Error calculating parent stats: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _process_child_data(self, df: pd.DataFrame, parent_stats: pd.DataFrame = None) -> pd.DataFrame:\n",
        "        \"\"\"Process child period data with detailed metrics.\"\"\"\n",
        "        try:\n",
        "            df = df.copy()\n",
        "\n",
        "            # Initialize columns with correct dtypes\n",
        "            numeric_cols = {\n",
        "                'intra_period_count': 'int64',\n",
        "                'intra_period_high': 'float64',\n",
        "                'intra_period_low': 'float64',\n",
        "                'intra_period_reu': 'float64',\n",
        "                'intra_period_red': 'float64',\n",
        "                'range_expansion_flag': 'int64',\n",
        "                'bar_rpc': 'int64',\n",
        "                'intra_period_cumulative_rpc': 'int64'\n",
        "            }\n",
        "\n",
        "            for col, dtype in numeric_cols.items():\n",
        "                df[col] = pd.Series(dtype=dtype)\n",
        "\n",
        "            df['rpc_direction'] = 'N'\n",
        "\n",
        "            # Process each parent period\n",
        "            for period in df['parent_period'].unique():\n",
        "                mask = df['parent_period'] == period\n",
        "                period_data = df[mask].copy()\n",
        "\n",
        "                processed_period = self._process_parent_period(period_data)\n",
        "                df.loc[mask] = processed_period.astype(df[mask].dtypes)\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing child data: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _process_parent_period(self, period_data: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Process data within a single parent period.\"\"\"\n",
        "        try:\n",
        "            active_bars = 0\n",
        "            cumulative_rpc = 0\n",
        "            prev_direction = 'N'\n",
        "            prior_intra_high = None\n",
        "            prior_intra_low = None\n",
        "\n",
        "            for idx in period_data.index:\n",
        "                row = period_data.loc[idx]\n",
        "\n",
        "                if pd.isna(row['high']) or pd.isna(row['low']):\n",
        "                    continue\n",
        "\n",
        "                active_bars += 1\n",
        "                period_data.at[idx, 'intra_period_count'] = active_bars\n",
        "\n",
        "                # Calculate range expansions using prior values\n",
        "                reu = max(0, row['high'] - prior_intra_high) if prior_intra_high is not None else 0\n",
        "                red = max(0, prior_intra_low - row['low']) if prior_intra_low is not None else 0\n",
        "\n",
        "                # Update metrics\n",
        "                period_data.at[idx, 'intra_period_reu'] = reu\n",
        "                period_data.at[idx, 'intra_period_red'] = red\n",
        "                period_data.at[idx, 'range_expansion_flag'] = 1 if (reu > 0 or red > 0) else 0\n",
        "\n",
        "                new_direction = self._determine_rpc_direction(\n",
        "                    reu, red, row['high'], row['low'], row['close'], prev_direction\n",
        "                )\n",
        "\n",
        "                if new_direction != prev_direction and prev_direction != 'N':\n",
        "                    cumulative_rpc += 1\n",
        "\n",
        "                period_data.at[idx, 'rpc_direction'] = new_direction\n",
        "                period_data.at[idx, 'intra_period_cumulative_rpc'] = cumulative_rpc\n",
        "                period_data.at[idx, 'bar_rpc'] = self._calculate_bar_rpc(\n",
        "                    reu, red, prev_direction, new_direction\n",
        "                )\n",
        "\n",
        "                # Update prior values for next iteration\n",
        "                prior_intra_high = max(row['high'], prior_intra_high if prior_intra_high is not None else row['high'])\n",
        "                prior_intra_low = min(row['low'], prior_intra_low if prior_intra_low is not None else row['low'])\n",
        "\n",
        "                prev_direction = new_direction\n",
        "\n",
        "            return period_data\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing parent period: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _determine_rpc_direction(\n",
        "        self, reu: float, red: float,\n",
        "        high: float, low: float, close: float,\n",
        "        prev_direction: str\n",
        "    ) -> str:\n",
        "        \"\"\"Determine the RPC direction based on expansions.\"\"\"\n",
        "        try:\n",
        "            if reu == 0 and red == 0:\n",
        "                return prev_direction\n",
        "            elif reu > 0 and red > 0:\n",
        "                # Two-way expansion - use close location\n",
        "                return 'U' if (close - low) / (high - low) >= 0.5 else 'D'\n",
        "            elif reu > 0:\n",
        "                return 'U'\n",
        "            else:\n",
        "                return 'D'\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error determining RPC direction: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_bar_rpc(\n",
        "        self, reu: float, red: float,\n",
        "        prev_direction: str, new_direction: str\n",
        "    ) -> int:\n",
        "        \"\"\"Calculate the bar's RPC value.\"\"\"\n",
        "        try:\n",
        "            if reu == 0 and red == 0:\n",
        "                return 0\n",
        "            elif reu > 0 and red > 0:\n",
        "                if prev_direction == 'N' or new_direction != prev_direction:\n",
        "                    return 2\n",
        "                return 1\n",
        "            elif new_direction != prev_direction and prev_direction != 'N':\n",
        "                return 1\n",
        "            return 0\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error calculating bar RPC: {e}\")\n",
        "            raise\n",
        "\n",
        "class TestTemporalFramework:\n",
        "    def run_all_tests(self):\n",
        "        processor = ConnectTemporalPeriods()\n",
        "\n",
        "        # Test RPC calculation\n",
        "        assert processor._calculate_bar_rpc(0, 0, 'N', 'N') == 0, \"No expansion should return 0\"\n",
        "        assert processor._calculate_bar_rpc(1, 1, 'N', 'U') == 2, \"Two-way expansion should return 2\"\n",
        "        assert processor._calculate_bar_rpc(1, 0, 'D', 'U') == 1, \"Direction change should return 1\"\n",
        "\n",
        "        # Test RPC direction\n",
        "        assert processor._determine_rpc_direction(0, 0, 100, 90, 95, 'U') == 'U', \"No expansion should maintain direction\"\n",
        "        assert processor._determine_rpc_direction(1, 1, 100, 90, 97, 'N') == 'U', \"Close in upper half should return U\"\n",
        "        assert processor._determine_rpc_direction(1, 0, 100, 90, 95, 'D') == 'U', \"Upward expansion should return U\"\n",
        "\n",
        "        print(\"All tests passed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run tests\n",
        "    tests = TestTemporalFramework()\n",
        "    tests.run_all_tests()\n",
        "\n",
        "    # Process files\n",
        "    processor = ConnectTemporalPeriods()\n",
        "    processor.process_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "Q18-gjDmYjLr",
        "outputId": "3bfba01d-c113-428b-99e7-a94939c4307a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error calculating parent stats: {e}\n",
            "ERROR:__main__:Error processing MMM: \"Column(s) ['intra_period_cumulative_rpc', 'range_expansion_flag'] do not exist\"\n",
            "ERROR:__main__:Error processing files: \"Column(s) ['intra_period_cumulative_rpc', 'range_expansion_flag'] do not exist\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"Column(s) ['intra_period_cumulative_rpc', 'range_expansion_flag'] do not exist\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-894978aad22e>\u001b[0m in \u001b[0;36m<cell line: 292>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;31m# Process files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConnectTemporalPeriods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m     \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-894978aad22e>\u001b[0m in \u001b[0;36mprocess_files\u001b[0;34m(self, input_dir)\u001b[0m\n\u001b[1;32m     68\u001b[0m                     \u001b[0mticker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Processing {ticker}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_single_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                     \u001b[0mfile_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-894978aad22e>\u001b[0m in \u001b[0;36mprocess_single_file\u001b[0;34m(self, filepath, ticker)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parent_period'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_period\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_period\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mparent_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_parent_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mprocessed_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_child_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-894978aad22e>\u001b[0m in \u001b[0;36m_calculate_parent_stats\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;31m# Basic stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             parent_stats = df.groupby('parent_period').agg({\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0;34m'date'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'first'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;34m'high'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'max'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGroupByApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1432\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m             \u001b[0;31m# GH #52849\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36magg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;31m# we require a list, but not a 'str'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36magg_dict_like\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mResult\u001b[0m \u001b[0mof\u001b[0m \u001b[0maggregation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \"\"\"\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg_or_apply_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"agg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     def compute_dict_like(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36magg_or_apply_dict_like\u001b[0;34m(self, op_name)\u001b[0m\n\u001b[1;32m   1606\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"as_index\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"as_index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         ):\n\u001b[0;32m-> 1608\u001b[0;31m             result_index, result_data = self.compute_dict_like(\n\u001b[0m\u001b[1;32m   1609\u001b[0m                 \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mcompute_dict_like\u001b[0;34m(self, op_name, selected_obj, selection, kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0mis_groupby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDataFrameGroupBy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeriesGroupBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAggFuncTypeDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_dictlike_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         is_non_unique_col = (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mnormalize_dictlike_arg\u001b[0;34m(self, how, obj, func)\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Column(s) {list(cols)} do not exist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0maggregator_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Column(s) ['intra_period_cumulative_rpc', 'range_expansion_flag'] do not exist\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _calculate_parent_stats(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Calculate parent period statistics including bar locations and counts.\"\"\"\n",
        "        try:\n",
        "            # Basic stats\n",
        "            parent_stats = df.groupby('parent_period').agg({\n",
        "                'date': ['first', 'count'],\n",
        "                'high': 'max',\n",
        "                'low': 'min'\n",
        "            }).reset_index()\n",
        "\n",
        "            parent_stats.columns = ['parent_period', 'date', 'duration', 'parent_high', 'parent_low']\n",
        "\n",
        "            # Calculate bar positions and supplementary stats for each parent period\n",
        "            for period in df['parent_period'].unique():\n",
        "                period_data = df[df['parent_period'] == period]\n",
        "                period_high = parent_stats.loc[parent_stats['parent_period'] == period, 'parent_high'].iloc[0]\n",
        "                period_low = parent_stats.loc[parent_stats['parent_period'] == period, 'parent_low'].iloc[0]\n",
        "\n",
        "                # Find first occurrence of high and low\n",
        "                bar_of_high = period_data[period_data['high'] == period_high].index.min()\n",
        "                bar_of_low = period_data[period_data['low'] == period_low].index.min()\n",
        "\n",
        "                if bar_of_high is not None:\n",
        "                    parent_stats.loc[parent_stats['parent_period'] == period, 'bar_of_high'] = \\\n",
        "                        period_data.loc[:bar_of_high].notna().sum()\n",
        "                if bar_of_low is not None:\n",
        "                    parent_stats.loc[parent_stats['parent_period'] == period, 'bar_of_low'] = \\\n",
        "                        period_data.loc[:bar_of_low].notna().sum()\n",
        "\n",
        "                # Add expansion and RPC counts\n",
        "                parent_stats.loc[parent_stats['parent_period'] == period, 'expansion_count'] = \\\n",
        "                    period_data['range_expansion_flag'].sum()\n",
        "                parent_stats.loc[parent_stats['parent_period'] == period, 'rpc_count'] = \\\n",
        "                    period_data['intra_period_cumulative_rpc'].max()\n",
        "\n",
        "            return parent_stats\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error calculating parent stats: {str(e)}\")\n",
        "            raiseimport pandas as pd\n",
        "import os\n",
        "import logging\n",
        "from typing import Tuple, Dict, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('temporal_processing.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "@dataclass\n",
        "class ParentPeriodStats:\n",
        "    high: float\n",
        "    low: float\n",
        "    duration: int\n",
        "    completed_bars: int\n",
        "    rpc_count: int\n",
        "    bar_of_high: int\n",
        "    bar_of_low: int\n",
        "\n",
        "class DataValidationError(Exception):\n",
        "    pass\n",
        "\n",
        "class ConnectTemporalPeriods:\n",
        "    REQUIRED_COLUMNS = {'date', 'open', 'high', 'low', 'close'}\n",
        "\n",
        "    def __init__(self, child_period: str = \"D\", parent_period: str = \"M\"):\n",
        "        self.child_period = child_period\n",
        "        self.parent_period = parent_period\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def validate_dataframe(self, df: pd.DataFrame) -> None:\n",
        "        \"\"\"Validate input dataframe structure and content.\"\"\"\n",
        "        missing_cols = self.REQUIRED_COLUMNS - set(df.columns)\n",
        "        if missing_cols:\n",
        "            raise DataValidationError(f\"Missing required columns: {missing_cols}\")\n",
        "\n",
        "        if not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
        "            try:\n",
        "                pd.to_datetime(df['date'])\n",
        "            except Exception as e:\n",
        "                raise DataValidationError(f\"Invalid date format: {e}\")\n",
        "\n",
        "        price_cols = ['open', 'high', 'low', 'close']\n",
        "        for col in price_cols:\n",
        "            if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "                raise DataValidationError(f\"Column {col} must be numeric\")\n",
        "\n",
        "        invalid_rows = df[df['high'] < df['low']].index\n",
        "        if len(invalid_rows) > 0:\n",
        "            raise DataValidationError(f\"Found {len(invalid_rows)} rows where high < low\")\n",
        "\n",
        "    def process_files(self, input_dir: str = \"input/\") -> None:\n",
        "        \"\"\"Process all CSV files in the input directory.\"\"\"\n",
        "        try:\n",
        "            os.makedirs(\"parent_output\", exist_ok=True)\n",
        "            os.makedirs(\"processed_output\", exist_ok=True)\n",
        "\n",
        "            file_count = 0\n",
        "            for filename in os.listdir(input_dir):\n",
        "                if filename.endswith(\".csv\"):\n",
        "                    ticker = filename.replace(\".csv\", \"\")\n",
        "                    self.logger.info(f\"Processing {ticker}\")\n",
        "                    self.process_single_file(os.path.join(input_dir, filename), ticker)\n",
        "                    file_count += 1\n",
        "\n",
        "            self.logger.info(f\"Successfully processed {file_count} files\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing files: {e}\")\n",
        "            raise\n",
        "\n",
        "    def process_single_file(self, filepath: str, ticker: str) -> None:\n",
        "        \"\"\"Process a single ticker's data file.\"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(filepath)\n",
        "            df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "            self.validate_dataframe(df)\n",
        "\n",
        "            df['parent_period'] = df['date'].dt.to_period(self.parent_period)\n",
        "            parent_stats = self._calculate_parent_stats(df)\n",
        "\n",
        "            processed_df = self._process_child_data(df, parent_stats)\n",
        "\n",
        "            self._safe_save_csv(parent_stats, f\"parent_output/{ticker}_parent.csv\")\n",
        "            self._safe_save_csv(processed_df, f\"processed_output/{ticker}_processed.csv\")\n",
        "\n",
        "            self.logger.info(f\"Successfully processed {ticker}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing {ticker}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _safe_save_csv(self, df: pd.DataFrame, filepath: str) -> None:\n",
        "        \"\"\"Safely save DataFrame to CSV with error handling.\"\"\"\n",
        "        try:\n",
        "            df.to_csv(filepath, index=False)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error saving to {filepath}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_parent_stats(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Calculate parent period statistics including bar locations and counts.\"\"\"\n",
        "        try:\n",
        "            # Basic stats\n",
        "            parent_stats = df.groupby('parent_period').agg({\n",
        "                'date': ['first', 'count'],\n",
        "                'high': 'max',\n",
        "                'low': 'min',\n",
        "                'range_expansion_flag': 'sum',\n",
        "                'intra_period_cumulative_rpc': 'max'\n",
        "            }).reset_index()\n",
        "\n",
        "            parent_stats.columns = ['parent_period', 'date', 'duration', 'parent_high',\n",
        "                                  'parent_low', 'expansion_count', 'rpc_count']\n",
        "\n",
        "            # Calculate bar positions for each parent period\n",
        "            for period in df['parent_period'].unique():\n",
        "                period_data = df[df['parent_period'] == period]\n",
        "                period_high = parent_stats.loc[parent_stats['parent_period'] == period, 'parent_high'].iloc[0]\n",
        "                period_low = parent_stats.loc[parent_stats['parent_period'] == period, 'parent_low'].iloc[0]\n",
        "\n",
        "                # Find first occurrence of high and low\n",
        "                bar_of_high = period_data[period_data['high'] == period_high].index.min()\n",
        "                bar_of_low = period_data[period_data['low'] == period_low].index.min()\n",
        "\n",
        "                if bar_of_high is not None:\n",
        "                    parent_stats.loc[parent_stats['parent_period'] == period, 'bar_of_high'] = \\\n",
        "                        period_data.loc[:bar_of_high].notna().sum()\n",
        "                if bar_of_low is not None:\n",
        "                    parent_stats.loc[parent_stats['parent_period'] == period, 'bar_of_low'] = \\\n",
        "                        period_data.loc[:bar_of_low].notna().sum()\n",
        "\n",
        "            return parent_stats\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(\"Error calculating parent stats: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _process_child_data(self, df: pd.DataFrame, parent_stats: pd.DataFrame = None) -> pd.DataFrame:\n",
        "        \"\"\"Process child period data with detailed metrics.\"\"\"\n",
        "        try:\n",
        "            df = df.copy()\n",
        "\n",
        "            # Initialize columns with correct dtypes\n",
        "            numeric_cols = {\n",
        "                'intra_period_count': 'int64',\n",
        "                'intra_period_high': 'float64',\n",
        "                'intra_period_low': 'float64',\n",
        "                'intra_period_reu': 'float64',\n",
        "                'intra_period_red': 'float64',\n",
        "                'range_expansion_flag': 'int64',\n",
        "                'bar_rpc': 'int64',\n",
        "                'intra_period_cumulative_rpc': 'int64'\n",
        "            }\n",
        "\n",
        "            for col, dtype in numeric_cols.items():\n",
        "                df[col] = pd.Series(dtype=dtype)\n",
        "\n",
        "            df['rpc_direction'] = 'N'\n",
        "\n",
        "            # Process each parent period\n",
        "            for period in df['parent_period'].unique():\n",
        "                mask = df['parent_period'] == period\n",
        "                period_data = df[mask].copy()\n",
        "\n",
        "                processed_period = self._process_parent_period(period_data)\n",
        "                df.loc[mask] = processed_period.astype(df[mask].dtypes)\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing child data: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _process_parent_period(self, period_data: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Process data within a single parent period.\"\"\"\n",
        "        try:\n",
        "            active_bars = 0\n",
        "            cumulative_rpc = 0\n",
        "            prev_direction = 'N'\n",
        "            prior_intra_high = None\n",
        "            prior_intra_low = None\n",
        "\n",
        "            for idx in period_data.index:\n",
        "                row = period_data.loc[idx]\n",
        "\n",
        "                if pd.isna(row['high']) or pd.isna(row['low']):\n",
        "                    continue\n",
        "\n",
        "                active_bars += 1\n",
        "                period_data.at[idx, 'intra_period_count'] = active_bars\n",
        "\n",
        "                # Calculate range expansions using prior values\n",
        "                reu = max(0, row['high'] - prior_intra_high) if prior_intra_high is not None else 0\n",
        "                red = max(0, prior_intra_low - row['low']) if prior_intra_low is not None else 0\n",
        "\n",
        "                # Update metrics\n",
        "                period_data.at[idx, 'intra_period_reu'] = reu\n",
        "                period_data.at[idx, 'intra_period_red'] = red\n",
        "                period_data.at[idx, 'range_expansion_flag'] = 1 if (reu > 0 or red > 0) else 0\n",
        "\n",
        "                new_direction = self._determine_rpc_direction(\n",
        "                    reu, red, row['high'], row['low'], row['close'], prev_direction\n",
        "                )\n",
        "\n",
        "                if new_direction != prev_direction and prev_direction != 'N':\n",
        "                    cumulative_rpc += 1\n",
        "\n",
        "                period_data.at[idx, 'rpc_direction'] = new_direction\n",
        "                period_data.at[idx, 'intra_period_cumulative_rpc'] = cumulative_rpc\n",
        "                period_data.at[idx, 'bar_rpc'] = self._calculate_bar_rpc(\n",
        "                    reu, red, prev_direction, new_direction\n",
        "                )\n",
        "\n",
        "                # Update prior values for next iteration\n",
        "                prior_intra_high = max(row['high'], prior_intra_high if prior_intra_high is not None else row['high'])\n",
        "                prior_intra_low = min(row['low'], prior_intra_low if prior_intra_low is not None else row['low'])\n",
        "\n",
        "                prev_direction = new_direction\n",
        "\n",
        "            return period_data\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing parent period: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _determine_rpc_direction(\n",
        "        self, reu: float, red: float,\n",
        "        high: float, low: float, close: float,\n",
        "        prev_direction: str\n",
        "    ) -> str:\n",
        "        \"\"\"Determine the RPC direction based on expansions.\"\"\"\n",
        "        try:\n",
        "            if reu == 0 and red == 0:\n",
        "                return prev_direction\n",
        "            elif reu > 0 and red > 0:\n",
        "                # Two-way expansion - use close location\n",
        "                return 'U' if (close - low) / (high - low) >= 0.5 else 'D'\n",
        "            elif reu > 0:\n",
        "                return 'U'\n",
        "            else:\n",
        "                return 'D'\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error determining RPC direction: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_bar_rpc(\n",
        "        self, reu: float, red: float,\n",
        "        prev_direction: str, new_direction: str\n",
        "    ) -> int:\n",
        "        \"\"\"Calculate the bar's RPC value.\"\"\"\n",
        "        try:\n",
        "            if reu == 0 and red == 0:\n",
        "                return 0\n",
        "            elif reu > 0 and red > 0:\n",
        "                if prev_direction == 'N' or new_direction != prev_direction:\n",
        "                    return 2\n",
        "                return 1\n",
        "            elif new_direction != prev_direction and prev_direction != 'N':\n",
        "                return 1\n",
        "            return 0\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error calculating bar RPC: {e}\")\n",
        "            raise\n",
        "\n",
        "class TestTemporalFramework:\n",
        "    def run_all_tests(self):\n",
        "        processor = ConnectTemporalPeriods()\n",
        "\n",
        "        # Test RPC calculation\n",
        "        assert processor._calculate_bar_rpc(0, 0, 'N', 'N') == 0, \"No expansion should return 0\"\n",
        "        assert processor._calculate_bar_rpc(1, 1, 'N', 'U') == 2, \"Two-way expansion should return 2\"\n",
        "        assert processor._calculate_bar_rpc(1, 0, 'D', 'U') == 1, \"Direction change should return 1\"\n",
        "\n",
        "        # Test RPC direction\n",
        "        assert processor._determine_rpc_direction(0, 0, 100, 90, 95, 'U') == 'U', \"No expansion should maintain direction\"\n",
        "        assert processor._determine_rpc_direction(1, 1, 100, 90, 97, 'N') == 'U', \"Close in upper half should return U\"\n",
        "        assert processor._determine_rpc_direction(1, 0, 100, 90, 95, 'D') == 'U', \"Upward expansion should return U\"\n",
        "\n",
        "        print(\"All tests passed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run tests\n",
        "    tests = TestTemporalFramework()\n",
        "    tests.run_all_tests()\n",
        "\n",
        "    # Process files\n",
        "    processor = ConnectTemporalPeriods()\n",
        "    processor.process_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "KpSBeg2mYtni",
        "outputId": "cb2b4370-93da-4a3a-9881-b5e06ce2d77d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-17-a46306e6a797>, line 40)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-a46306e6a797>\"\u001b[0;36m, line \u001b[0;32m40\u001b[0m\n\u001b[0;31m    raiseimport pandas as pd\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import logging\n",
        "from typing import Tuple, Dict, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('temporal_processing.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "@dataclass\n",
        "class ParentPeriodStats:\n",
        "    high: float\n",
        "    low: float\n",
        "    duration: int\n",
        "    completed_bars: int\n",
        "    rpc_count: int\n",
        "    bar_of_high: int\n",
        "    bar_of_low: int\n",
        "\n",
        "class DataValidationError(Exception):\n",
        "    pass\n",
        "\n",
        "class ConnectTemporalPeriods:\n",
        "    REQUIRED_COLUMNS = {'date', 'open', 'high', 'low', 'close'}\n",
        "\n",
        "    def __init__(self, child_period: str = \"D\", parent_period: str = \"M\"):\n",
        "        self.child_period = child_period\n",
        "        self.parent_period = parent_period\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def validate_dataframe(self, df: pd.DataFrame) -> None:\n",
        "        missing_cols = self.REQUIRED_COLUMNS - set(df.columns)\n",
        "        if missing_cols:\n",
        "            raise DataValidationError(f\"Missing required columns: {missing_cols}\")\n",
        "\n",
        "        if not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
        "            try:\n",
        "                pd.to_datetime(df['date'])\n",
        "            except Exception as e:\n",
        "                raise DataValidationError(f\"Invalid date format: {e}\")\n",
        "\n",
        "        price_cols = ['open', 'high', 'low', 'close']\n",
        "        for col in price_cols:\n",
        "            if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "                raise DataValidationError(f\"Column {col} must be numeric\")\n",
        "\n",
        "        invalid_rows = df[df['high'] < df['low']].index\n",
        "        if len(invalid_rows) > 0:\n",
        "            raise DataValidationError(f\"Found {len(invalid_rows)} rows where high < low\")\n",
        "\n",
        "    def process_files(self, input_dir: str = \"input/\") -> None:\n",
        "        try:\n",
        "            os.makedirs(\"parent_output\", exist_ok=True)\n",
        "            os.makedirs(\"processed_output\", exist_ok=True)\n",
        "\n",
        "            file_count = 0\n",
        "            for filename in os.listdir(input_dir):\n",
        "                if filename.endswith(\".csv\"):\n",
        "                    ticker = filename.replace(\".csv\", \"\")\n",
        "                    self.logger.info(f\"Processing {ticker}\")\n",
        "                    self.process_single_file(os.path.join(input_dir, filename), ticker)\n",
        "                    file_count += 1\n",
        "\n",
        "            self.logger.info(f\"Successfully processed {file_count} files\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing files: {e}\")\n",
        "            raise\n",
        "\n",
        "    def process_single_file(self, filepath: str, ticker: str) -> None:\n",
        "        try:\n",
        "            # Load and validate data\n",
        "            df = pd.read_csv(filepath)\n",
        "            df['date'] = pd.to_datetime(df['date'])\n",
        "            self.validate_dataframe(df)\n",
        "\n",
        "            # Add period column and initialize fields\n",
        "            df['parent_period'] = df['date'].dt.to_period(self.parent_period)\n",
        "\n",
        "            numeric_cols = {\n",
        "                'intra_period_count': 'int64',\n",
        "                'intra_period_high': 'float64',\n",
        "                'intra_period_low': 'float64',\n",
        "                'intra_period_reu': 'float64',\n",
        "                'intra_period_red': 'float64',\n",
        "                'range_expansion_flag': 'int64',\n",
        "                'bar_rpc': 'int64',\n",
        "                'intra_period_cumulative_rpc': 'int64'\n",
        "            }\n",
        "\n",
        "            for col, dtype in numeric_cols.items():\n",
        "                df[col] = pd.Series(0, index=df.index, dtype=dtype)\n",
        "\n",
        "            df['rpc_direction'] = 'N'\n",
        "\n",
        "            # Process each parent period\n",
        "            for period in df['parent_period'].unique():\n",
        "                mask = df['parent_period'] == period\n",
        "                period_data = df[mask].copy()\n",
        "                processed_period = self._process_parent_period(period_data)\n",
        "                df.loc[mask] = processed_period.astype(df[mask].dtypes)\n",
        "\n",
        "            # Calculate parent stats\n",
        "            parent_stats = self._calculate_parent_stats(df)\n",
        "\n",
        "            # Save outputs\n",
        "            self._safe_save_csv(parent_stats, f\"parent_output/{ticker}_parent.csv\")\n",
        "            self._safe_save_csv(df, f\"processed_output/{ticker}_processed.csv\")\n",
        "\n",
        "            self.logger.info(f\"Successfully processed {ticker}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing {ticker}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _safe_save_csv(self, df: pd.DataFrame, filepath: str) -> None:\n",
        "        try:\n",
        "            df.to_csv(filepath, index=False)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error saving to {filepath}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_parent_stats(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        try:\n",
        "            # Basic stats\n",
        "            parent_stats = df.groupby('parent_period').agg({\n",
        "                'date': 'first',\n",
        "                'high': 'max',\n",
        "                'low': 'min',\n",
        "                'date': 'count'\n",
        "            }).reset_index()\n",
        "\n",
        "            parent_stats.columns = ['parent_period', 'date', 'parent_high', 'parent_low', 'duration']\n",
        "\n",
        "            # Initialize supplementary columns\n",
        "            parent_stats['bar_of_high'] = 0\n",
        "            parent_stats['bar_of_low'] = 0\n",
        "            parent_stats['expansion_count'] = 0\n",
        "            parent_stats['rpc_count'] = 0\n",
        "\n",
        "            # Calculate additional stats per period\n",
        "            for period in df['parent_period'].unique():\n",
        "                period_data = df[df['parent_period'] == period]\n",
        "                period_high = parent_stats.loc[parent_stats['parent_period'] == period, 'parent_high'].iloc[0]\n",
        "                period_low = parent_stats.loc[parent_stats['parent_period'] == period, 'parent_low'].iloc[0]\n",
        "\n",
        "                # First occurrence of high/low\n",
        "                high_idx = period_data[period_data['high'] == period_high].index.min()\n",
        "                low_idx = period_data[period_data['low'] == period_low].index.min()\n",
        "\n",
        "                idx = parent_stats['parent_period'] == period\n",
        "                if high_idx is not None:\n",
        "                    high_bar = period_data.loc[:high_idx, 'high'].notna().sum()\n",
        "                    parent_stats.loc[idx, 'bar_of_high'] = high_bar\n",
        "\n",
        "                if low_idx is not None:\n",
        "                    low_bar = period_data.loc[:low_idx, 'low'].notna().sum()\n",
        "                    parent_stats.loc[idx, 'bar_of_low'] = low_bar\n",
        "\n",
        "                # Count expansions and RPCs\n",
        "                parent_stats.loc[idx, 'expansion_count'] = period_data['range_expansion_flag'].sum()\n",
        "                parent_stats.loc[idx, 'rpc_count'] = period_data['intra_period_cumulative_rpc'].max()\n",
        "\n",
        "            return parent_stats\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error calculating parent stats: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _process_parent_period(self, period_data: pd.DataFrame) -> pd.DataFrame:\n",
        "        try:\n",
        "            active_bars = 0\n",
        "            cumulative_rpc = 0\n",
        "            prev_direction = 'N'\n",
        "            prior_intra_high = None\n",
        "            prior_intra_low = None\n",
        "\n",
        "            for idx in period_data.index:\n",
        "                row = period_data.loc[idx]\n",
        "\n",
        "                if pd.isna(row['high']) or pd.isna(row['low']):\n",
        "                    continue\n",
        "\n",
        "                active_bars += 1\n",
        "                period_data.at[idx, 'intra_period_count'] = active_bars\n",
        "\n",
        "                # Calculate range expansions\n",
        "                reu = max(0, row['high'] - prior_intra_high) if prior_intra_high is not None else 0\n",
        "                red = max(0, prior_intra_low - row['low']) if prior_intra_low is not None else 0\n",
        "\n",
        "                # Update metrics\n",
        "                period_data.at[idx, 'intra_period_reu'] = reu\n",
        "                period_data.at[idx, 'intra_period_red'] = red\n",
        "                period_data.at[idx, 'range_expansion_flag'] = 1 if (reu > 0 or red > 0) else 0\n",
        "\n",
        "                # Calculate RPC\n",
        "                new_direction = self._determine_rpc_direction(\n",
        "                    reu, red, row['high'], row['low'], row['close'], prev_direction\n",
        "                )\n",
        "\n",
        "                if new_direction != prev_direction and prev_direction != 'N':\n",
        "                    cumulative_rpc += 1\n",
        "\n",
        "                period_data.at[idx, 'rpc_direction'] = new_direction\n",
        "                period_data.at[idx, 'intra_period_cumulative_rpc'] = cumulative_rpc\n",
        "                period_data.at[idx, 'bar_rpc'] = self._calculate_bar_rpc(\n",
        "                    reu, red, prev_direction, new_direction\n",
        "                )\n",
        "\n",
        "                # Update prior values for next iteration\n",
        "                prior_intra_high = max(row['high'], prior_intra_high if prior_intra_high is not None else row['high'])\n",
        "                prior_intra_low = min(row['low'], prior_intra_low if prior_intra_low is not None else row['low'])\n",
        "\n",
        "                prev_direction = new_direction\n",
        "\n",
        "            return period_data\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing parent period: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _determine_rpc_direction(\n",
        "        self, reu: float, red: float,\n",
        "        high: float, low: float, close: float,\n",
        "        prev_direction: str\n",
        "    ) -> str:\n",
        "        try:\n",
        "            if reu == 0 and red == 0:\n",
        "                return prev_direction\n",
        "            elif reu > 0 and red > 0:\n",
        "                return 'U' if (close - low) / (high - low) >= 0.5 else 'D'\n",
        "            elif reu > 0:\n",
        "                return 'U'\n",
        "            else:\n",
        "                return 'D'\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error determining RPC direction: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_bar_rpc(\n",
        "        self, reu: float, red: float,\n",
        "        prev_direction: str, new_direction: str\n",
        "    ) -> int:\n",
        "        try:\n",
        "            if reu == 0 and red == 0:\n",
        "                return 0\n",
        "            elif reu > 0 and red > 0:\n",
        "                if prev_direction == 'N' or new_direction != prev_direction:\n",
        "                    return 2\n",
        "                return 1\n",
        "            elif new_direction != prev_direction and prev_direction != 'N':\n",
        "                return 1\n",
        "            return 0\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error calculating bar RPC: {e}\")\n",
        "            raise\n",
        "\n",
        "class TestTemporalFramework:\n",
        "    def run_all_tests(self):\n",
        "        processor = ConnectTemporalPeriods()\n",
        "\n",
        "        assert processor._calculate_bar_rpc(0, 0, 'N', 'N') == 0, \"No expansion should return 0\"\n",
        "        assert processor._calculate_bar_rpc(1, 1, 'N', 'U') == 2, \"Two-way expansion should return 2\"\n",
        "        assert processor._calculate_bar_rpc(1, 0, 'D', 'U') == 1, \"Direction change should return 1\"\n",
        "\n",
        "        assert processor._determine_rpc_direction(0, 0, 100, 90, 95, 'U') == 'U', \"No expansion should maintain direction\"\n",
        "        assert processor._determine_rpc_direction(1, 1, 100, 90, 97, 'N') == 'U', \"Close in upper half should return U\"\n",
        "        assert processor._determine_rpc_direction(1, 0, 100, 90, 95, 'D') == 'U', \"Upward expansion should return U\"\n",
        "\n",
        "        print(\"All tests passed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tests = TestTemporalFramework()\n",
        "    tests.run_all_tests()\n",
        "\n",
        "    processor = ConnectTemporalPeriods()\n",
        "    processor.process_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "MuWhb5Z-Y1MT",
        "outputId": "40a43a55-b7cd-40ce-85e1-950a7dc1dd91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error calculating parent stats: Length mismatch: Expected axis has 4 elements, new values have 5 elements\n",
            "ERROR:__main__:Error processing MMM: Length mismatch: Expected axis has 4 elements, new values have 5 elements\n",
            "ERROR:__main__:Error processing files: Length mismatch: Expected axis has 4 elements, new values have 5 elements\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Length mismatch: Expected axis has 4 elements, new values have 5 elements",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-7240d21be92b>\u001b[0m in \u001b[0;36m<cell line: 280>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConnectTemporalPeriods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m     \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-7240d21be92b>\u001b[0m in \u001b[0;36mprocess_files\u001b[0;34m(self, input_dir)\u001b[0m\n\u001b[1;32m     65\u001b[0m                     \u001b[0mticker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Processing {ticker}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_single_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                     \u001b[0mfile_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-7240d21be92b>\u001b[0m in \u001b[0;36mprocess_single_file\u001b[0;34m(self, filepath, ticker)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;31m# Calculate parent stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mparent_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_parent_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;31m# Save outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-7240d21be92b>\u001b[0m in \u001b[0;36m_calculate_parent_stats\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    137\u001b[0m             }).reset_index()\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mparent_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'parent_period'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'parent_high'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'parent_low'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'duration'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;31m# Initialize supplementary columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   6311\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6312\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6313\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6314\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6315\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mproperties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \"\"\"\n\u001b[1;32m    813\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mset_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAxisInt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;31m# Caller is responsible for ensuring we have an Index object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_set_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/base.py\u001b[0m in \u001b[0;36m_validate_set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mnew_len\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mold_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0;34mf\"Length mismatch: Expected axis has {old_len} elements, new \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34mf\"values have {new_len} elements\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected axis has 4 elements, new values have 5 elements"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import logging\n",
        "from typing import Tuple, Dict, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('temporal_processing.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "@dataclass\n",
        "class ParentPeriodStats:\n",
        "    high: float\n",
        "    low: float\n",
        "    duration: int\n",
        "    completed_bars: int\n",
        "    rpc_count: int\n",
        "    bar_of_high: int\n",
        "    bar_of_low: int\n",
        "\n",
        "class DataValidationError(Exception):\n",
        "    pass\n",
        "\n",
        "class ConnectTemporalPeriods:\n",
        "    REQUIRED_COLUMNS = {'date', 'open', 'high', 'low', 'close'}\n",
        "\n",
        "    def __init__(self, child_period: str = \"D\", parent_period: str = \"M\"):\n",
        "        self.child_period = child_period\n",
        "        self.parent_period = parent_period\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def validate_dataframe(self, df: pd.DataFrame) -> None:\n",
        "        missing_cols = self.REQUIRED_COLUMNS - set(df.columns)\n",
        "        if missing_cols:\n",
        "            raise DataValidationError(f\"Missing required columns: {missing_cols}\")\n",
        "\n",
        "        if not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
        "            try:\n",
        "                pd.to_datetime(df['date'])\n",
        "            except Exception as e:\n",
        "                raise DataValidationError(f\"Invalid date format: {e}\")\n",
        "\n",
        "        price_cols = ['open', 'high', 'low', 'close']\n",
        "        for col in price_cols:\n",
        "            if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "                raise DataValidationError(f\"Column {col} must be numeric\")\n",
        "\n",
        "        invalid_rows = df[df['high'] < df['low']].index\n",
        "        if len(invalid_rows) > 0:\n",
        "            raise DataValidationError(f\"Found {len(invalid_rows)} rows where high < low\")\n",
        "\n",
        "    def process_files(self, input_dir: str = \"input/\") -> None:\n",
        "        try:\n",
        "            os.makedirs(\"parent_output\", exist_ok=True)\n",
        "            os.makedirs(\"processed_output\", exist_ok=True)\n",
        "\n",
        "            file_count = 0\n",
        "            for filename in os.listdir(input_dir):\n",
        "                if filename.endswith(\".csv\"):\n",
        "                    ticker = filename.replace(\".csv\", \"\")\n",
        "                    self.logger.info(f\"Processing {ticker}\")\n",
        "                    self.process_single_file(os.path.join(input_dir, filename), ticker)\n",
        "                    file_count += 1\n",
        "\n",
        "            self.logger.info(f\"Successfully processed {file_count} files\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing files: {e}\")\n",
        "            raise\n",
        "\n",
        "    def process_single_file(self, filepath: str, ticker: str) -> None:\n",
        "        try:\n",
        "            # Load and validate data\n",
        "            df = pd.read_csv(filepath)\n",
        "            df['date'] = pd.to_datetime(df['date'])\n",
        "            self.validate_dataframe(df)\n",
        "\n",
        "            # Add period column and initialize fields\n",
        "            df['parent_period'] = df['date'].dt.to_period(self.parent_period)\n",
        "\n",
        "            numeric_cols = {\n",
        "                'intra_period_count': 'int64',\n",
        "                'intra_period_high': 'float64',\n",
        "                'intra_period_low': 'float64',\n",
        "                'intra_period_reu': 'float64',\n",
        "                'intra_period_red': 'float64',\n",
        "                'range_expansion_flag': 'int64',\n",
        "                'bar_rpc': 'int64',\n",
        "                'intra_period_cumulative_rpc': 'int64'\n",
        "            }\n",
        "\n",
        "            for col, dtype in numeric_cols.items():\n",
        "                df[col] = pd.Series(0, index=df.index, dtype=dtype)\n",
        "\n",
        "            df['rpc_direction'] = 'N'\n",
        "\n",
        "            # Process each parent period\n",
        "            for period in df['parent_period'].unique():\n",
        "                mask = df['parent_period'] == period\n",
        "                period_data = df[mask].copy()\n",
        "                processed_period = self._process_parent_period(period_data)\n",
        "                df.loc[mask] = processed_period.astype(df[mask].dtypes)\n",
        "\n",
        "            # Calculate parent stats\n",
        "            parent_stats = self._calculate_parent_stats(df)\n",
        "\n",
        "            # Save outputs\n",
        "            self._safe_save_csv(parent_stats, f\"parent_output/{ticker}_parent.csv\")\n",
        "            self._safe_save_csv(df, f\"processed_output/{ticker}_processed.csv\")\n",
        "\n",
        "            self.logger.info(f\"Successfully processed {ticker}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing {ticker}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _safe_save_csv(self, df: pd.DataFrame, filepath: str) -> None:\n",
        "        try:\n",
        "            df.to_csv(filepath, index=False)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error saving to {filepath}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_parent_stats(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        try:\n",
        "            # Basic stats\n",
        "            parent_stats = df.groupby('parent_period').agg({\n",
        "                'date': 'first',\n",
        "                'high': 'max',\n",
        "                'low': 'min',\n",
        "                'range_expansion_flag': 'sum'  # Count\n",
        "            }).reset_index()\n",
        "\n",
        "            parent_stats.columns = ['parent_period', 'date', 'parent_high', 'parent_low', 'duration']\n",
        "\n",
        "            # Initialize supplementary columns\n",
        "            parent_stats['bar_of_high'] = 0\n",
        "            parent_stats['bar_of_low'] = 0\n",
        "            parent_stats['expansion_count'] = 0\n",
        "            parent_stats['rpc_count'] = 0\n",
        "\n",
        "            # Calculate additional stats per period\n",
        "            for period in df['parent_period'].unique():\n",
        "                period_data = df[df['parent_period'] == period]\n",
        "                period_high = parent_stats.loc[parent_stats['parent_period'] == period, 'parent_high'].iloc[0]\n",
        "                period_low = parent_stats.loc[parent_stats['parent_period'] == period, 'parent_low'].iloc[0]\n",
        "\n",
        "                # First occurrence of high/low\n",
        "                high_idx = period_data[period_data['high'] == period_high].index.min()\n",
        "                low_idx = period_data[period_data['low'] == period_low].index.min()\n",
        "\n",
        "                idx = parent_stats['parent_period'] == period\n",
        "                if high_idx is not None:\n",
        "                    high_bar = period_data.loc[:high_idx, 'high'].notna().sum()\n",
        "                    parent_stats.loc[idx, 'bar_of_high'] = high_bar\n",
        "\n",
        "                if low_idx is not None:\n",
        "                    low_bar = period_data.loc[:low_idx, 'low'].notna().sum()\n",
        "                    parent_stats.loc[idx, 'bar_of_low'] = low_bar\n",
        "\n",
        "                # Count expansions and RPCs\n",
        "                parent_stats.loc[idx, 'expansion_count'] = period_data['range_expansion_flag'].sum()\n",
        "                parent_stats.loc[idx, 'rpc_count'] = period_data['intra_period_cumulative_rpc'].max()\n",
        "\n",
        "            return parent_stats\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error calculating parent stats: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _process_parent_period(self, period_data: pd.DataFrame) -> pd.DataFrame:\n",
        "        try:\n",
        "            active_bars = 0\n",
        "            cumulative_rpc = 0\n",
        "            prev_direction = 'N'\n",
        "            prior_intra_high = None\n",
        "            prior_intra_low = None\n",
        "\n",
        "            for idx in period_data.index:\n",
        "                row = period_data.loc[idx]\n",
        "\n",
        "                if pd.isna(row['high']) or pd.isna(row['low']):\n",
        "                    continue\n",
        "\n",
        "                active_bars += 1\n",
        "                period_data.at[idx, 'intra_period_count'] = active_bars\n",
        "\n",
        "                # Calculate range expansions\n",
        "                reu = max(0, row['high'] - prior_intra_high) if prior_intra_high is not None else 0\n",
        "                red = max(0, prior_intra_low - row['low']) if prior_intra_low is not None else 0\n",
        "\n",
        "                # Update metrics\n",
        "                period_data.at[idx, 'intra_period_reu'] = reu\n",
        "                period_data.at[idx, 'intra_period_red'] = red\n",
        "                period_data.at[idx, 'range_expansion_flag'] = 1 if (reu > 0 or red > 0) else 0\n",
        "\n",
        "                # Calculate RPC\n",
        "                new_direction = self._determine_rpc_direction(\n",
        "                    reu, red, row['high'], row['low'], row['close'], prev_direction\n",
        "                )\n",
        "\n",
        "                if new_direction != prev_direction and prev_direction != 'N':\n",
        "                    cumulative_rpc += 1\n",
        "\n",
        "                period_data.at[idx, 'rpc_direction'] = new_direction\n",
        "                period_data.at[idx, 'intra_period_cumulative_rpc'] = cumulative_rpc\n",
        "                period_data.at[idx, 'bar_rpc'] = self._calculate_bar_rpc(\n",
        "                    reu, red, prev_direction, new_direction\n",
        "                )\n",
        "\n",
        "                # Update prior values for next iteration\n",
        "                prior_intra_high = max(row['high'], prior_intra_high if prior_intra_high is not None else row['high'])\n",
        "                prior_intra_low = min(row['low'], prior_intra_low if prior_intra_low is not None else row['low'])\n",
        "\n",
        "                prev_direction = new_direction\n",
        "\n",
        "            return period_data\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing parent period: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _determine_rpc_direction(\n",
        "        self, reu: float, red: float,\n",
        "        high: float, low: float, close: float,\n",
        "        prev_direction: str\n",
        "    ) -> str:\n",
        "        try:\n",
        "            if reu == 0 and red == 0:\n",
        "                return prev_direction\n",
        "            elif reu > 0 and red > 0:\n",
        "                return 'U' if (close - low) / (high - low) >= 0.5 else 'D'\n",
        "            elif reu > 0:\n",
        "                return 'U'\n",
        "            else:\n",
        "                return 'D'\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error determining RPC direction: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_bar_rpc(\n",
        "        self, reu: float, red: float,\n",
        "        prev_direction: str, new_direction: str\n",
        "    ) -> int:\n",
        "        try:\n",
        "            if reu == 0 and red == 0:\n",
        "                return 0\n",
        "            elif reu > 0 and red > 0:\n",
        "                if prev_direction == 'N' or new_direction != prev_direction:\n",
        "                    return 2\n",
        "                return 1\n",
        "            elif new_direction != prev_direction and prev_direction != 'N':\n",
        "                return 1\n",
        "            return 0\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error calculating bar RPC: {e}\")\n",
        "            raise\n",
        "\n",
        "class TestTemporalFramework:\n",
        "    def run_all_tests(self):\n",
        "        processor = ConnectTemporalPeriods()\n",
        "\n",
        "        assert processor._calculate_bar_rpc(0, 0, 'N', 'N') == 0, \"No expansion should return 0\"\n",
        "        assert processor._calculate_bar_rpc(1, 1, 'N', 'U') == 2, \"Two-way expansion should return 2\"\n",
        "        assert processor._calculate_bar_rpc(1, 0, 'D', 'U') == 1, \"Direction change should return 1\"\n",
        "\n",
        "        assert processor._determine_rpc_direction(0, 0, 100, 90, 95, 'U') == 'U', \"No expansion should maintain direction\"\n",
        "        assert processor._determine_rpc_direction(1, 1, 100, 90, 97, 'N') == 'U', \"Close in upper half should return U\"\n",
        "        assert processor._determine_rpc_direction(1, 0, 100, 90, 95, 'D') == 'U', \"Upward expansion should return U\"\n",
        "\n",
        "        print(\"All tests passed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tests = TestTemporalFramework()\n",
        "    tests.run_all_tests()\n",
        "\n",
        "    processor = ConnectTemporalPeriods()\n",
        "    processor.process_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oOJcXumZJPy",
        "outputId": "9ef98998-b9f4-44f6-c575-3077a6f4560d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import logging\n",
        "from typing import Tuple, Dict, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('temporal_processing.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "@dataclass\n",
        "class ParentPeriodStats:\n",
        "    high: float\n",
        "    low: float\n",
        "    duration: int\n",
        "    completed_bars: int\n",
        "    rpc_count: int\n",
        "    bar_of_high: int\n",
        "    bar_of_low: int\n",
        "\n",
        "class DataValidationError(Exception):\n",
        "    pass\n",
        "\n",
        "class ConnectTemporalPeriods:\n",
        "    REQUIRED_COLUMNS = {'date', 'open', 'high', 'low', 'close'}\n",
        "\n",
        "    def __init__(self, child_period: str = \"D\", parent_period: str = \"M\"):\n",
        "        self.child_period = child_period\n",
        "        self.parent_period = parent_period\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def validate_dataframe(self, df: pd.DataFrame) -> None:\n",
        "        missing_cols = self.REQUIRED_COLUMNS - set(df.columns)\n",
        "        if missing_cols:\n",
        "            raise DataValidationError(f\"Missing required columns: {missing_cols}\")\n",
        "\n",
        "        if not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
        "            try:\n",
        "                pd.to_datetime(df['date'])\n",
        "            except Exception as e:\n",
        "                raise DataValidationError(f\"Invalid date format: {e}\")\n",
        "\n",
        "        price_cols = ['open', 'high', 'low', 'close']\n",
        "        for col in price_cols:\n",
        "            if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "                raise DataValidationError(f\"Column {col} must be numeric\")\n",
        "\n",
        "        invalid_rows = df[df['high'] < df['low']].index\n",
        "        if len(invalid_rows) > 0:\n",
        "            raise DataValidationError(f\"Found {len(invalid_rows)} rows where high < low\")\n",
        "\n",
        "    def process_files(self, input_dir: str = \"input/\") -> None:\n",
        "        try:\n",
        "            os.makedirs(\"parent_output\", exist_ok=True)\n",
        "            os.makedirs(\"processed_output\", exist_ok=True)\n",
        "\n",
        "            file_count = 0\n",
        "            for filename in os.listdir(input_dir):\n",
        "                if filename.endswith(\".csv\"):\n",
        "                    ticker = filename.replace(\".csv\", \"\")\n",
        "                    self.logger.info(f\"Processing {ticker}\")\n",
        "                    self.process_single_file(os.path.join(input_dir, filename), ticker)\n",
        "                    file_count += 1\n",
        "\n",
        "            self.logger.info(f\"Successfully processed {file_count} files\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing files: {e}\")\n",
        "            raise\n",
        "\n",
        "    def process_single_file(self, filepath: str, ticker: str) -> None:\n",
        "        try:\n",
        "            # Load and validate data\n",
        "            df = pd.read_csv(filepath)\n",
        "            df['date'] = pd.to_datetime(df['date'])\n",
        "            self.validate_dataframe(df)\n",
        "\n",
        "            # Add period column and initialize fields\n",
        "            df['parent_period'] = df['date'].dt.to_period(self.parent_period)\n",
        "\n",
        "            numeric_cols = {\n",
        "                'intra_period_count': 'int64',\n",
        "                'intra_period_high': 'float64',\n",
        "                'intra_period_low': 'float64',\n",
        "                'intra_period_reu': 'float64',\n",
        "                'intra_period_red': 'float64',\n",
        "                'range_expansion_flag': 'int64',\n",
        "                'bar_rpc': 'int64',\n",
        "                'intra_period_cumulative_rpc': 'int64'\n",
        "            }\n",
        "\n",
        "            for col, dtype in numeric_cols.items():\n",
        "                df[col] = pd.Series(0, index=df.index, dtype=dtype)\n",
        "\n",
        "            df['rpc_direction'] = 'N'\n",
        "\n",
        "            # Process each parent period\n",
        "            for period in df['parent_period'].unique():\n",
        "                mask = df['parent_period'] == period\n",
        "                period_data = df[mask].copy()\n",
        "                processed_period = self._process_parent_period(period_data)\n",
        "                df.loc[mask] = processed_period.astype(df[mask].dtypes)\n",
        "\n",
        "            # Calculate parent stats\n",
        "            parent_stats = self._calculate_parent_stats(df)\n",
        "\n",
        "            # Save outputs\n",
        "            self._safe_save_csv(parent_stats, f\"parent_output/{ticker}_parent.csv\")\n",
        "            self._safe_save_csv(df, f\"processed_output/{ticker}_processed.csv\")\n",
        "\n",
        "            self.logger.info(f\"Successfully processed {ticker}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing {ticker}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _safe_save_csv(self, df: pd.DataFrame, filepath: str) -> None:\n",
        "        try:\n",
        "            df.to_csv(filepath, index=False)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error saving to {filepath}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_parent_stats(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        try:\n",
        "            # Basic stats\n",
        "            parent_stats = df.groupby('parent_period').agg({\n",
        "                'date': 'first',\n",
        "                'high': 'max',\n",
        "                'low': 'min',\n",
        "                'range_expansion_flag': 'sum'  # Count\n",
        "            }).reset_index()\n",
        "\n",
        "            parent_stats.columns = ['parent_period', 'date', 'parent_high', 'parent_low', 'duration']\n",
        "\n",
        "            # Initialize supplementary columns\n",
        "            parent_stats['bar_of_high'] = 0\n",
        "            parent_stats['bar_of_low'] = 0\n",
        "            parent_stats['expansion_count'] = 0\n",
        "            parent_stats['rpc_count'] = 0\n",
        "\n",
        "            # Calculate additional stats per period\n",
        "            for period in df['parent_period'].unique():\n",
        "                period_data = df[df['parent_period'] == period]\n",
        "                period_high = parent_stats.loc[parent_stats['parent_period'] == period, 'parent_high'].iloc[0]\n",
        "                period_low = parent_stats.loc[parent_stats['parent_period'] == period, 'parent_low'].iloc[0]\n",
        "\n",
        "                # First occurrence of high/low\n",
        "                high_idx = period_data[period_data['high'] == period_high].index.min()\n",
        "                low_idx = period_data[period_data['low'] == period_low].index.min()\n",
        "\n",
        "                idx = parent_stats['parent_period'] == period\n",
        "                if high_idx is not None:\n",
        "                    high_bar = period_data.loc[:high_idx, 'high'].notna().sum()\n",
        "                    parent_stats.loc[idx, 'bar_of_high'] = high_bar\n",
        "\n",
        "                if low_idx is not None:\n",
        "                    low_bar = period_data.loc[:low_idx, 'low'].notna().sum()\n",
        "                    parent_stats.loc[idx, 'bar_of_low'] = low_bar\n",
        "\n",
        "                # Count expansions and RPCs\n",
        "                parent_stats.loc[idx, 'expansion_count'] = period_data['range_expansion_flag'].sum()\n",
        "                parent_stats.loc[idx, 'rpc_count'] = period_data['intra_period_cumulative_rpc'].max()\n",
        "\n",
        "            return parent_stats\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error calculating parent stats: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _process_parent_period(self, period_data: pd.DataFrame) -> pd.DataFrame:\n",
        "        try:\n",
        "            active_bars = 0\n",
        "            cumulative_rpc = 0\n",
        "            prev_direction = None  # Changed from 'N' to None for initial direction\n",
        "            prior_intra_high = None\n",
        "            prior_intra_low = None\n",
        "\n",
        "            for idx in period_data.index:\n",
        "                row = period_data.loc[idx]\n",
        "\n",
        "                if pd.isna(row['high']) or pd.isna(row['low']):\n",
        "                    continue\n",
        "\n",
        "                active_bars += 1\n",
        "                period_data.at[idx, 'intra_period_count'] = active_bars\n",
        "\n",
        "                # Calculate range expansions\n",
        "                reu = max(0, row['high'] - prior_intra_high) if prior_intra_high is not None else 0\n",
        "                red = max(0, prior_intra_low - row['low']) if prior_intra_low is not None else 0\n",
        "\n",
        "                # Update metrics\n",
        "                period_data.at[idx, 'intra_period_reu'] = reu\n",
        "                period_data.at[idx, 'intra_period_red'] = red\n",
        "                period_data.at[idx, 'range_expansion_flag'] = 1 if (reu > 0 or red > 0) else 0\n",
        "\n",
        "                # Calculate RPC\n",
        "                new_direction = self._determine_rpc_direction(\n",
        "                    reu, red, row['high'], row['low'], row['close'], prev_direction if prev_direction else 'N'\n",
        "                )\n",
        "\n",
        "                # First movement establishes direction without incrementing RPC\n",
        "                if prev_direction is None and (reu > 0 or red > 0):\n",
        "                    prev_direction = new_direction\n",
        "                elif new_direction != prev_direction and prev_direction is not None:\n",
        "                    cumulative_rpc += 1\n",
        "\n",
        "                period_data.at[idx, 'rpc_direction'] = new_direction\n",
        "                period_data.at[idx, 'intra_period_cumulative_rpc'] = cumulative_rpc\n",
        "                period_data.at[idx, 'bar_rpc'] = self._calculate_bar_rpc(\n",
        "                    reu, red, prev_direction if prev_direction else 'N', new_direction\n",
        "                )\n",
        "\n",
        "                # Update prior values for next iteration\n",
        "                prior_intra_high = max(row['high'], prior_intra_high if prior_intra_high is not None else row['high'])\n",
        "                prior_intra_low = min(row['low'], prior_intra_low if prior_intra_low is not None else row['low'])\n",
        "\n",
        "                if reu > 0 or red > 0:  # Only update direction if there's movement\n",
        "                    prev_direction = new_direction\n",
        "\n",
        "            return period_data\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing parent period: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _determine_rpc_direction(\n",
        "        self, reu: float, red: float,\n",
        "        high: float, low: float, close: float,\n",
        "        prev_direction: str\n",
        "    ) -> str:\n",
        "        try:\n",
        "            if reu == 0 and red == 0:\n",
        "                return prev_direction\n",
        "            elif reu > 0 and red > 0:\n",
        "                return 'U' if (close - low) / (high - low) >= 0.5 else 'D'\n",
        "            elif reu > 0:\n",
        "                return 'U'\n",
        "            else:\n",
        "                return 'D'\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error determining RPC direction: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_bar_rpc(\n",
        "        self, reu: float, red: float,\n",
        "        prev_direction: str, new_direction: str\n",
        "    ) -> int:\n",
        "        try:\n",
        "            if reu == 0 and red == 0:\n",
        "                return 0\n",
        "            elif reu > 0 and red > 0:\n",
        "                if prev_direction == 'N' or new_direction != prev_direction:\n",
        "                    return 2\n",
        "                return 1\n",
        "            elif new_direction != prev_direction and prev_direction != 'N':\n",
        "                return 1\n",
        "            return 0\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error calculating bar RPC: {e}\")\n",
        "            raise\n",
        "\n",
        "class TestTemporalFramework:\n",
        "    def run_all_tests(self):\n",
        "        processor = ConnectTemporalPeriods()\n",
        "\n",
        "        assert processor._calculate_bar_rpc(0, 0, 'N', 'N') == 0, \"No expansion should return 0\"\n",
        "        assert processor._calculate_bar_rpc(1, 1, 'N', 'U') == 2, \"Two-way expansion should return 2\"\n",
        "        assert processor._calculate_bar_rpc(1, 0, 'D', 'U') == 1, \"Direction change should return 1\"\n",
        "\n",
        "        assert processor._determine_rpc_direction(0, 0, 100, 90, 95, 'U') == 'U', \"No expansion should maintain direction\"\n",
        "        assert processor._determine_rpc_direction(1, 1, 100, 90, 97, 'N') == 'U', \"Close in upper half should return U\"\n",
        "        assert processor._determine_rpc_direction(1, 0, 100, 90, 95, 'D') == 'U', \"Upward expansion should return U\"\n",
        "\n",
        "        print(\"All tests passed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tests = TestTemporalFramework()\n",
        "    tests.run_all_tests()\n",
        "\n",
        "    processor = ConnectTemporalPeriods()\n",
        "    processor.process_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5c3VWi-ZbBB",
        "outputId": "e8a9c3c6-9a6c-424e-fb7d-5ff4645ab7d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import logging\n",
        "from typing import Tuple, Dict, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('temporal_processing.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "@dataclass\n",
        "class ParentPeriodStats:\n",
        "    high: float\n",
        "    low: float\n",
        "    duration: int\n",
        "    completed_bars: int\n",
        "    rpc_count: int\n",
        "    bar_of_high: int\n",
        "    bar_of_low: int\n",
        "\n",
        "class DataValidationError(Exception):\n",
        "    pass\n",
        "\n",
        "class ConnectTemporalPeriods:\n",
        "    REQUIRED_COLUMNS = {'date', 'open', 'high', 'low', 'close'}\n",
        "\n",
        "    def __init__(self, child_period: str = \"D\", parent_period: str = \"M\"):\n",
        "        self.child_period = child_period\n",
        "        self.parent_period = parent_period\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def validate_dataframe(self, df: pd.DataFrame) -> None:\n",
        "        missing_cols = self.REQUIRED_COLUMNS - set(df.columns)\n",
        "        if missing_cols:\n",
        "            raise DataValidationError(f\"Missing required columns: {missing_cols}\")\n",
        "\n",
        "        if not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
        "            try:\n",
        "                pd.to_datetime(df['date'])\n",
        "            except Exception as e:\n",
        "                raise DataValidationError(f\"Invalid date format: {e}\")\n",
        "\n",
        "        price_cols = ['open', 'high', 'low', 'close']\n",
        "        for col in price_cols:\n",
        "            if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "                raise DataValidationError(f\"Column {col} must be numeric\")\n",
        "\n",
        "        invalid_rows = df[df['high'] < df['low']].index\n",
        "        if len(invalid_rows) > 0:\n",
        "            raise DataValidationError(f\"Found {len(invalid_rows)} rows where high < low\")\n",
        "\n",
        "    def process_files(self, input_dir: str = \"input/\") -> None:\n",
        "        try:\n",
        "            os.makedirs(\"parent_output\", exist_ok=True)\n",
        "            os.makedirs(\"processed_output\", exist_ok=True)\n",
        "\n",
        "            file_count = 0\n",
        "            for filename in os.listdir(input_dir):\n",
        "                if filename.endswith(\".csv\"):\n",
        "                    ticker = filename.replace(\".csv\", \"\")\n",
        "                    self.logger.info(f\"Processing {ticker}\")\n",
        "                    self.process_single_file(os.path.join(input_dir, filename), ticker)\n",
        "                    file_count += 1\n",
        "\n",
        "            self.logger.info(f\"Successfully processed {file_count} files\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing files: {e}\")\n",
        "            raise\n",
        "\n",
        "    def process_single_file(self, filepath: str, ticker: str) -> None:\n",
        "        try:\n",
        "            # Load and validate data\n",
        "            df = pd.read_csv(filepath)\n",
        "            df['date'] = pd.to_datetime(df['date'])\n",
        "            self.validate_dataframe(df)\n",
        "\n",
        "            # Add period column and initialize fields\n",
        "            df['parent_period'] = df['date'].dt.to_period(self.parent_period)\n",
        "\n",
        "            numeric_cols = {\n",
        "                'intra_period_count': 'int64',\n",
        "                'intra_period_high': 'float64',\n",
        "                'intra_period_low': 'float64',\n",
        "                'intra_period_reu': 'float64',\n",
        "                'intra_period_red': 'float64',\n",
        "                'range_expansion_flag': 'int64',\n",
        "                'bar_rpc': 'int64',\n",
        "                'intra_period_cumulative_rpc': 'int64'\n",
        "            }\n",
        "\n",
        "            for col, dtype in numeric_cols.items():\n",
        "                df[col] = pd.Series(0, index=df.index, dtype=dtype)\n",
        "\n",
        "            df['rpc_direction'] = 'N'\n",
        "\n",
        "            # Process each parent period\n",
        "            for period in df['parent_period'].unique():\n",
        "                mask = df['parent_period'] == period\n",
        "                period_data = df[mask].copy()\n",
        "                processed_period = self._process_parent_period(period_data)\n",
        "                df.loc[mask] = processed_period.astype(df[mask].dtypes)\n",
        "\n",
        "            # Calculate parent stats\n",
        "            parent_stats = self._calculate_parent_stats(df)\n",
        "\n",
        "            # Save outputs\n",
        "            self._safe_save_csv(parent_stats, f\"parent_output/{ticker}_parent.csv\")\n",
        "            self._safe_save_csv(df, f\"processed_output/{ticker}_processed.csv\")\n",
        "\n",
        "            self.logger.info(f\"Successfully processed {ticker}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing {ticker}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _safe_save_csv(self, df: pd.DataFrame, filepath: str) -> None:\n",
        "        try:\n",
        "            df.to_csv(filepath, index=False)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error saving to {filepath}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_parent_stats(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        try:\n",
        "            # Basic stats\n",
        "            parent_stats = df.groupby('parent_period').agg({\n",
        "                'date': 'first',\n",
        "                'high': 'max',\n",
        "                'low': 'min',\n",
        "                'range_expansion_flag': 'sum'  # Count\n",
        "            }).reset_index()\n",
        "\n",
        "            parent_stats.columns = ['parent_period', 'date', 'parent_high', 'parent_low', 'duration']\n",
        "\n",
        "            # Initialize supplementary columns\n",
        "            parent_stats['bar_of_high'] = 0\n",
        "            parent_stats['bar_of_low'] = 0\n",
        "            parent_stats['expansion_count'] = 0\n",
        "            parent_stats['rpc_count'] = 0\n",
        "\n",
        "            # Calculate additional stats per period\n",
        "            for period in df['parent_period'].unique():\n",
        "                period_data = df[df['parent_period'] == period]\n",
        "                period_high = parent_stats.loc[parent_stats['parent_period'] == period, 'parent_high'].iloc[0]\n",
        "                period_low = parent_stats.loc[parent_stats['parent_period'] == period, 'parent_low'].iloc[0]\n",
        "\n",
        "                # First occurrence of high/low\n",
        "                high_idx = period_data[period_data['high'] == period_high].index.min()\n",
        "                low_idx = period_data[period_data['low'] == period_low].index.min()\n",
        "\n",
        "                idx = parent_stats['parent_period'] == period\n",
        "                if high_idx is not None:\n",
        "                    high_bar = period_data.loc[:high_idx, 'high'].notna().sum()\n",
        "                    parent_stats.loc[idx, 'bar_of_high'] = high_bar\n",
        "\n",
        "                if low_idx is not None:\n",
        "                    low_bar = period_data.loc[:low_idx, 'low'].notna().sum()\n",
        "                    parent_stats.loc[idx, 'bar_of_low'] = low_bar\n",
        "\n",
        "                # Count expansions and RPCs\n",
        "                parent_stats.loc[idx, 'expansion_count'] = period_data['range_expansion_flag'].sum()\n",
        "                parent_stats.loc[idx, 'rpc_count'] = period_data['intra_period_cumulative_rpc'].max()\n",
        "\n",
        "            return parent_stats\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error calculating parent stats: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _process_parent_period(self, period_data: pd.DataFrame) -> pd.DataFrame:\n",
        "        try:\n",
        "            active_bars = 0\n",
        "            cumulative_rpc = 0\n",
        "            prev_direction = None\n",
        "            prior_intra_high = None\n",
        "            prior_intra_low = None\n",
        "\n",
        "            for idx in period_data.index:\n",
        "                row = period_data.loc[idx]\n",
        "\n",
        "                if pd.isna(row['high']) or pd.isna(row['low']):\n",
        "                    continue\n",
        "\n",
        "                active_bars += 1\n",
        "                period_data.at[idx, 'intra_period_count'] = active_bars\n",
        "\n",
        "                # Calculate expansions\n",
        "                reu = max(0, row['high'] - prior_intra_high) if prior_intra_high is not None else 0\n",
        "                red = max(0, prior_intra_low - row['low']) if prior_intra_low is not None else 0\n",
        "\n",
        "                # Update metrics\n",
        "                period_data.at[idx, 'intra_period_reu'] = reu\n",
        "                period_data.at[idx, 'intra_period_red'] = red\n",
        "                has_expansion = (reu > 0 or red > 0)\n",
        "                period_data.at[idx, 'range_expansion_flag'] = 1 if has_expansion else 0\n",
        "\n",
        "                # Calculate bar's RPC\n",
        "                if has_expansion:\n",
        "                    new_direction = self._determine_rpc_direction(\n",
        "                        reu, red, row['high'], row['low'], row['close'], prev_direction if prev_direction else 'N'\n",
        "                    )\n",
        "\n",
        "                    if prev_direction is None:\n",
        "                        # First direction establishes\n",
        "                        prev_direction = new_direction\n",
        "                        period_data.at[idx, 'bar_rpc'] = 1\n",
        "                        cumulative_rpc += 1\n",
        "                    elif new_direction != prev_direction:\n",
        "                        # New direction counts as RPC\n",
        "                        period_data.at[idx, 'bar_rpc'] = 1\n",
        "                        cumulative_rpc += 1\n",
        "                        prev_direction = new_direction\n",
        "                    else:\n",
        "                        # Expansion in same direction\n",
        "                        period_data.at[idx, 'bar_rpc'] = 0\n",
        "                else:\n",
        "                    new_direction = prev_direction if prev_direction else 'N'\n",
        "                    period_data.at[idx, 'bar_rpc'] = 0\n",
        "\n",
        "                # Update priors\n",
        "                prior_intra_high = max(row['high'], prior_intra_high if prior_intra_high is not None else row['high'])\n",
        "                prior_intra_low = min(row['low'], prior_intra_low if prior_intra_low is not None else row['low'])\n",
        "\n",
        "            return period_data\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing parent period: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _determine_rpc_direction(\n",
        "        self, reu: float, red: float,\n",
        "        high: float, low: float, close: float,\n",
        "        prev_direction: str\n",
        "    ) -> str:\n",
        "        try:\n",
        "            if reu == 0 and red == 0:\n",
        "                return prev_direction\n",
        "            elif reu > 0 and red > 0:\n",
        "                return 'U' if (close - low) / (high - low) >= 0.5 else 'D'\n",
        "            elif reu > 0:\n",
        "                return 'U'\n",
        "            else:\n",
        "                return 'D'\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error determining RPC direction: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_bar_rpc(\n",
        "        self, reu: float, red: float,\n",
        "        prev_direction: str, new_direction: str\n",
        "    ) -> int:\n",
        "        try:\n",
        "            if reu == 0 and red == 0:\n",
        "                return 0\n",
        "            elif reu > 0 and red > 0:\n",
        "                if prev_direction == 'N' or new_direction != prev_direction:\n",
        "                    return 2\n",
        "                return 1\n",
        "            elif new_direction != prev_direction and prev_direction != 'N':\n",
        "                return 1\n",
        "            return 0\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error calculating bar RPC: {e}\")\n",
        "            raise\n",
        "\n",
        "class TestTemporalFramework:\n",
        "    def run_all_tests(self):\n",
        "        processor = ConnectTemporalPeriods()\n",
        "\n",
        "        assert processor._calculate_bar_rpc(0, 0, 'N', 'N') == 0, \"No expansion should return 0\"\n",
        "        assert processor._calculate_bar_rpc(1, 1, 'N', 'U') == 2, \"Two-way expansion should return 2\"\n",
        "        assert processor._calculate_bar_rpc(1, 0, 'D', 'U') == 1, \"Direction change should return 1\"\n",
        "\n",
        "        assert processor._determine_rpc_direction(0, 0, 100, 90, 95, 'U') == 'U', \"No expansion should maintain direction\"\n",
        "        assert processor._determine_rpc_direction(1, 1, 100, 90, 97, 'N') == 'U', \"Close in upper half should return U\"\n",
        "        assert processor._determine_rpc_direction(1, 0, 100, 90, 95, 'D') == 'U', \"Upward expansion should return U\"\n",
        "\n",
        "        print(\"All tests passed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tests = TestTemporalFramework()\n",
        "    tests.run_all_tests()\n",
        "\n",
        "    processor = ConnectTemporalPeriods()\n",
        "    processor.process_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cvWiIG_asi5",
        "outputId": "495da78a-414e-4e35-894e-a1e03275f1e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import logging\n",
        "from typing import Tuple, Dict, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('temporal_processing.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "@dataclass\n",
        "class ParentPeriodStats:\n",
        "    high: float\n",
        "    low: float\n",
        "    duration: int\n",
        "    completed_bars: int\n",
        "    rpc_count: int\n",
        "    bar_of_high: int\n",
        "    bar_of_low: int\n",
        "\n",
        "class DataValidationError(Exception):\n",
        "    pass\n",
        "\n",
        "class ConnectTemporalPeriods:\n",
        "    REQUIRED_COLUMNS = {'date', 'open', 'high', 'low', 'close'}\n",
        "\n",
        "    def __init__(self, child_period: str = \"D\", parent_period: str = \"M\"):\n",
        "        self.child_period = child_period\n",
        "        self.parent_period = parent_period\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def validate_dataframe(self, df: pd.DataFrame) -> None:\n",
        "        missing_cols = self.REQUIRED_COLUMNS - set(df.columns)\n",
        "        if missing_cols:\n",
        "            raise DataValidationError(f\"Missing required columns: {missing_cols}\")\n",
        "\n",
        "        if not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
        "            try:\n",
        "                pd.to_datetime(df['date'])\n",
        "            except Exception as e:\n",
        "                raise DataValidationError(f\"Invalid date format: {e}\")\n",
        "\n",
        "        price_cols = ['open', 'high', 'low', 'close']\n",
        "        for col in price_cols:\n",
        "            if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "                raise DataValidationError(f\"Column {col} must be numeric\")\n",
        "\n",
        "        invalid_rows = df[df['high'] < df['low']].index\n",
        "        if len(invalid_rows) > 0:\n",
        "            raise DataValidationError(f\"Found {len(invalid_rows)} rows where high < low\")\n",
        "\n",
        "    def process_files(self, input_dir: str = \"input/\") -> None:\n",
        "        try:\n",
        "            os.makedirs(\"parent_output\", exist_ok=True)\n",
        "            os.makedirs(\"processed_output\", exist_ok=True)\n",
        "\n",
        "            file_count = 0\n",
        "            for filename in os.listdir(input_dir):\n",
        "                if filename.endswith(\".csv\"):\n",
        "                    ticker = filename.replace(\".csv\", \"\")\n",
        "                    self.logger.info(f\"Processing {ticker}\")\n",
        "                    self.process_single_file(os.path.join(input_dir, filename), ticker)\n",
        "                    file_count += 1\n",
        "\n",
        "            self.logger.info(f\"Successfully processed {file_count} files\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing files: {e}\")\n",
        "            raise\n",
        "\n",
        "    def process_single_file(self, filepath: str, ticker: str) -> None:\n",
        "        try:\n",
        "            # Load and validate data\n",
        "            df = pd.read_csv(filepath)\n",
        "            df['date'] = pd.to_datetime(df['date'])\n",
        "            self.validate_dataframe(df)\n",
        "\n",
        "            # Add period column and initialize fields\n",
        "            df['parent_period'] = df['date'].dt.to_period(self.parent_period)\n",
        "\n",
        "            numeric_cols = {\n",
        "                'intra_period_count': 'int64',\n",
        "                'intra_period_high': 'float64',\n",
        "                'intra_period_low': 'float64',\n",
        "                'intra_period_reu': 'float64',\n",
        "                'intra_period_red': 'float64',\n",
        "                'range_expansion_flag': 'int64',\n",
        "                'bar_rpc': 'int64',\n",
        "                'intra_period_cumulative_rpc': 'int64'\n",
        "            }\n",
        "\n",
        "            for col, dtype in numeric_cols.items():\n",
        "                df[col] = pd.Series(0, index=df.index, dtype=dtype)\n",
        "\n",
        "            df['rpc_direction'] = 'N'\n",
        "\n",
        "            # Process each parent period\n",
        "            for period in df['parent_period'].unique():\n",
        "                mask = df['parent_period'] == period\n",
        "                period_data = df[mask].copy()\n",
        "                processed_period = self._process_parent_period(period_data)\n",
        "                df.loc[mask] = processed_period.astype(df[mask].dtypes)\n",
        "\n",
        "            # Calculate parent stats\n",
        "            parent_stats = self._calculate_parent_stats(df)\n",
        "\n",
        "            # Save outputs\n",
        "            self._safe_save_csv(parent_stats, f\"parent_output/{ticker}_parent.csv\")\n",
        "            self._safe_save_csv(df, f\"processed_output/{ticker}_processed.csv\")\n",
        "\n",
        "            self.logger.info(f\"Successfully processed {ticker}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing {ticker}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _safe_save_csv(self, df: pd.DataFrame, filepath: str) -> None:\n",
        "        try:\n",
        "            df.to_csv(filepath, index=False)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error saving to {filepath}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_parent_stats(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        try:\n",
        "            # Basic stats\n",
        "            parent_stats = df.groupby('parent_period').agg({\n",
        "                'date': 'first',\n",
        "                'high': 'max',\n",
        "                'low': 'min',\n",
        "                'range_expansion_flag': 'sum'  # Count\n",
        "            }).reset_index()\n",
        "\n",
        "            parent_stats.columns = ['parent_period', 'date', 'parent_high', 'parent_low', 'duration']\n",
        "\n",
        "            # Initialize supplementary columns\n",
        "            parent_stats['bar_of_high'] = 0\n",
        "            parent_stats['bar_of_low'] = 0\n",
        "            parent_stats['expansion_count'] = 0\n",
        "            parent_stats['rpc_count'] = 0\n",
        "\n",
        "            # Calculate additional stats per period\n",
        "            for period in df['parent_period'].unique():\n",
        "                period_data = df[df['parent_period'] == period]\n",
        "                period_high = parent_stats.loc[parent_stats['parent_period'] == period, 'parent_high'].iloc[0]\n",
        "                period_low = parent_stats.loc[parent_stats['parent_period'] == period, 'parent_low'].iloc[0]\n",
        "\n",
        "                # First occurrence of high/low\n",
        "                high_idx = period_data[period_data['high'] == period_high].index.min()\n",
        "                low_idx = period_data[period_data['low'] == period_low].index.min()\n",
        "\n",
        "                idx = parent_stats['parent_period'] == period\n",
        "                if high_idx is not None:\n",
        "                    high_bar = period_data.loc[:high_idx, 'high'].notna().sum()\n",
        "                    parent_stats.loc[idx, 'bar_of_high'] = high_bar\n",
        "\n",
        "                if low_idx is not None:\n",
        "                    low_bar = period_data.loc[:low_idx, 'low'].notna().sum()\n",
        "                    parent_stats.loc[idx, 'bar_of_low'] = low_bar\n",
        "\n",
        "                # Count expansions and RPCs\n",
        "                parent_stats.loc[idx, 'expansion_count'] = period_data['range_expansion_flag'].sum()\n",
        "                parent_stats.loc[idx, 'rpc_count'] = period_data['intra_period_cumulative_rpc'].max()\n",
        "\n",
        "            return parent_stats\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error calculating parent stats: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _process_parent_period(self, period_data: pd.DataFrame) -> pd.DataFrame:\n",
        "        try:\n",
        "            active_bars = 0\n",
        "            cumulative_rpc = 0\n",
        "            prev_direction = None\n",
        "            prior_intra_high = None\n",
        "            prior_intra_low = None\n",
        "\n",
        "            for idx in period_data.index:\n",
        "                row = period_data.loc[idx]\n",
        "\n",
        "                if pd.isna(row['high']) or pd.isna(row['low']):\n",
        "                    continue\n",
        "\n",
        "                active_bars += 1\n",
        "                period_data.at[idx, 'intra_period_count'] = active_bars\n",
        "\n",
        "                # Calculate expansions\n",
        "                reu = max(0, row['high'] - prior_intra_high) if prior_intra_high is not None else 0\n",
        "                red = max(0, prior_intra_low - row['low']) if prior_intra_low is not None else 0\n",
        "\n",
        "                # Update metrics\n",
        "                period_data.at[idx, 'intra_period_reu'] = reu\n",
        "                period_data.at[idx, 'intra_period_red'] = red\n",
        "                has_expansion = (reu > 0 or red > 0)\n",
        "                period_data.at[idx, 'range_expansion_flag'] = 1 if has_expansion else 0\n",
        "\n",
        "                # Calculate bar's RPC\n",
        "                if has_expansion:\n",
        "                    new_direction = self._determine_rpc_direction(\n",
        "                        reu, red, row['high'], row['low'], row['close'], prev_direction if prev_direction else 'N'\n",
        "                    )\n",
        "\n",
        "                    if prev_direction is None:\n",
        "                        # First direction establishes\n",
        "                        prev_direction = new_direction\n",
        "                        period_data.at[idx, 'bar_rpc'] = 1\n",
        "                        cumulative_rpc += 1\n",
        "                    elif new_direction != prev_direction:\n",
        "                        # New direction counts as RPC\n",
        "                        period_data.at[idx, 'bar_rpc'] = 1\n",
        "                        cumulative_rpc += 1\n",
        "                        prev_direction = new_direction\n",
        "                    else:\n",
        "                        # Expansion in same direction\n",
        "                        period_data.at[idx, 'bar_rpc'] = 0\n",
        "                else:\n",
        "                    new_direction = prev_direction if prev_direction else 'N'\n",
        "                    period_data.at[idx, 'bar_rpc'] = 0\n",
        "\n",
        "                period_data.at[idx, 'rpc_direction'] = new_direction\n",
        "                period_data.at[idx, 'intra_period_cumulative_rpc'] = cumulative_rpc\n",
        "\n",
        "                # Update priors\n",
        "                prior_intra_high = max(row['high'], prior_intra_high if prior_intra_high is not None else row['high'])\n",
        "                prior_intra_low = min(row['low'], prior_intra_low if prior_intra_low is not None else row['low'])\n",
        "\n",
        "            return period_data\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing parent period: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _determine_rpc_direction(\n",
        "        self, reu: float, red: float,\n",
        "        high: float, low: float, close: float,\n",
        "        prev_direction: str\n",
        "    ) -> str:\n",
        "        try:\n",
        "            if reu == 0 and red == 0:\n",
        "                return prev_direction\n",
        "            elif reu > 0 and red > 0:\n",
        "                return 'U' if (close - low) / (high - low) >= 0.5 else 'D'\n",
        "            elif reu > 0:\n",
        "                return 'U'\n",
        "            else:\n",
        "                return 'D'\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error determining RPC direction: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_bar_rpc(\n",
        "        self, reu: float, red: float,\n",
        "        prev_direction: str, new_direction: str\n",
        "    ) -> int:\n",
        "        try:\n",
        "            if reu == 0 and red == 0:\n",
        "                return 0\n",
        "            elif reu > 0 and red > 0:\n",
        "                if prev_direction == 'N' or new_direction != prev_direction:\n",
        "                    return 2\n",
        "                return 1\n",
        "            elif new_direction != prev_direction and prev_direction != 'N':\n",
        "                return 1\n",
        "            return 0\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error calculating bar RPC: {e}\")\n",
        "            raise\n",
        "\n",
        "class TestTemporalFramework:\n",
        "    def run_all_tests(self):\n",
        "        processor = ConnectTemporalPeriods()\n",
        "\n",
        "        assert processor._calculate_bar_rpc(0, 0, 'N', 'N') == 0, \"No expansion should return 0\"\n",
        "        assert processor._calculate_bar_rpc(1, 1, 'N', 'U') == 2, \"Two-way expansion should return 2\"\n",
        "        assert processor._calculate_bar_rpc(1, 0, 'D', 'U') == 1, \"Direction change should return 1\"\n",
        "\n",
        "        assert processor._determine_rpc_direction(0, 0, 100, 90, 95, 'U') == 'U', \"No expansion should maintain direction\"\n",
        "        assert processor._determine_rpc_direction(1, 1, 100, 90, 97, 'N') == 'U', \"Close in upper half should return U\"\n",
        "        assert processor._determine_rpc_direction(1, 0, 100, 90, 95, 'D') == 'U', \"Upward expansion should return U\"\n",
        "\n",
        "        print(\"All tests passed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tests = TestTemporalFramework()\n",
        "    tests.run_all_tests()\n",
        "\n",
        "    processor = ConnectTemporalPeriods()\n",
        "    processor.process_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNcpw9lot6pS",
        "outputId": "fa8fe517-275a-463d-f1f8-09e3e60da387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ]
    }
  ]
}