{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/hkUNNfWILSdcxXIgBaO2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BaronVonBussin/NewTransit/blob/main/Untitled57.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNvPG5WlAQGc",
        "outputId": "b8ae7cfd-c3a5-4509-cefb-f7f35b8672f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and exported: AAPL_D_1.csv\n",
            "Processed and exported: MMM_D_1.csv\n",
            "Processed and exported: AFL_D_1.csv\n"
          ]
        }
      ],
      "source": [
        "# Parameters\n",
        "child_period = \"D\"  # Default: \"D\" (Daily)\n",
        "parent_period = \"M\"  # Default: \"M\" (Monthly)\n",
        "jobname = \"BPB_20250105\"  # Job name to include in output files\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Input and output paths\n",
        "input_path = \"/content/input\"\n",
        "output_path = \"/content/output_gel\"\n",
        "parent_output_path = \"/content/output_parent\"\n",
        "\n",
        "# Ensure output directories exist\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "os.makedirs(parent_output_path, exist_ok=True)\n",
        "\n",
        "# Define the parent period lookup logic\n",
        "def assign_parent_lookup_date(row, child_period, parent_period):\n",
        "    \"\"\"\n",
        "    Assigns the parent_lookup_date based on the child and parent periods.\n",
        "    \"\"\"\n",
        "    if child_period == \"D\" and parent_period == \"W\":  # Daily to Weekly\n",
        "        year, week, _ = row['date'].isocalendar()\n",
        "        return f\"{year}/{week:02d}\"\n",
        "    elif child_period == \"D\" and parent_period == \"M\":  # Daily to Monthly\n",
        "        return row['date'].strftime('%Y/%m')\n",
        "    elif child_period == \"M\" and parent_period == \"Q\":  # Monthly to Quarterly\n",
        "        quarter = (row['date'].month - 1) // 3 + 1\n",
        "        return f\"{row['date'].year}/Q{quarter}\"\n",
        "    elif child_period == \"M\" and parent_period == \"Y\":  # Monthly to Yearly\n",
        "        return f\"{row['date'].year}\"\n",
        "    elif child_period == \"Q\" and parent_period == \"Y\":  # Quarterly to Yearly\n",
        "        return f\"{row['date'].year}/Q{(row['date'].month - 1) // 3 + 1}\"\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported child-parent combination: {child_period}:{parent_period}\")\n",
        "\n",
        "\n",
        "# Process each file in the input directory\n",
        "for file_name in os.listdir(input_path):\n",
        "    # Extract metadata from file name\n",
        "    if file_name.endswith('.csv'):\n",
        "        parts = file_name.split('_')\n",
        "        if len(parts) < 3:\n",
        "            print(f\"Skipping invalid file: {file_name}\")\n",
        "            continue\n",
        "\n",
        "        ticker, temporal_period, rolling_range_dur = parts[:3]\n",
        "        temporal_period = temporal_period.upper()\n",
        "\n",
        "        # Load the file\n",
        "        input_file = os.path.join(input_path, file_name)\n",
        "        data = pd.read_csv(input_file)\n",
        "\n",
        "        # Validate mandatory fields\n",
        "        required_columns = ['date', 'open', 'high', 'low', 'close']\n",
        "        for col in required_columns:\n",
        "            if col not in data.columns:\n",
        "                raise ValueError(f\"Mandatory field '{col}' missing in {file_name}\")\n",
        "\n",
        "        # Convert date column to datetime\n",
        "        data['date'] = pd.to_datetime(data['date'])\n",
        "\n",
        "        # Assign parent_lookup_date\n",
        "        data['parent_lookup_date'] = data.apply(\n",
        "            lambda row: assign_parent_lookup_date(row, period=temporal_period), axis=1\n",
        "        )\n",
        "\n",
        "        # Group and sort\n",
        "        data = data.sort_values(by=['parent_lookup_date', 'date'])\n",
        "        grouped = data.groupby('parent_lookup_date')\n",
        "\n",
        "        # Initialize parent fields\n",
        "        data['gel_reu_value'] = 0.0\n",
        "        data['gel_red_value'] = 0.0\n",
        "        data['gel_reu_flag'] = False\n",
        "        data['gel_red_flag'] = False\n",
        "        data['gel_re_flag'] = False\n",
        "\n",
        "        # Bar-by-bar processing\n",
        "        for name, group in grouped:\n",
        "            previous_high = None\n",
        "            previous_low = None\n",
        "\n",
        "            for idx, row in group.iterrows():\n",
        "                high = row['high']\n",
        "                low = row['low']\n",
        "\n",
        "                # Calculate reu and red values\n",
        "                if previous_high is not None and high > previous_high:\n",
        "                    data.loc[idx, 'gel_reu_value'] = high - previous_high\n",
        "                    data.loc[idx, 'gel_reu_flag'] = True\n",
        "\n",
        "                if previous_low is not None and low < previous_low:\n",
        "                    data.loc[idx, 'gel_red_value'] = previous_low - low\n",
        "                    data.loc[idx, 'gel_red_flag'] = True\n",
        "\n",
        "                # Set re_flag\n",
        "                data.loc[idx, 'gel_re_flag'] = (\n",
        "                    data.loc[idx, 'gel_reu_flag'] or data.loc[idx, 'gel_red_flag']\n",
        "                )\n",
        "\n",
        "                # Update previous values\n",
        "                previous_high = max(previous_high, high) if previous_high is not None else high\n",
        "                previous_low = min(previous_low, low) if previous_low is not None else low\n",
        "\n",
        "        # Generate summary metrics\n",
        "        summary = grouped.agg(\n",
        "            start_date=('date', 'min'),\n",
        "            end_date=('date', 'max'),\n",
        "            child_count=('date', 'count'),\n",
        "            reu_count=('gel_reu_flag', 'sum'),\n",
        "            red_count=('gel_red_flag', 'sum'),\n",
        "            total_rpc=('gel_re_flag', 'sum')\n",
        "        ).reset_index()\n",
        "\n",
        "        # Add metadata\n",
        "        summary['create_date'] = datetime.now().strftime('%Y-%m-%d')\n",
        "        summary['create_time'] = datetime.now().strftime('%H:%M:%S')\n",
        "        summary['jobname'] = f\"{ticker}_output_parent_{temporal_period}\"\n",
        "\n",
        "        # Export files\n",
        "        gel_output_file = os.path.join(output_path, f\"{ticker}_Gel_{temporal_period}.csv\")\n",
        "        parent_output_file = os.path.join(parent_output_path, f\"{ticker}_output_parent_{temporal_period}.csv\")\n",
        "\n",
        "        data.to_csv(gel_output_file, index=False)\n",
        "        summary.to_csv(parent_output_file, index=False)\n",
        "\n",
        "        print(f\"Processed and exported: {file_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "child_period = \"D\"  # Default: \"D\" (Daily)\n",
        "parent_period = \"M\"  # Default: \"M\" (Monthly)\n",
        "jobname = \"BPB_20250105\"  # Job name to include in output files\n",
        "\n",
        "# Paths\n",
        "input_path = \"/content/input\"\n",
        "output_path = \"/content/output_gel\"\n",
        "parent_output_path = \"/content/output_parent\"\n",
        "\n",
        "# Ensure output directories exist\n",
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "os.makedirs(parent_output_path, exist_ok=True)\n",
        "\n",
        "# Parent lookup logic\n",
        "def assign_parent_lookup_date(row, child_period, parent_period):\n",
        "    if child_period == \"D\" and parent_period == \"W\":\n",
        "        year, week, _ = row['date'].isocalendar()\n",
        "        return f\"{year}/{week:02d}\"\n",
        "    elif child_period == \"D\" and parent_period == \"M\":\n",
        "        return row['date'].strftime('%Y/%m')\n",
        "    elif child_period == \"M\" and parent_period == \"Q\":\n",
        "        quarter = (row['date'].month - 1) // 3 + 1\n",
        "        return f\"{row['date'].year}/Q{quarter}\"\n",
        "    elif child_period == \"M\" and parent_period == \"Y\":\n",
        "        return f\"{row['date'].year}\"\n",
        "    elif child_period == \"Q\" and parent_period == \"Y\":\n",
        "        return f\"{row['date'].year}/Q{(row['date'].month - 1) // 3 + 1}\"\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported child-parent combination: {child_period}:{parent_period}\")\n",
        "\n",
        "# Process each input file\n",
        "for file_name in os.listdir(input_path):\n",
        "    if file_name.endswith('.csv'):\n",
        "        parts = file_name.split('_')\n",
        "        if len(parts) < 3:\n",
        "            print(f\"Skipping invalid file: {file_name}\")\n",
        "            continue\n",
        "\n",
        "        ticker = parts[0]  # Extract Ticker\n",
        "        print(f\"Processing {file_name} for Ticker: {ticker}\")\n",
        "\n",
        "        # Load and validate data\n",
        "        input_file = os.path.join(input_path, file_name)\n",
        "        data = pd.read_csv(input_file)\n",
        "        data['date'] = pd.to_datetime(data['date'])\n",
        "        required_columns = ['date', 'open', 'high', 'low', 'close']\n",
        "        for col in required_columns:\n",
        "            if col not in data.columns:\n",
        "                raise ValueError(f\"Missing required column: {col} in {file_name}\")\n",
        "\n",
        "        # Assign parent_lookup_date\n",
        "        data['parent_lookup_date'] = data.apply(\n",
        "            lambda row: assign_parent_lookup_date(row, child_period, parent_period), axis=1\n",
        "        )\n",
        "\n",
        "        # Ensure mandatory and calculated columns\n",
        "        mandatory_columns = ['date', 'open', 'high', 'low', 'close', 'parent_lookup_date']\n",
        "        calculated_columns = [\n",
        "            'gel_reu_value', 'gel_red_value', 'gel_reu_flag', 'gel_red_flag',\n",
        "            'gel_re_flag', 'gel_range', 'gel_percent_r', 'gel_ce_percent', 'gel_epc_dir'\n",
        "        ]\n",
        "        for col in mandatory_columns + calculated_columns:\n",
        "            if col not in data.columns:\n",
        "                data[col] = None\n",
        "\n",
        "        # Export gel file\n",
        "        gel_output_file = os.path.join(output_path, f\"{ticker}_Gel_{child_period}.csv\")\n",
        "        data.to_csv(gel_output_file, index=False)\n",
        "\n",
        "        # Generate summary metrics for parent export\n",
        "        grouped = data.groupby('parent_lookup_date')\n",
        "        summary = grouped.agg(\n",
        "            start_date=('date', 'min'),\n",
        "            end_date=('date', 'max'),\n",
        "            child_count=('date', 'count'),\n",
        "            reu_count=('gel_reu_flag', 'sum'),\n",
        "            red_count=('gel_red_flag', 'sum'),\n",
        "            total_rpc=('gel_re_flag', 'sum')\n",
        "        ).reset_index()\n",
        "\n",
        "        # Add metadata to summary\n",
        "        summary['create_date'] = datetime.now().strftime('%Y-%m-%d')\n",
        "        summary['create_time'] = datetime.now().strftime('%H:%M:%S')\n",
        "        summary['jobname'] = jobname\n",
        "\n",
        "        # Export parent summary file\n",
        "        parent_output_file = os.path.join(parent_output_path, f\"{ticker}_output_parent_{parent_period}.csv\")\n",
        "        summary.to_csv(parent_output_file, index=False)\n",
        "\n",
        "        print(f\"Exported Gel File: {gel_output_file}\")\n",
        "        print(f\"Exported Parent Summary: {parent_output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5scn35L8ELig",
        "outputId": "42d5811e-d7e5-4f49-a797-d10f9bf96b64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing AAPL_D_1.csv for Ticker: AAPL\n",
            "Exported Gel File: /content/output_gel/AAPL_Gel_D.csv\n",
            "Exported Parent Summary: /content/output_parent/AAPL_output_parent_M.csv\n",
            "Processing MMM_D_1.csv for Ticker: MMM\n",
            "Exported Gel File: /content/output_gel/MMM_Gel_D.csv\n",
            "Exported Parent Summary: /content/output_parent/MMM_output_parent_M.csv\n",
            "Processing AFL_D_1.csv for Ticker: AFL\n",
            "Exported Gel File: /content/output_gel/AFL_Gel_D.csv\n",
            "Exported Parent Summary: /content/output_parent/AFL_output_parent_M.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Parameters\n",
        "child_period = \"D\"  # Default: \"D\" (Daily)\n",
        "parent_period = \"M\"  # Default: \"M\" (Monthly)\n",
        "jobname = \"BPB_20250105\"  # Job name to include in output files\n",
        "\n",
        "# Paths\n",
        "input_path = \"/content/input\"\n",
        "output_path = \"/content/output_gel\"\n",
        "parent_output_path = \"/content/output_parent\"\n",
        "\n",
        "# Ensure output directories exist\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "os.makedirs(parent_output_path, exist_ok=True)\n",
        "\n",
        "# Parent lookup logic\n",
        "def assign_parent_lookup_date(row, child_period, parent_period):\n",
        "    if child_period == \"D\" and parent_period == \"W\":\n",
        "        year, week, _ = row['date'].isocalendar()\n",
        "        return f\"{year}/{week:02d}\"\n",
        "    elif child_period == \"D\" and parent_period == \"M\":\n",
        "        return row['date'].strftime('%Y/%m')\n",
        "    elif child_period == \"M\" and parent_period == \"Q\":\n",
        "        quarter = (row['date'].month - 1) // 3 + 1\n",
        "        return f\"{row['date'].year}/Q{quarter}\"\n",
        "    elif child_period == \"M\" and parent_period == \"Y\":\n",
        "        return f\"{row['date'].year}\"\n",
        "    elif child_period == \"Q\" and parent_period == \"Y\":\n",
        "        return f\"{row['date'].year}/Q{(row['date'].month - 1) // 3 + 1}\"\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported child-parent combination: {child_period}:{parent_period}\")\n",
        "\n",
        "# Process each input file\n",
        "for file_name in os.listdir(input_path):\n",
        "    if file_name.endswith('.csv'):\n",
        "        parts = file_name.split('_')\n",
        "        if len(parts) < 3:\n",
        "            print(f\"Skipping invalid file: {file_name}\")\n",
        "            continue\n",
        "\n",
        "        ticker = parts[0]  # Extract Ticker\n",
        "        print(f\"Processing {file_name} for Ticker: {ticker}\")\n",
        "\n",
        "        # Load and validate data\n",
        "        input_file = os.path.join(input_path, file_name)\n",
        "        data = pd.read_csv(input_file)\n",
        "        data['date'] = pd.to_datetime(data['date'])\n",
        "        required_columns = ['date', 'open', 'high', 'low', 'close']\n",
        "        for col in required_columns:\n",
        "            if col not in data.columns:\n",
        "                raise ValueError(f\"Missing required column: {col} in {file_name}\")\n",
        "\n",
        "        # Assign parent_lookup_date\n",
        "        data['parent_lookup_date'] = data.apply(\n",
        "            lambda row: assign_parent_lookup_date(row, child_period, parent_period), axis=1\n",
        "        )\n",
        "\n",
        "        # Sort data for processing\n",
        "        data = data.sort_values(by=['parent_lookup_date', 'date'])\n",
        "\n",
        "        # Initialize calculated fields\n",
        "        calculated_fields = [\n",
        "            'gel_reu_value', 'gel_red_value', 'gel_reu_flag', 'gel_red_flag', 'gel_re_flag',\n",
        "            'gel_range', 'gel_percent_r', 'gel_ce_percent', 'gel_epc_dir', 'gel_epc',\n",
        "            'gel_epc_hp', 'gel_rpc', 'gel_e1_value', 'gel_e2_value', 'gel_fre_dir'\n",
        "        ]\n",
        "        for field in calculated_fields:\n",
        "            data[field] = None  # Initialize all calculated fields with None\n",
        "\n",
        "        # Process each parent group\n",
        "        grouped = data.groupby('parent_lookup_date')\n",
        "        for parent, group in grouped:\n",
        "            previous_high = None\n",
        "            previous_low = None\n",
        "\n",
        "            for idx, row in group.iterrows():\n",
        "                high = row['high']\n",
        "                low = row['low']\n",
        "\n",
        "                # Calculate gel_reu and gel_red values\n",
        "                gel_reu_value = max(0, high - previous_high) if previous_high is not None else 0\n",
        "                gel_red_value = max(0, previous_low - low) if previous_low is not None else 0\n",
        "\n",
        "                # Update calculated fields\n",
        "                data.at[idx, 'gel_reu_value'] = gel_reu_value\n",
        "                data.at[idx, 'gel_red_value'] = gel_red_value\n",
        "                data.at[idx, 'gel_reu_flag'] = gel_reu_value > 0\n",
        "                data.at[idx, 'gel_red_flag'] = gel_red_value > 0\n",
        "                data.at[idx, 'gel_re_flag'] = (gel_reu_value > 0) or (gel_red_value > 0)\n",
        "\n",
        "                # Update previous values\n",
        "                previous_high = max(previous_high, high) if previous_high is not None else high\n",
        "                previous_low = min(previous_low, low) if previous_low is not None else low\n",
        "\n",
        "        # Export Gel File\n",
        "        gel_output_file = os.path.join(output_path, f\"{ticker}_Gel_{child_period}.csv\")\n",
        "        data.to_csv(gel_output_file, index=False)\n",
        "\n",
        "        # Generate summary metrics for parent export\n",
        "        summary = grouped.agg(\n",
        "            start_date=('date', 'min'),\n",
        "            end_date=('date', 'max'),\n",
        "            child_count=('date', 'count'),\n",
        "            reu_count=('gel_reu_flag', 'sum'),\n",
        "            red_count=('gel_red_flag', 'sum'),\n",
        "            total_rpc=('gel_re_flag', 'sum')\n",
        "        ).reset_index()\n",
        "\n",
        "        # Add metadata to summary\n",
        "        summary['create_date'] = datetime.now().strftime('%Y-%m-%d')\n",
        "        summary['create_time'] = datetime.now().strftime('%H:%M:%S')\n",
        "        summary['jobname'] = jobname\n",
        "\n",
        "        # Export parent summary file\n",
        "        parent_output_file = os.path.join(parent_output_path, f\"{ticker}_output_parent_{parent_period}.csv\")\n",
        "        summary.to_csv(parent_output_file, index=False)\n",
        "\n",
        "        print(f\"Exported Gel File: {gel_output_file}\")\n",
        "        print(f\"Exported Parent Summary: {parent_output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8huxCQFlFJF4",
        "outputId": "4d36157a-88a9-4d8e-f84d-35a13c9627c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing AAPL_D_1.csv for Ticker: AAPL\n",
            "Exported Gel File: /content/output_gel/AAPL_Gel_D.csv\n",
            "Exported Parent Summary: /content/output_parent/AAPL_output_parent_M.csv\n",
            "Processing MMM_D_1.csv for Ticker: MMM\n",
            "Exported Gel File: /content/output_gel/MMM_Gel_D.csv\n",
            "Exported Parent Summary: /content/output_parent/MMM_output_parent_M.csv\n",
            "Processing AFL_D_1.csv for Ticker: AFL\n",
            "Exported Gel File: /content/output_gel/AFL_Gel_D.csv\n",
            "Exported Parent Summary: /content/output_parent/AFL_output_parent_M.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Parameters\n",
        "child_period = \"D\"  # Default: \"D\" (Daily)\n",
        "parent_period = \"M\"  # Default: \"M\" (Monthly)\n",
        "jobname = \"BPB_20250105\"  # Job name to include in output files\n",
        "\n",
        "# Paths\n",
        "input_path = \"/content/input\"\n",
        "output_path = \"/content/output_gel\"\n",
        "parent_output_path = \"/content/output_parent\"\n",
        "\n",
        "# Ensure output directories exist\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "os.makedirs(parent_output_path, exist_ok=True)\n",
        "\n",
        "# Parent lookup logic\n",
        "def assign_parent_lookup_date(row, child_period, parent_period):\n",
        "    if child_period == \"D\" and parent_period == \"W\":\n",
        "        year, week, _ = row['date'].isocalendar()\n",
        "        return f\"{year}/{week:02d}\"\n",
        "    elif child_period == \"D\" and parent_period == \"M\":\n",
        "        return row['date'].strftime('%Y/%m')\n",
        "    elif child_period == \"M\" and parent_period == \"Q\":\n",
        "        quarter = (row['date'].month - 1) // 3 + 1\n",
        "        return f\"{row['date'].year}/Q{quarter}\"\n",
        "    elif child_period == \"M\" and parent_period == \"Y\":\n",
        "        return f\"{row['date'].year}\"\n",
        "    elif child_period == \"Q\" and parent_period == \"Y\":\n",
        "        return f\"{row['date'].year}/Q{(row['date'].month - 1) // 3 + 1}\"\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported child-parent combination: {child_period}:{parent_period}\")\n",
        "\n",
        "# Process each input file\n",
        "for file_name in os.listdir(input_path):\n",
        "    if file_name.endswith('.csv'):\n",
        "        parts = file_name.split('_')\n",
        "        if len(parts) < 3:\n",
        "            print(f\"Skipping invalid file: {file_name}\")\n",
        "            continue\n",
        "\n",
        "        ticker = parts[0]  # Extract Ticker\n",
        "        rolling_range_dur = int(parts[2])  # Extract rolling range duration\n",
        "        print(f\"Processing {file_name} for Ticker: {ticker}\")\n",
        "\n",
        "        # Load and validate data\n",
        "        input_file = os.path.join(input_path, file_name)\n",
        "        data = pd.read_csv(input_file)\n",
        "        data['date'] = pd.to_datetime(data['date'])\n",
        "        required_columns = ['date', 'open', 'high', 'low', 'close']\n",
        "        for col in required_columns:\n",
        "            if col not in data.columns:\n",
        "                raise ValueError(f\"Missing required column: {col} in {file_name}\")\n",
        "\n",
        "        # Add metadata fields\n",
        "        data['serial_id'] = data.index + 1  # Simple row-based serial ID\n",
        "        data['row_number'] = data.index + 1\n",
        "        data['ticker'] = ticker\n",
        "        data['rolling_range_dur'] = rolling_range_dur\n",
        "        data['temporal_period'] = child_period\n",
        "\n",
        "        # Assign parent_lookup_date\n",
        "        data['parent_lookup_date'] = data.apply(\n",
        "            lambda row: assign_parent_lookup_date(row, child_period, parent_period), axis=1\n",
        "        )\n",
        "\n",
        "        # Sort data for processing\n",
        "        data = data.sort_values(by=['parent_lookup_date', 'date'])\n",
        "\n",
        "        # Initialize calculated fields\n",
        "        calculated_fields = [\n",
        "            'gel_reu_value', 'gel_red_value', 'gel_reu_flag', 'gel_red_flag', 'gel_re_flag',\n",
        "            'gel_range', 'gel_percent_r', 'gel_ce_percent', 'gel_epc_dir', 'gel_epc',\n",
        "            'gel_epc_hp', 'gel_rpc', 'gel_e1_value', 'gel_e2_value', 'gel_fre_dir', 'gelo',\n",
        "            'gelh', 'gell', 'gelc', 'gel_total_rpc', 'range', 'gel_period_percent_r',\n",
        "            'gel_twoway', 'gel_dir_count', 'gel_e1_flag', 'gel_e2_flag', 're_flag',\n",
        "            're_value', 'twoway', 'fre_dir_input', 'ro', 'rh', 'rl', 'rc'\n",
        "        ]\n",
        "        for field in calculated_fields:\n",
        "            data[field] = None  # Initialize all calculated fields with None\n",
        "\n",
        "        # Process each parent group\n",
        "        grouped = data.groupby('parent_lookup_date')\n",
        "        for parent, group in grouped:\n",
        "            previous_high = None\n",
        "            previous_low = None\n",
        "\n",
        "            for idx, row in group.iterrows():\n",
        "                high = row['high']\n",
        "                low = row['low']\n",
        "                close = row['close']\n",
        "                open_ = row['open']\n",
        "\n",
        "                # Calculate gel_reu and gel_red values\n",
        "                gel_reu_value = max(0, high - previous_high) if previous_high is not None else 0\n",
        "                gel_red_value = max(0, previous_low - low) if previous_low is not None else 0\n",
        "\n",
        "                # Calculate bar-specific fields\n",
        "                gelo = open_ if previous_high is None else previous_high\n",
        "                gell = low if previous_low is None else previous_low\n",
        "                gelh = high if previous_high is None else max(previous_high, high)\n",
        "                gelc = close\n",
        "\n",
        "                # Update calculated fields\n",
        "                data.at[idx, 'gel_reu_value'] = gel_reu_value\n",
        "                data.at[idx, 'gel_red_value'] = gel_red_value\n",
        "                data.at[idx, 'gel_reu_flag'] = gel_reu_value > 0\n",
        "                data.at[idx, 'gel_red_flag'] = gel_red_value > 0\n",
        "                data.at[idx, 'gel_re_flag'] = (gel_reu_value > 0) or (gel_red_value > 0)\n",
        "                data.at[idx, 'gelo'] = gelo\n",
        "                data.at[idx, 'gell'] = gell\n",
        "                data.at[idx, 'gelh'] = gelh\n",
        "                data.at[idx, 'gelc'] = gelc\n",
        "                data.at[idx, 'range'] = gelh - gell\n",
        "                data.at[idx, 'gel_range'] = gelh - gell\n",
        "\n",
        "                # Update previous values\n",
        "                previous_high = gelh\n",
        "                previous_low = gell\n",
        "\n",
        "        # Add metadata\n",
        "        data['create_date'] = datetime.now().strftime('%Y-%m-%d')\n",
        "        data['create_time'] = datetime.now().strftime('%H:%M:%S')\n",
        "        data['jobname'] = jobname\n",
        "\n",
        "        # Export Gel File\n",
        "        gel_output_file = os.path.join(output_path, f\"{ticker}_Gel_{child_period}.csv\")\n",
        "        data.to_csv(gel_output_file, index=False)\n",
        "\n",
        "        # Generate summary metrics for parent export\n",
        "        summary = grouped.agg(\n",
        "            start_date=('date', 'min'),\n",
        "            end_date=('date', 'max'),\n",
        "            child_count=('date', 'count'),\n",
        "            reu_count=('gel_reu_flag', 'sum'),\n",
        "            red_count=('gel_red_flag', 'sum'),\n",
        "            total_rpc=('gel_re_flag', 'sum')\n",
        "        ).reset_index()\n",
        "\n",
        "        # Add metadata to summary\n",
        "        summary['create_date'] = datetime.now().strftime('%Y-%m-%d')\n",
        "        summary['create_time'] = datetime.now().strftime('%H:%M:%S')\n",
        "        summary['jobname'] = jobname\n",
        "\n",
        "        # Export parent summary file\n",
        "        parent_output_file = os.path.join(parent_output_path, f\"{ticker}_output_parent_{parent_period}.csv\")\n",
        "        summary.to_csv(parent_output_file, index=False)\n",
        "\n",
        "        print(f\"Exported Gel File: {gel_output_file}\")\n",
        "        print(f\"Exported Parent Summary: {parent_output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "gJZf5V_hGLAn",
        "outputId": "b5d7e8aa-e3bd-4f6e-b027-1c36831d8e9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "invalid literal for int() with base 10: '1.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-36e01119e627>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mticker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Extract Ticker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mrolling_range_dur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Extract rolling range duration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Processing {file_name} for Ticker: {ticker}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '1.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Parameters\n",
        "child_period = \"D\"  # Default: \"D\" (Daily)\n",
        "parent_period = \"M\"  # Default: \"M\" (Monthly)\n",
        "jobname = \"BPB_20250105\"  # Job name to include in output files\n",
        "\n",
        "# Paths\n",
        "input_path = \"/content/input\"\n",
        "output_path = \"/content/output_gel\"\n",
        "parent_output_path = \"/content/output_parent\"\n",
        "\n",
        "# Ensure output directories exist\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "os.makedirs(parent_output_path, exist_ok=True)\n",
        "\n",
        "# Parent lookup logic\n",
        "def assign_parent_lookup_date(row, child_period, parent_period):\n",
        "    if child_period == \"D\" and parent_period == \"W\":\n",
        "        year, week, _ = row['date'].isocalendar()\n",
        "        return f\"{year}/{week:02d}\"\n",
        "    elif child_period == \"D\" and parent_period == \"M\":\n",
        "        return row['date'].strftime('%Y/%m')\n",
        "    elif child_period == \"M\" and parent_period == \"Q\":\n",
        "        quarter = (row['date'].month - 1) // 3 + 1\n",
        "        return f\"{row['date'].year}/Q{quarter}\"\n",
        "    elif child_period == \"M\" and parent_period == \"Y\":\n",
        "        return f\"{row['date'].year}\"\n",
        "    elif child_period == \"Q\" and parent_period == \"Y\":\n",
        "        return f\"{row['date'].year}/Q{(row['date'].month - 1) // 3 + 1}\"\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported child-parent combination: {child_period}:{parent_period}\")\n",
        "\n",
        "# Process each input file\n",
        "for file_name in os.listdir(input_path):\n",
        "    if file_name.endswith('.csv'):\n",
        "        parts = file_name.split('_')\n",
        "        if len(parts) < 3:\n",
        "            print(f\"Skipping invalid file: {file_name}\")\n",
        "            continue\n",
        "\n",
        "        ticker = parts[0]\n",
        "        rolling_range_dur = int(parts[2].split('.')[0])\n",
        "        print(f\"Processing {file_name} for Ticker: {ticker}\")\n",
        "\n",
        "        # Load and validate data\n",
        "        input_file = os.path.join(input_path, file_name)\n",
        "        data = pd.read_csv(input_file)\n",
        "        data['date'] = pd.to_datetime(data['date'])\n",
        "        required_columns = ['date', 'open', 'high', 'low', 'close']\n",
        "        for col in required_columns:\n",
        "            if col not in data.columns:\n",
        "                raise ValueError(f\"Missing required column: {col} in {file_name}\")\n",
        "\n",
        "        # Add metadata fields\n",
        "        data['serial_id'] = data.index + 1\n",
        "        data['row_number'] = data.index + 1\n",
        "        data['ticker'] = ticker\n",
        "        data['rolling_range_dur'] = rolling_range_dur\n",
        "        data['temporal_period'] = child_period\n",
        "\n",
        "        # Assign parent_lookup_date\n",
        "        data['parent_lookup_date'] = data.apply(\n",
        "            lambda row: assign_parent_lookup_date(row, child_period, parent_period), axis=1\n",
        "        )\n",
        "\n",
        "        # Sort data for processing\n",
        "        data = data.sort_values(by=['parent_lookup_date', 'date'])\n",
        "\n",
        "        # Initialize calculated fields\n",
        "        calculated_fields = [\n",
        "            'gel_rpc', 'gel_e1_value', 'gel_e2_value', 'gel_fre_dir',\n",
        "            'gel_total_rpc', 'range', 'gel_period_percent_r', 'gel_twoway',\n",
        "            'gel_dir_count', 'gel_e1_flag', 'gel_e2_flag', 're_flag', 're_value',\n",
        "            'twoway', 'fre_dir_input'\n",
        "        ]\n",
        "        for field in calculated_fields:\n",
        "            data[field] = None\n",
        "\n",
        "        # Process each parent group\n",
        "        grouped = data.groupby('parent_lookup_date')\n",
        "        for parent, group in grouped:\n",
        "            previous_gelo = None\n",
        "            previous_gelh = None\n",
        "            previous_gell = None\n",
        "            previous_gelc = None\n",
        "\n",
        "            # Assign trading_bop\n",
        "            group = group.sort_values('date')\n",
        "            group['trading_bop'] = range(1, len(group) + 1)\n",
        "\n",
        "            for idx, row in group.iterrows():\n",
        "                high = row['high']\n",
        "                low = row['low']\n",
        "                close = row['close']\n",
        "                open_ = row['open']\n",
        "\n",
        "                if row['trading_bop'] == 1:\n",
        "                    gelo = open_\n",
        "                    gelh = high\n",
        "                    gell = low\n",
        "                    gelc = close\n",
        "                else:\n",
        "                    gelo = previous_gelo\n",
        "                    gelh = max(previous_gelh, high) if previous_gelh is not None else high\n",
        "                    gell = min(previous_gell, low) if previous_gell is not None else low\n",
        "                    gelc = close\n",
        "\n",
        "                # Calculate basic metrics\n",
        "                range_ = high - low\n",
        "                gel_range = gelh - gell\n",
        "                gel_percent_r = (gelc - gell) / gel_range if gel_range > 0 else 0\n",
        "                gel_ce_percent = (1 - gel_percent_r) if gel_percent_r >= 0.5 else gel_percent_r\n",
        "                gel_epc = round(gel_ce_percent / 0.1, 0)\n",
        "                gel_epc_dir = \"U\" if gel_percent_r >= 0.5 else \"D\"\n",
        "                gel_epc_hp = gel_ce_percent >= 0.25\n",
        "\n",
        "                # Calculate RPC-related metrics\n",
        "                gel_rpc = 1 if gel_reu_value > 0 or gel_red_value > 0 else 0\n",
        "                gel_total_rpc = (data.at[idx - 1, 'gel_total_rpc'] if idx > 0 else 0) + gel_rpc\n",
        "                gel_e1_value = gel_reu_value if gel_epc_dir == \"U\" else gel_red_value\n",
        "                gel_e2_value = gel_red_value if gel_epc_dir == \"U\" else gel_reu_value\n",
        "                gel_fre_dir = (\n",
        "                    \"U\" if gel_reu_value > 0 else \"D\" if gel_red_value > 0 else \"N\"\n",
        "                )\n",
        "\n",
        "                # Populate prior row values for the current row\n",
        "                ro = previous_gelo\n",
        "                rh = previous_gelh\n",
        "                rl = previous_gell\n",
        "                rc = previous_gelc\n",
        "\n",
        "                # Update fields\n",
        "                data.at[idx, 'range'] = range_\n",
        "                data.at[idx, 'gel_period_percent_r'] = gel_percent_r\n",
        "                data.at[idx, 'gel_rpc'] = gel_rpc\n",
        "                data.at[idx, 'gel_total_rpc'] = gel_total_rpc\n",
        "                data.at[idx, 'gel_e1_value'] = gel_e1_value\n",
        "                data.at[idx, 'gel_e2_value'] = gel_e2_value\n",
        "                data.at[idx, 'gel_fre_dir'] = gel_fre_dir\n",
        "                data.at[idx, 're_flag'] = gel_reu_value > 0 or gel_red_value > 0\n",
        "                data.at[idx, 'twoway'] = gel_reu_value > 0 and gel_red_value > 0\n",
        "\n",
        "                # Update previous values\n",
        "                previous_gelo = gelo\n",
        "                previous_gelh = gelh\n",
        "                previous_gell = gell\n",
        "                previous_gelc = gelc\n",
        "\n",
        "        # Add metadata\n",
        "        data['create_date'] = datetime.now().strftime('%Y-%m-%d')\n",
        "        data['create_time'] = datetime.now().strftime('%H:%M:%S')\n",
        "        data['jobname'] = jobname\n",
        "\n",
        "        # Export Gel File\n",
        "        gel_output_file = os.path.join(output_path, f\"{ticker}_Gel_{child_period}.csv\")\n",
        "        data.to_csv(gel_output_file, index=False)\n",
        "\n",
        "        # Generate summary metrics for parent export\n",
        "        summary = grouped.agg(\n",
        "            start_date=('date', 'min'),\n",
        "            end_date=('date', 'max'),\n",
        "            child_count=('date', 'count'),\n",
        "            reu_count=('gel_reu_flag', 'sum'),\n",
        "            red_count=('gel_red_flag', 'sum'),\n",
        "            total_rpc=('gel_re_flag', 'sum')\n",
        "        ).reset_index()\n",
        "\n",
        "        # Add metadata to summary\n",
        "        summary['create_date'] = datetime.now().strftime('%Y-%m-%d')\n",
        "        summary['create_time'] = datetime.now().strftime('%H:%M:%S')\n",
        "        summary['jobname'] = jobname\n",
        "\n",
        "        # Export parent summary file\n",
        "        parent_output_file = os.path.join(parent_output_path, f\"{ticker}_output_parent_{parent_period}.csv\")\n",
        "        summary.to_csv(parent_output_file, index=False)\n",
        "\n",
        "        print(f\"Exported Gel File: {gel_output_file}\")\n",
        "        print(f\"Exported Parent Summary: {parent_output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "4bzW19wjGYgX",
        "outputId": "9d26d441-10dd-4319-ba8b-9ceb913d746a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing AAPL_D_1.csv for Ticker: AAPL\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"Column(s) ['gel_re_flag', 'gel_red_flag', 'gel_reu_flag'] do not exist\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-2d1302daac41>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;31m# Generate summary metrics for parent export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         summary = grouped.agg(\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0mstart_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'min'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mend_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'max'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGroupByApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1432\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m             \u001b[0;31m# GH #52849\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36magg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;31m# we require a list, but not a 'str'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36magg_dict_like\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mResult\u001b[0m \u001b[0mof\u001b[0m \u001b[0maggregation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \"\"\"\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg_or_apply_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"agg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     def compute_dict_like(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36magg_or_apply_dict_like\u001b[0;34m(self, op_name)\u001b[0m\n\u001b[1;32m   1606\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"as_index\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"as_index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         ):\n\u001b[0;32m-> 1608\u001b[0;31m             result_index, result_data = self.compute_dict_like(\n\u001b[0m\u001b[1;32m   1609\u001b[0m                 \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mcompute_dict_like\u001b[0;34m(self, op_name, selected_obj, selection, kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0mis_groupby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDataFrameGroupBy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeriesGroupBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAggFuncTypeDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_dictlike_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         is_non_unique_col = (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mnormalize_dictlike_arg\u001b[0;34m(self, how, obj, func)\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Column(s) {list(cols)} do not exist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0maggregator_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Column(s) ['gel_re_flag', 'gel_red_flag', 'gel_reu_flag'] do not exist\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Parameters\n",
        "child_period = \"D\"  # Default: \"D\" (Daily)\n",
        "parent_period = \"M\"  # Default: \"M\" (Monthly)\n",
        "jobname = \"BPB_20250105\"  # Job name to include in output files\n",
        "\n",
        "# Paths\n",
        "input_path = \"/content/input\"\n",
        "output_path = \"/content/output_gel\"\n",
        "parent_output_path = \"/content/output_parent\"\n",
        "\n",
        "# Ensure output directories exist\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "os.makedirs(parent_output_path, exist_ok=True)\n",
        "\n",
        "# Parent lookup logic\n",
        "def assign_parent_lookup_date(row, child_period, parent_period):\n",
        "    if child_period == \"D\" and parent_period == \"W\":\n",
        "        year, week, _ = row['date'].isocalendar()\n",
        "        return f\"{year}/{week:02d}\"\n",
        "    elif child_period == \"D\" and parent_period == \"M\":\n",
        "        return row['date'].strftime('%Y/%m')\n",
        "    elif child_period == \"M\" and parent_period == \"Q\":\n",
        "        quarter = (row['date'].month - 1) // 3 + 1\n",
        "        return f\"{row['date'].year}/Q{quarter}\"\n",
        "    elif child_period == \"M\" and parent_period == \"Y\":\n",
        "        return f\"{row['date'].year}\"\n",
        "    elif child_period == \"Q\" and parent_period == \"Y\":\n",
        "        return f\"{row['date'].year}/Q{(row['date'].month - 1) // 3 + 1}\"\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported child-parent combination: {child_period}:{parent_period}\")\n",
        "\n",
        "for file_name in os.listdir(input_path):\n",
        "    if file_name.endswith('.csv'):\n",
        "        parts = file_name.split('_')\n",
        "        if len(parts) < 3:\n",
        "            print(f\"Skipping invalid file: {file_name}\")\n",
        "            continue\n",
        "\n",
        "        ticker = parts[0]\n",
        "        rolling_range_dur = int(parts[2].split('.')[0])\n",
        "        print(f\"Processing {file_name} for Ticker: {ticker}\")\n",
        "\n",
        "        # Load and validate data\n",
        "        input_file = os.path.join(input_path, file_name)\n",
        "        data = pd.read_csv(input_file)\n",
        "        data['date'] = pd.to_datetime(data['date'])\n",
        "        required_columns = ['date', 'open', 'high', 'low', 'close']\n",
        "        for col in required_columns:\n",
        "            if col not in data.columns:\n",
        "                raise ValueError(f\"Missing required column: {col} in {file_name}\")\n",
        "\n",
        "        required_columns_for_summary = ['gel_re_flag', 'gel_red_flag', 'gel_reu_flag']\n",
        "\n",
        "        # Add metadata fields\n",
        "        data['serial_id'] = data.index + 1\n",
        "        data['row_number'] = data.index + 1\n",
        "        data['ticker'] = ticker\n",
        "        data['rolling_range_dur'] = rolling_range_dur\n",
        "        data['temporal_period'] = child_period\n",
        "\n",
        "        # Assign parent_lookup_date\n",
        "        data['parent_lookup_date'] = data.apply(\n",
        "            lambda row: assign_parent_lookup_date(row, child_period, parent_period), axis=1\n",
        "        )\n",
        "\n",
        "        # Sort data for processing\n",
        "        data = data.sort_values(by=['parent_lookup_date', 'date'])\n",
        "\n",
        "        # Initialize calculated fields\n",
        "        calculated_fields = [\n",
        "            'gel_rpc', 'gel_e1_value', 'gel_e2_value', 'gel_fre_dir',\n",
        "            'gel_total_rpc', 'range', 'gel_period_percent_r', 'gel_twoway',\n",
        "            'gel_dir_count', 'gel_e1_flag', 'gel_e2_flag', 're_flag', 'gel_re_flag',\n",
        "            'gel_red_flag', 'gel_reu_flag', 're_value', 'twoway', 'fre_dir_input'\n",
        "        ]\n",
        "        for field in calculated_fields:\n",
        "            data[field] = None\n",
        "\n",
        "        # Process each parent group\n",
        "        grouped = data.groupby('parent_lookup_date')\n",
        "        for parent, group in grouped:\n",
        "            previous_gelo = None\n",
        "            previous_gelh = None\n",
        "            previous_gell = None\n",
        "            previous_gelc = None\n",
        "\n",
        "            # Assign trading_bop\n",
        "            group = group.sort_values('date')\n",
        "            group['trading_bop'] = range(1, len(group) + 1)\n",
        "\n",
        "            for idx, row in group.iterrows():\n",
        "                high = row['high']\n",
        "                low = row['low']\n",
        "                close = row['close']\n",
        "                open_ = row['open']\n",
        "\n",
        "                if row['trading_bop'] == 1:\n",
        "                    gelo = open_\n",
        "                    gelh = high\n",
        "                    gell = low\n",
        "                    gelc = close\n",
        "                else:\n",
        "                    gelo = previous_gelo\n",
        "                    gelh = max(previous_gelh, high) if previous_gelh is not None else high\n",
        "                    gell = min(previous_gell, low) if previous_gell is not None else low\n",
        "                    gelc = close\n",
        "\n",
        "                # Calculate basic metrics\n",
        "                range_ = high - low\n",
        "                gel_range = gelh - gell\n",
        "                gel_percent_r = (gelc - gell) / gel_range if gel_range > 0 else 0\n",
        "\n",
        "                # Calculate flags\n",
        "                gel_reu_flag = gel_reu_value > 0\n",
        "                gel_red_flag = gel_red_value > 0\n",
        "                gel_re_flag = gel_reu_flag or gel_red_flag\n",
        "\n",
        "                # Update fields\n",
        "                data.at[idx, 'gel_reu_flag'] = gel_reu_flag\n",
        "                data.at[idx, 'gel_red_flag'] = gel_red_flag\n",
        "                data.at[idx, 'gel_re_flag'] = gel_re_flag\n",
        "\n",
        "                # Update previous values\n",
        "                previous_gelo = gelo\n",
        "                previous_gelh = gelh\n",
        "                previous_gell = gell\n",
        "                previous_gelc = gelc\n",
        "\n",
        "        # Ensure fields for aggregation\n",
        "        for col in required_columns_for_summary:\n",
        "            if col not in data.columns:\n",
        "                data[col] = False\n",
        "\n",
        "        # Generate summary metrics\n",
        "        summary = grouped.agg(\n",
        "            start_date=('date', 'min'),\n",
        "            end_date=('date', 'max'),\n",
        "            child_count=('date', 'count'),\n",
        "            reu_count=('gel_reu_flag', 'sum'),\n",
        "            red_count=('gel_red_flag', 'sum'),\n",
        "            total_rpc=('gel_re_flag', 'sum')\n",
        "        ).reset_index()\n",
        "\n",
        "        # Export files\n",
        "        gel_output_file = os.path.join(output_path, f\"{ticker}_Gel_{child_period}.csv\")\n",
        "        data.to_csv(gel_output_file, index=False)\n",
        "\n",
        "        parent_output_file = os.path.join(parent_output_path, f\"{ticker}_output_parent_{parent_period}.csv\")\n",
        "        summary.to_csv(parent_output_file, index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuvbAGo3GubA",
        "outputId": "1b5993a8-803a-41c8-b212-d522cced860c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing AAPL_D_1.csv for Ticker: AAPL\n",
            "Processing MMM_D_1.csv for Ticker: MMM\n",
            "Processing AFL_D_1.csv for Ticker: AFL\n"
          ]
        }
      ]
    }
  ]
}