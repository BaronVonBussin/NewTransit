{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxxP/g0o5/9zYvCIH3wzhV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BaronVonBussin/NewTransit/blob/main/get_ticker_data_20241223.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0tKO5PoKLSp"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "\n",
        "# List of tickers\n",
        "tickers = [\"SPY\", \"TQQQ\", \"QQQ\", \"SQQQ\", \"EEM\", \"XLF\", \"GLD\", \"XLE\", \"EFA\", \"GDX\", \"XLK\", \"TLT\", \"XLV\", \"FXI\", \"XLY\", \"XLI\", \"XLU\", \"XLP\", \"XLB\", \"TSLA\", \"AMD\", \"AMZN\", \"META\", \"NVDA\", \"BAC\", \"F\", \"LCID\", \"WFC\", \"T\", \"PLTR\", \"GE\", \"GOOGL\", \"GOOG\", \"PYPL\", \"NFLX\", \"MRO\", \"PFE\", \"SOFI\", \"UBER\", \"XOM\", \"AAP\", \"MS\", \"BMY\", \"GM\", \"OXY\", \"C\", \"CCL\", \"SNAP\", \"SQ\", \"NIO\", \"X\", \"DAL\", \"FCX\", \"BABA\", \"DVN\", \"MPC\", \"RIVN\", \"UAL\", \"AAL\", \"GOLD\", \"ABBV\", \"HAL\", \"TGT\", \"MU\", \"HOOD\", \"DKNG\", \"COP\", \"CVE\", \"PENN\"]\n",
        "\n",
        "# Download data since 1995\n",
        "data = yf.download(tickers, start=\"2004-01-01\", end=\"2024-12-31\", group_by=\"ticker\")\n",
        "\n",
        "# Save each ticker's data as a CSV\n",
        "for ticker in tickers:\n",
        "    data[ticker].to_csv(f\"{ticker}_OHLC.csv\")\n",
        "\n",
        "print(\"OHLC data downloaded and saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Use same tickers as cell 1\n",
        "tickers = [\"SPY\", \"TQQQ\", \"QQQ\", \"SQQQ\", \"EEM\", \"XLF\", \"GLD\", \"XLE\", \"EFA\", \"GDX\", \"XLK\", \"TLT\", \"XLV\", \"FXI\", \"XLY\", \"XLI\", \"XLU\", \"XLP\", \"XLB\", \"TSLA\", \"AMD\", \"AMZN\", \"META\", \"NVDA\", \"BAC\", \"F\", \"LCID\", \"WFC\", \"T\", \"PLTR\", \"GE\", \"GOOGL\", \"GOOG\", \"PYPL\", \"NFLX\", \"MRO\", \"PFE\", \"SOFI\", \"UBER\", \"XOM\", \"AAP\", \"MS\", \"BMY\", \"GM\", \"OXY\", \"C\", \"CCL\", \"SNAP\", \"SQ\", \"NIO\", \"X\", \"DAL\", \"FCX\", \"BABA\", \"DVN\", \"MPC\", \"RIVN\", \"UAL\", \"AAL\", \"GOLD\", \"ABBV\", \"HAL\", \"TGT\", \"MU\", \"HOOD\", \"DKNG\", \"COP\", \"CVE\", \"PENN\"]\n",
        "\n",
        "# Create export directories if they don't exist\n",
        "os.makedirs('/content/export_daily', exist_ok=True)\n",
        "os.makedirs('/content/export_weekly', exist_ok=True)\n",
        "os.makedirs('/content/export_monthly', exist_ok=True)\n",
        "\n",
        "def validate_data(df, ticker):\n",
        "    \"\"\"Validate OHLC data for errors\"\"\"\n",
        "    # Store original row count\n",
        "    original_count = len(df)\n",
        "\n",
        "    # Round all values to 4 decimal places\n",
        "    for col in ['Open', 'High', 'Low', 'Close']:\n",
        "        df[col] = df[col].round(4)\n",
        "\n",
        "    # Create mask for each condition\n",
        "    non_zero = (df['Open'] > 0) & (df['High'] > 0) & (df['Low'] > 0) & (df['Close'] > 0)\n",
        "    valid_low = (df['Low'] <= df['Close']) & (df['Low'] <= df['Open'])\n",
        "    valid_high = (df['High'] >= df['Close']) & (df['High'] >= df['Open'])\n",
        "    valid_range = (df['High'] - df['Low']) > 0\n",
        "\n",
        "    # Combine all conditions\n",
        "    valid_rows = non_zero & valid_low & valid_high & valid_range\n",
        "\n",
        "    # Filter data\n",
        "    df_clean = df[valid_rows].copy()\n",
        "\n",
        "    # Report removed rows\n",
        "    removed_count = original_count - len(df_clean)\n",
        "    if removed_count > 0:\n",
        "        print(f\"{ticker}: Removed {removed_count} invalid rows out of {original_count}\")\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "def load_data(ticker):\n",
        "    file_path = f'{ticker}_OHLC.csv'\n",
        "    df = pd.read_csv(file_path)\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "    # Ensure column order for daily data\n",
        "    column_order = ['Date', 'Open', 'High', 'Low', 'Close']\n",
        "    df = df[column_order]\n",
        "\n",
        "    # Validate and clean data before proceeding\n",
        "    df = validate_data(df, ticker)\n",
        "\n",
        "    return df\n",
        "\n",
        "def aggregate_data(df):\n",
        "    agg_rules = {\n",
        "        'Open': 'first',\n",
        "        'High': 'max',\n",
        "        'Low': 'min',\n",
        "        'Close': 'last'\n",
        "    }\n",
        "\n",
        "    weekly_data = df.set_index('Date').resample('W').agg(agg_rules).reset_index()\n",
        "    monthly_data = df.set_index('Date').resample('M').agg(agg_rules).reset_index()\n",
        "\n",
        "    column_order = ['Date', 'Open', 'High', 'Low', 'Close']\n",
        "    weekly_data = weekly_data[column_order]\n",
        "    monthly_data = monthly_data[column_order]\n",
        "\n",
        "    # Validate aggregated data\n",
        "    weekly_data = validate_data(weekly_data, f\"{df.name}_weekly\")\n",
        "    monthly_data = validate_data(monthly_data, f\"{df.name}_monthly\")\n",
        "\n",
        "    return weekly_data, monthly_data\n",
        "\n",
        "def save_to_csv(data, ticker, period):\n",
        "    if period == 'daily':\n",
        "        file_path = f'/content/export_daily/{ticker}_daily.csv'\n",
        "    elif period == 'weekly':\n",
        "        file_path = f'/content/export_weekly/{ticker}_weekly.csv'\n",
        "    else:\n",
        "        file_path = f'/content/export_monthly/{ticker}_monthly.csv'\n",
        "\n",
        "    data.to_csv(file_path, index=False)\n",
        "    print(f\"Saved {file_path}\")\n",
        "\n",
        "def process_ticker(ticker):\n",
        "    print(f\"Processing {ticker}...\")\n",
        "    try:\n",
        "        # Load and save daily data\n",
        "        df = load_data(ticker)\n",
        "        df.name = ticker  # Add name attribute for reference in aggregation\n",
        "\n",
        "        if len(df) > 0:  # Only proceed if we have valid data\n",
        "            save_to_csv(df, ticker, 'daily')\n",
        "\n",
        "            # Process and save weekly/monthly data\n",
        "            weekly_data, monthly_data = aggregate_data(df)\n",
        "\n",
        "            if len(weekly_data) > 0:\n",
        "                save_to_csv(weekly_data, ticker, 'weekly')\n",
        "            if len(monthly_data) > 0:\n",
        "                save_to_csv(monthly_data, ticker, 'monthly')\n",
        "\n",
        "            print(f\"Successfully processed {ticker}\")\n",
        "        else:\n",
        "            print(f\"No valid data for {ticker}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {ticker}: {str(e)}\")\n",
        "\n",
        "# Process all tickers\n",
        "for ticker in tickers:\n",
        "    process_ticker(ticker)"
      ],
      "metadata": {
        "id": "v54f4Pja7eBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Use same tickers as cell 1\n",
        "tickers = [\"SPY\", \"TQQQ\", \"QQQ\", \"SQQQ\", \"EEM\", \"XLF\", \"GLD\", \"XLE\", \"EFA\", \"GDX\", \"XLK\",\n",
        "           \"TLT\", \"XLV\", \"FXI\", \"XLY\",\n",
        "           \"XLI\", \"XLU\", \"XLP\", \"XLB\"]\n",
        "\n",
        "# Create export directories if they don't exist\n",
        "os.makedirs('/content/export_daily', exist_ok=True)\n",
        "os.makedirs('/content/export_weekly', exist_ok=True)\n",
        "os.makedirs('/content/export_monthly', exist_ok=True)\n",
        "\n",
        "def load_data(ticker):\n",
        "    file_path = f'{ticker}_OHLC.csv'\n",
        "    df = pd.read_csv(file_path)\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "    # Ensure column order for daily data\n",
        "    column_order = ['Date', 'Open', 'High', 'Low', 'Close']\n",
        "    df = df[column_order]\n",
        "\n",
        "    return df\n",
        "\n",
        "def aggregate_data(df):\n",
        "    agg_rules = {\n",
        "        'Open': 'first',\n",
        "        'High': 'max',\n",
        "        'Low': 'min',\n",
        "        'Close': 'last'\n",
        "    }\n",
        "\n",
        "    weekly_data = df.set_index('Date').resample('W').agg(agg_rules).reset_index()\n",
        "    monthly_data = df.set_index('Date').resample('M').agg(agg_rules).reset_index()\n",
        "\n",
        "    column_order = ['Date', 'Open', 'High', 'Low', 'Close']\n",
        "    weekly_data = weekly_data[column_order]\n",
        "    monthly_data = monthly_data[column_order]\n",
        "\n",
        "    return weekly_data, monthly_data\n",
        "\n",
        "def save_to_csv(data, ticker, period):\n",
        "    if period == 'daily':\n",
        "        file_path = f'/content/export_daily/{ticker}_daily.csv'\n",
        "    elif period == 'weekly':\n",
        "        file_path = f'/content/export_weekly/{ticker}_weekly.csv'\n",
        "    else:\n",
        "        file_path = f'/content/export_monthly/{ticker}_monthly.csv'\n",
        "\n",
        "    data.to_csv(file_path, index=False)\n",
        "    print(f\"Saved {file_path}\")\n",
        "\n",
        "def process_ticker(ticker):\n",
        "    print(f\"Processing {ticker}...\")\n",
        "    try:\n",
        "        # Load and save daily data\n",
        "        df = load_data(ticker)\n",
        "        save_to_csv(df, ticker, 'daily')\n",
        "\n",
        "        # Process and save weekly/monthly data\n",
        "        weekly_data, monthly_data = aggregate_data(df)\n",
        "        save_to_csv(weekly_data, ticker, 'weekly')\n",
        "        save_to_csv(monthly_data, ticker, 'monthly')\n",
        "\n",
        "        print(f\"Successfully processed {ticker}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {ticker}: {str(e)}\")\n",
        "\n",
        "# Process all tickers\n",
        "for ticker in tickers:\n",
        "    process_ticker(ticker)"
      ],
      "metadata": {
        "id": "6ve3zS_MiUOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "def count_and_download_files(directory, period):\n",
        "    file_count = len([name for name in os.listdir(directory) if os.path.isfile(os.path.join(directory, name))])\n",
        "    print(f\"Found {file_count} {period} files\")\n",
        "\n",
        "    # Download files\n",
        "    for filename in os.listdir(directory):\n",
        "        filepath = os.path.join(directory, filename)\n",
        "        if os.path.isfile(filepath):\n",
        "            files.download(filepath)\n",
        "\n",
        "print(\"\\nCounting and downloading files...\\n\")\n",
        "count_and_download_files('/content/export_monthly', 'monthly')"
      ],
      "metadata": {
        "id": "oJTyFfojgdFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "def zip_and_download(source_dir, period):\n",
        "    # Count files\n",
        "    file_count = len([name for name in os.listdir(source_dir) if os.path.isfile(os.path.join(source_dir, name))])\n",
        "    print(f\"Found {file_count} {period} files\")\n",
        "\n",
        "    # Create zip file\n",
        "    zip_filename = f'{period}_files.zip'\n",
        "    with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "        for file in os.listdir(source_dir):\n",
        "            file_path = os.path.join(source_dir, file)\n",
        "            if os.path.isfile(file_path):\n",
        "                zipf.write(file_path, arcname=file)\n",
        "\n",
        "    # Download zip file\n",
        "    files.download(zip_filename)\n",
        "    print(f\"Downloaded {zip_filename}\")\n",
        "\n",
        "# Create and download zips for each period\n",
        "print(\"\\nCreating and downloading zip files...\\n\")\n",
        "zip_and_download('/content/export_monthly', 'monthly')\n",
        "zip_and_download('/content/export_weekly', 'weekly')\n",
        "#zip_and_download('/content/export_daily', 'daily')"
      ],
      "metadata": {
        "id": "TR6zwuh8lwNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "def count_and_download_files(directory, period):\n",
        "    file_count = len([name for name in os.listdir(directory) if os.path.isfile(os.path.join(directory, name))])\n",
        "    print(f\"Found {file_count} {period} files\")\n",
        "\n",
        "    # Download files\n",
        "    for filename in os.listdir(directory):\n",
        "        filepath = os.path.join(directory, filename)\n",
        "        if os.path.isfile(filepath):\n",
        "            files.download(filepath)\n",
        "\n",
        "print(\"\\nCounting and downloading files...\\n\")\n",
        "count_and_download_files('/content/export_daily', 'daily')\n",
        "count_and_download_files('/content/export_weekly', 'weekly')\n",
        "count_and_download_files('/content/export_monthly', 'monthly')"
      ],
      "metadata": {
        "id": "IP0pN_KwlWiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load data from CSV\n",
        "def load_data(file_path):\n",
        "    # Load the CSV file\n",
        "    df = pd.read_csv(file_path)\n",
        "    # Ensure 'Date' column is in datetime format\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    # Set 'Date' as the index\n",
        "    df.set_index('Date', inplace=True)\n",
        "    return df\n",
        "\n",
        "# Aggregate data to weekly and monthly levels\n",
        "def aggregate_data(df):\n",
        "    # Weekly aggregation (sum)\n",
        "    weekly_data = df.resample('W').sum()\n",
        "\n",
        "    # Monthly aggregation (mean)\n",
        "    monthly_data = df.resample('ME').mean()\n",
        "\n",
        "    return weekly_data, monthly_data\n",
        "\n",
        "# Save aggregated data to CSV\n",
        "def save_to_csv(data, file_name):\n",
        "    data.to_csv(file_name)\n",
        "    print(f\"Saved {file_name}\")\n",
        "\n",
        "# Main program\n",
        "if __name__ == \"__main__\":\n",
        "    # Path to the input CSV file\n",
        "    input_file = \"/content/\"  # Replace with your file path\n",
        "\n",
        "    print(\"Loading data from CSV...\")\n",
        "    df = load_data(input_file)\n",
        "    print(\"Daily Data:\")\n",
        "    print(df.head())\n",
        "\n",
        "    print(\"\\nAggregating data to weekly and monthly levels...\")\n",
        "    weekly_data, monthly_data = aggregate_data(df)\n",
        "\n",
        "    print(\"\\nWeekly Data (Sum):\")\n",
        "    print(weekly_data.head())\n",
        "\n",
        "    print(\"\\nMonthly Data (Mean):\")\n",
        "    print(monthly_data.head())\n",
        "\n",
        "    # Save aggregated data to CSV files\n",
        "    save_to_csv(weekly_data, \"XLB_weekly_data.csv\")\n",
        "    save_to_csv(monthly_data, \"XLB_monthly_data.csv\")\n",
        "    print(\"\\nAggregated data saved to CSV files.\")\n"
      ],
      "metadata": {
        "id": "BgFJ5ijRpc93"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}